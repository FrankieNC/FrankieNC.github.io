\documentclass[12pt, a4paper]{article}
\usepackage{francesco}
\usepackage[colorlinks=true,
            urlcolor=RubineRed,
            linktoc=all,
            linkcolor=black,
            pdfauthor={Francesco N. Chotuck},
            pdftitle={Applied Differential Equations Notes}
            ]{hyperref}

\pagestyle{fancy}
\lhead{Francesco Chotuck}
\rhead{5CCM211A Applied Differential Equations}
\setlength{\headheight}{15pt}

\newcommand{\CL}{\mathcal{L}}

\title{Applied Differential Equations Notes}
\date{}
\author{Francesco Chotuck}
\begin{document}
\maketitle

\begin{abstract}
    This is KCL undergraduate module 5CCM211A, instructed by Professor \\ Giuseppe Tinaglia. The formal name for this class is ``Applied Differential Equations''.
\end{abstract}

\tableofcontents

\pagebreak

\section{The Laplace transform}

\begin{definition}
    The \textbf{Laplace transform} of a function 
    \[y(t) : [0,\infty) \to \RR\]
    is the function 
    \[\CL[y(t)](s) = Y(s) = \int_{0}^{\infty} y(t) e^{-st} \, dt\]
    for all numbers \(s\) for which this integral converges.
\end{definition}

\begin{mdnote}
    The Laplace transform takes a function of \(t\) as an input and outputs a function of \(s\).
\end{mdnote}

\begin{mdremark}
    Not all functions have a Laplace transform.
\end{mdremark}

% \begin{mdremark}
%     The Laplace transform is not an injective operator so it is not invertible.
% \end{mdremark}

\begin{example}
    Functions which do not have Laplace transform: 
    \begin{itemize}
        \item \(y(t) = \frac{1}{t}\) grows to fast near zero independently of \(s\):
        \[\int_{0}^{\infty} \frac{1}{t} e^{-st} \, dt = \infty.\]
        \item \(y(t) = e^{(t^2)}\) grows too fast as \(t \to \infty\) independently of \(s\):
        \[\int_{0}^{\infty} e^{(t^2)} e^{-st} \, dt =\infty.\]
    \end{itemize}
\end{example}

\begin{mdexample}
    Find the Laplace transform of 
    \[\begin{aligned}
        f(t) = \begin{cases}
            1 &\text{if } t \in [0,1) \\
            k &\text{if } t=1 \\
            0 &\text{if } t \in (1,\infty).
        \end{cases}
    \end{aligned}\]
    \begin{solution}
        \[\begin{aligned}
            \CL[f(t)](s) &= \int_{0}^{\infty} f(t)e^{-st} \,dt \\
            &= \int_{0}^{1} e^{-st} \, dt + \int_{1}^{1} k \, dt + \int_{1}^{\infty} 0 \, dt \\
            &= \int_{0}^{1} e^{-st} \, dt \\\
            &= \frac{1-e^{-s}}{s}.
        \end{aligned}\]
    \end{solution}
\end{mdexample}

\begin{mdexample}
    Compute the Laplace transform of \(y(t) = e^{at}\).
    \begin{solution}
        Applying the definition of Laplace transform we have 
        \[\begin{aligned}
            \CL[e^{at}](s) &= \int_{0}^{\infty} e^{at} e^{-st} \, dt \\
            &= \int_{0}^{\infty} e^{(a-s)t} \, dt \\
            &= \lim_{N \to \infty} \int_{0}^{N} e^{(a-s)t} \, dt \\
            &= \lim_{N \to \infty} \left[ \frac{e^{(a-s)t}}{a-s}\right]_0^N \\
            &= \frac{1}{a-s} \lim_{N \to \infty} \left( e^{(a-s) N} - e^0 \right) \\
            &= \frac{1}{a-s} \lim_{N \to \infty} \left( e^{(a-s) N} - 1 \right).
        \end{aligned}\]
        We now consider when the integral converges and diverges thus, we look at the size of \(s\) relative to \(a\).
        \begin{itemize}
            \item If \(s = a\) then the integral diverges.
            \item If \(s <a\) then the integral diverges.
            \item If \(s>a\) then the integral converges to
            \[- \frac{1}{a-s} = \frac{1}{s-a}.\]
        \end{itemize}
        Therefore, the Laplace transform of \(y(t) = e^{at}\) is 
        \[\begin{aligned}
            \CL[e^{at}](s) = \begin{cases}
            \frac{1}{s-a} &\text{if } s>a \\
            \text{undefined} &\text{if } s\leq a.
            \end{cases}
        \end{aligned}\]
    \end{solution}
\end{mdexample}

\begin{example}
    Compute the Laplace transform of \(y(t)=1\).
    \begin{solution}
        By the previous example we have,
        \[\CL[e^{at}](s) = \frac{1}{s-a} \quad \text{if } s>a.\]
        If \(a=0\) then, \(e^{at} = e^0 =1\) for all \(t\). Therefore,
        \[\CL[1](s) = \frac{1}{s} \quad  \text{if } s>0. \]
    \end{solution}
\end{example}

\begin{mdthm}
    We have the following property
    \[\CL[e^{at}f(t)](s) = \CL[f(t)](s-a).\]
\end{mdthm}

\begin{proof}
    Consider
    \[\begin{aligned}
        \CL[e^{at}f(t)(s)] &= \int_{0}^{\infty} f(t) e^{at} e^{-st} \, dt \\
        &= \int_{0}^{\infty} f(t) e^{(a-s)t} \, dt \\
        &= \int_{0}^{\infty} f(t) e^{-(s-a)t} \, dt \\
        &= \CL[f(t)](s-a).
    \end{aligned}\]
\end{proof}

\begin{definition}
    A function, \(y(t)\), is said to be \textbf{piecewise continuous} on a finite interval \([a,b]\) if it is continuous at every point in \([a,b]\), except possibly for a finite number of points at which \(y(t)\) has a jump discontinuity.
\end{definition}

\begin{example}
    Consider the function 
    \[\begin{aligned}
        g(t) = \begin{cases}
            t &\text{if } t\in[0,1) \\
            0 &\text{if } t\in[1,2].
        \end{cases}
    \end{aligned}\]
    Then \(g(t)\) is continuous on \([0,2]\) with a jump discontinuity at \(t=1\). Whereas, the function 
    \[\begin{aligned}
        f(t) = \begin{cases}
            \frac{1}{1-t} &\text{if } t\in[0,1) \\
            0 &\text{if } t\in[1,2]
        \end{cases}
    \end{aligned}\]
    has an infinite discontinuity at \(t=1\) so, it does not have a jump discontinuity on \([0,2]\). Therefore, it is not piecewise continuous.
\end{example}

\begin{definition}
    A function, \(y(t)\), is said to be \textbf{piecewise continuous} on \([0,\infty)\) if it is piecewise continuous on \([0,N]\) for any \(N >0\).
\end{definition}

\begin{definition}
    A function, \(y(t)\), is said to be of \textbf{exponential order} (or of exponential order \(\alpha\)) if there exist positive constants, \(T,M,\alpha >0\) such that 
    \[\forall t \in [T, \infty) \quad \text{we have} \quad \abs{y(t)} \leq Me^{\alpha t}.\]
\end{definition}

\begin{mdthm}
    Let \(y(t)\) be a piecewise continuous function on \([0,\infty)\) and of exponential order (\(\alpha>0\)). Then, \(Y(s) = \CL[y](s)\) exists for all \(s > \alpha\).
\end{mdthm}

\begin{proof}
    Fix \(s>\alpha\) then we need to show that
    \[Y(s) = \int_{0}^{\infty} y(t) e^{-st} \, dt\]
    is finite i.e. 
    \[Y(s) = \int_{0}^{\infty} y(t) e^{-st} \, dt < \infty.\]
    By the triangle inequality we have
    \[\abs{\int_{0}^{\infty} y(t) e^{-st} \, dt} \leq \abs{\int_{0}^{T} y(t) e^{-st} \, dt} + \abs{\int_{T}^{\infty} y(t) e^{-st} \, dt}.\]
    We consider the two integrals separately and prove they are both finite. Note that \(y(t)\) is a function of exponential therefore, 
    \[\forall t \in [T, \infty] \quad \text{we have} \quad \abs{y(t)} \leq Me^{\alpha t}.\]
    \begin{itemize}
        \item We prove \(\abs{\int_{0}^{T} y(t) e^{-st} \, dt} < \infty\).
        First, we note that since \(y(t)\) is a piecewise continuous function on \([0,T]\), there exists a constant \(K\) such that 
        \[\max_{t \in [0,T]} \abs{y(t)} \leq K.\]
        This implies that 
        \[\begin{aligned}
            \abs{\int_{0}^{T} y(t) e^{-st} \, dt} &\leq \int_{0}^{T} \abs{y(t) e^{-st}} \, dt \\
            &\leq \max_{t \in [0,T]} \abs{y(t) e^{-st}} \, dt \\
            &\leq \int_{0}^{T} \max_{t \in [0,T]}\abs{y(t)}\max_{t \in [0,T]} \abs{e^{-st}} \, dt \\
            &\leq \int_{0}^{T} \max_{t \in [0,T]}\abs{y(t)}\, dt \\
            &\leq \int_{0}^{T} K \, dt  \\
            &= TK \\
            &< \infty.
        \end{aligned}\]
        \item We prove \(\abs{\int_{0}^{\infty} y(t) e^{-st} \, dt} < \infty\).
        \[\begin{aligned}
            \abs{\int_{T}^{\infty} y(t) e^{-st} \, dt} &\leq \int_{T}^{\infty} \abs{y(t) e^{-st}} \, dt \\
            &\leq \int_{T}^{\infty} Me^{\alpha t} e^{-st} \, dt \\
            &\leq M \lim_{N \to \infty} \int_{T}^{N} e^{(\alpha-s)t} \, dt \\
            &=M \lim_{N \to \infty} \left[ \frac{e^{(\alpha-s)t}}{\alpha-s} \right]_T^N \\
            % &= \frac{M}{\alpha-s} \lim_{N \to \infty} \left( e^{(\alpha-s)N} - e^{(\alpha-s)T} \right) \\
            &= -\frac{M}{\alpha -s} e^{(\alpha-s)T} \\
            &< \infty.
        \end{aligned}\]
    \end{itemize}
\end{proof}

\subsection{The step function}

From now (unless stated otherwise), assume \(y(t)\) will be a piecewise continuous function on \([0,\infty)\), and it is of exponential order.

\begin{definition}
    Given \(a \in \RR\) such that \(a \geq 0\) let 
    \[\begin{aligned}
        u_a(t) = \begin{cases}
            0 &\text{if } t < a \\
            1 &\text{if } t \geq a.
        \end{cases}
    \end{aligned}\]
    We call this the \textbf{step function}.
\end{definition}

\begin{figure}[H]
     \begin{center}
         \include{./Resources/Step function.tex}
     \end{center}
     \caption{Graph of the step function}
\end{figure}

\begin{mdprop}
    Let \(f(t)\) be a piecewise continuous function of exponential order. Then 
    \[\CL \left[ f(t-a) \, u_a(t) \right](s) = e^{-as} \CL[f(t)](s).\]
    In particular,
    \[\CL\left[ u_a(t) \right](s) = \frac{e^{-as}}{s}.\]
\end{mdprop}

\begin{proof}
    Applying the definition of the Laplace transform: 
    \[\begin{aligned}
        \CL \left[ f(t-a) \, u_a(t) \right](s) &= \int_{0}^{\infty} f(t-a) \, u_a(t) e^{-st} \, dt \\ 
        &= \int_{0}^{a} f(t-a) e^{-st} \, dt + \int_{a}^{\infty} f(t-a) e^{-st} \, dt \\
        &= \int_{a}^{\infty} f(t-a) e^{-st} \, dt.
    \end{aligned}\]
    When \(t<a\), the first integral vanishes by the definition of the step function. Now, we change variable to \(z = t-a\) which gives 
    \[\begin{aligned}
        \int_a^{\infty} f(t-a) e^{-st} \, dt &= \int_{0}^{\infty} f(z) e^{-as-sz} \, dz \\
        &= e^{-as} \int_{0}^{\infty} f(z) e^{-sz} \, dz \\
        &= e^{-as} \CL[f(t)](s).
    \end{aligned}\]
    To prove that 
    \[\CL\left[ u_a(t) \right](s) = \frac{e^{-as}}{s}\]
    we apply the previous formula with \(f(t)=1\).
\end{proof}

\begin{corollary}
    The proposition above is equivalent to the statement 
    \[\CL[u_a(t)f(t)](s) = e^{-as}\CL[f(t+a)](s).\]
\end{corollary}

\begin{proof}
    Let \(g(t)=f(t+a)\) and \(f(t)=g(t-a)\) therefore,
    \[\CL[g(t)](s)=\CL[f(t+a)](s).\]
    We can write 
    \[\begin{aligned}
        \CL[u_a(t)f(t)](s) &= \CL[u_a(t)g(t-a)](s) \\
        &= e^{-as} \CL[g(t)](s) \\
        &= e^{-as}\CL[f(t+a)](s).
    \end{aligned}\]
\end{proof}

\begin{mdexample}
    Compute the Laplace transform of \(u_{2\pi}(t) \cos(t)\).
    \begin{solution}
        First note that by adding \(0\) we can write
        \[u_{2\pi}(t)\cos(t)= u_{2\pi}(t)\cos(t+2\pi-2\pi).\]
        Therefore, by using the formula in Proposition \(1.1\) the Laplace transform is as follows:
        \[\begin{aligned}
            \CL[u_{2\pi}(t)\cos(t+2\pi-2\pi)](s) &= e^{-2\pi s}\CL[\cos(t+2\pi)](s) \\
            &= e^{-2\pi s}\CL[\cos(t)](s) \\
            &= e^{-2\pi s} \frac{s}{s^2+1}.
        \end{aligned}\]
    \end{solution}
\end{mdexample}

\subsection{Properties of the Laplace transform}

\begin{theorem}
    Given functions \(f\) and \(g\) and a constant \(c \in \RR\),
    \[\begin{aligned}
        \CL[f+g] &= \CL[f]+\CL[g] \\
        \CL[cf] &= c \, \CL[f].
    \end{aligned}\]
    In other words, the Laplace transform is a \textbf{linear operator}.
\end{theorem}

\begin{theorem}
    The inverse Laplace transform is a linear operator.
\end{theorem}

\begin{proof}
    We prove the first property of linearity, 
    \[\begin{aligned}
        \CL\inv [f+g] &= \CL\inv \left( \CL \left( \CL\inv[f] \right) +\CL \left( \CL\inv[g] \right) \right)\\
        &= \CL\inv \left( \CL \left( \CL\inv[f]+\CL\inv[g] \right) \right) \\
        &= \CL\inv[f] +\CL\inv[g].
    \end{aligned}\]
    We prove the second property of linearity,
    \[\begin{aligned}
        \CL\inv[\alpha f ] &= \CL\inv \left( \alpha \CL\left( \CL\inv[f] \right) \right) \\
        &= \CL\inv\left( \CL\left( \alpha \CL\inv[f] \right) \right) \\
        &= \alpha\CL\inv[f].
    \end{aligned}\]
\end{proof}

\begin{example}
    Compute the Laplace transform of \(y(t)=c \in \RR\) for \(s >0\).
    \begin{solution}
        We previously computed the Laplace transform of \(1\) which is \(\CL[1](s)=\frac{1}{s}\) for \(s >0\). Using the linearity of the Laplace transform we have that 
        \[\CL[c](s) = c \, \CL[1](s) = \frac{c}{s}\]
        for \(s >0\).
    \end{solution}
\end{example}

\begin{mdthm}
    Given a function \(y(t)\) with Laplace transform, \(Y(s)=\CL[y(t)](s)\), the Laplace transform of \(\diff{y}{t}(t)\) is 
    \[\CL \left[ \diff[n]{y}{t}(t) \right] (s) = s^n \CL[y(t)](s) - \sum_{i=0}^{n-1} s^{n-i-1} \diff[i]{y}{t}(0),\]
    for \(n\geq 1\).
\end{mdthm}

\begin{mdcor}
    In particular, 
    \begin{itemize}
        \item the Laplace transform of \(\diff{y}{t}(t)\) is 
        \[\CL \left[ \diff{y}{t}(t) \right](s) = s\CL[y(t)](s)-y(0).\]
        \item the Laplace transform of \(\diff[2]{y}{t}(t)\) is 
        \[\CL \left[ \diff[2]{y}{t}(t)\right](s) = s^2 \CL[y(t)](s) - s \, y(0)- \diff{y}{t}(0).\]
    \end{itemize}
\end{mdcor}

\begin{mdexample}
    Let \(y(t)\) be a solution to the initial value problem 
    \[y'=y-4e^{-t} \quad \text{for } y(0)=1.\]
    Compute the Laplace transform of \(y(t)\).
    \begin{solution}
        We apply the Laplace transform to both sides of the equation i.e.
    \[\CL \left[ \diff{y}{t} \right](s) = \CL \left[ y-4e^{-t} \right](s).\]
    Using the properties of the Laplace transform we get 
    \[s Y(s) - y(0) = Y(s)-4\CL\left[ e^{-t} \right](s).\]
    Substituting the initial condition, \(y(0)=1\), the equation above becomes 
    \[sY(s)-1 = Y(s)-4\CL\left[ e^{-t} \right](s).\]
    Recall that \(\CL\left[ e^{at} \right](s) =\frac{1}{s-a}\). Applying this result with \(a=-1\) we obtain that 
    \[sY(s)-1= Y(s)-\frac{4}{s+1}.\]
    Rearranging for \(Y(s)\), we obtain 
    \[Y(s) = \frac{1}{s-1} -\frac{4}{(s-1)(s+1)}.\]
    \end{solution}
\end{mdexample}

% \begin{mdthm}[Uniqueness for inverse Laplace transform]
%     If \(f(t)\) is a continuous function with \(\CL[f(t)](s) = F(s)\), then \(f(t)\) is the \textbf{ONLY} continuous function whose Laplace transform is \(F(s)\).
% \end{mdthm}

\subsection{Solving \texorpdfstring{\(y''(t)+\lambda y(t)=0\)}{TEXT} for \texorpdfstring{\(\lambda>0\)}{TEXT}}

\begin{mdprop}
    Let \(\omega \neq 0\) then
    \[\CL\left[ \cos(\omega t) \right](s) = \frac{s}{s^2+\omega^2}\]
    and
    \[\CL\left[ \sin(\omega t) \right](s) = \frac{\omega}{s^2+\omega^2}.\]
\end{mdprop}

\begin{mdprop}
    Consider the ODE
    \[y''(t) + \lambda y(t)=0 \quad \text{for } \lambda>0.\]
    Then the general solution of this ODE is given by 
    \[y(t) = A \sin\left( t\sqrt{\lambda} \right) + B \cos\left( t\sqrt{\lambda} \right)\]
    for \(A,B \in \RR\).
\end{mdprop}

\begin{mdremark}
    By general solution we mean ``every solution can be written as''.
\end{mdremark}

\begin{proof}
    Assume \(y(t)\) is a solution to the ODE above. Applying Laplace transform to the differential equation we have that 
    \[\begin{aligned}
        0 &= \CL\left[ y''(t)+\lambda y(t) \right](s) \\
        &= \CL[y''(t)](s) +\lambda \CL[y(t)](s) \\
        &= s^2 Y(s)- sy(0)-y'(0) + \lambda Y(s).
    \end{aligned}\]
    Rearranging for \(\CL[y(t)](s) = Y(s)\), we have that 
    \[\begin{aligned}
        Y(s) &= \frac{s y(0)}{s^2+\lambda} + \frac{y'(0)}{s^2+\lambda} \\
        &= y(0) \frac{s}{s^2+\left( \sqrt{\lambda} \right)^2} + \frac{y'(0)}{\sqrt{\lambda}} \frac{\sqrt{\lambda}}{s^2+\left( \sqrt{\lambda} \right)^2},
    \end{aligned}\]
    this holds as \(\lambda>0\). Notice that 
    \[\CL\left[ y(0)\cos\left( t\sqrt{\lambda} \right) \right](s) = y(0)\frac{s}{s^2+\left( \sqrt{\lambda}^2 \right)}\]
    and that 
    \[\CL\left[ \frac{y'(0)}{\sqrt{\lambda}} \sin\left( t\sqrt{\lambda} \right) \right](s) =\frac{y'(0)}{\sqrt{\lambda}} \frac{\sqrt{\lambda}}{s^2+\left( \sqrt{\lambda} \right)^2}.\]
    Therefore, we can write 
    \[y(t) = y(0) \cos\left( t\sqrt{\lambda} \right)+\frac{y'(0)}{\sqrt{\lambda}}\sin\left( t\sqrt{\lambda} \right).\]
\end{proof}

\subsection{Solving \texorpdfstring{\(y''(t)+\lambda y(t)=0\)}{TEXT} for \texorpdfstring{\(\lambda<0\)}{TEXT}}

\begin{mdprop}
    Consider the ODE
    \[y''(t) + \lambda y(t)=0 \quad \text{for } \lambda<0.\]
    Then the general solution of this ODE is given by 
    \[y(t) = Ae^{t\sqrt{-\lambda}}+Be^{-t\sqrt{-\lambda}}\]
    for \(A,B \in \RR\).
\end{mdprop}

\begin{proof}
    Apply Laplace transform.
\end{proof}

\subsection{Convolution}

\begin{mdnote}
    The Laplace transform of the product of two functions is \textbf{not} the product of the related Laplace transforms.
\end{mdnote}

\begin{definition}
    Let \(f,g : [0,\infty) \to \RR\) be two integrable functions. Then the \textbf{convolution} of \(f\) and \(g\), denoted by \(f*g\), is the function 
    \[(f*g)(t) : = \int_{0}^{t} f(k)g(t-k) \, dk.\]
\end{definition}

\begin{theorem}
    Properties of the convolution: let \(c\) be a constant and \(f,g\) and \(h\) be functions then
    \begin{itemize}
        \item \(f*g=g*f\);
        \item \((cf)*g=f*(cg)=c(f*g)\);
        \item \((f*g)*h=f*(g*h)\).
    \end{itemize}
\end{theorem}

\begin{mdthm}
    Let \(f\) and \(g\) be piecewise continuous functions of exponential order, then 
    \[\CL[(f*g)(t)](s) = \CL[f(t)](s) \cdot \CL[g(t)](s).\]
\end{mdthm}

\begin{mdremark}
    This statement is equivalent to 
    \[(f*g)(t) = \CL\inv\{\CL[f(t)](s) \cdot \CL[g(t)](s)\}(t).\]
\end{mdremark}

\begin{mdnote}
    The \(\cdot\) is to emphasise the multiplication.
\end{mdnote}

\begin{proof}
    Let \(F(s) = \CL[f(t)](s)\) and \(G(s) = \CL[g(t)](s)\). From the definition of the Laplace transform we have:
    \[F(s) = \int_{0}^{\infty} f(k) e^{-sk} \, dk \quad \text{and} \quad G(s) = \int_{0}^{\infty} g(u) e^{-su} \, du.\]
    The product of \(F(s)\) and \(G(s)\) is given by 
    \[\left( \int_{0}^{\infty} f(k)e^{sk} \, dk \right)\left( \int_{0}^{\infty} f(u)e^{-su} \,du \right).\]
    Since first integral does not depend on \(u\), we can write the product as a double integral: 
    \[F(s)G(s) = \int_{0}^{\infty} \int_{0}^{\infty} f(k)g(u) e^{-s(k+u)} \, dk du.\]
    Changing variable to \(t= k+u\) for each fixed \(u\). So, \(dt= dk\) and that \(k=t-u\). We obtain 
    \[\begin{aligned}
        F(s)G(s) &= \int_{0}^{\infty} \int_{u}^{\infty} f(t-u)g(u) e^{-st}\, dt du \\
        &= \int_{0}^{\infty} \int_{0}^{t} f(t-u)g(u) e^{-st} \, du dt.
    \end{aligned}\]
    (Note that the domain of integration changes when switching the order of the integrals). Finally, isolating the terms that contain \(u\), we get 
    \[\begin{aligned}
        F(s)G(s) &= \int_{0}^{\infty} \int_{0}^{t} f(t-u)g(u) \, du \, e^{-st} \, dt \\
        &= \int_{0}^{\infty} (f*g)(t) e^{-st} \, dt \\
        &= \CL[(f*g)(t)](s).
    \end{aligned}\]
\end{proof}

\begin{mdexample}
    Suppose we have the function defined by 
    \[\frac{1}{(s+1)s^2} = \frac{1}{s+1} \cdot \frac{1}{s^2}.\]
    We recognise the entries as 
    \[\CL\inv\left[ \frac{1}{s+1} \right] = e^{-t} \quad \text{and} \quad \CL\inv\left[ \frac{1}{s^2} \right] =t.\]
    Therefore, 
    \[\begin{aligned}
        \CL\inv\left[ \frac{1}{s+1} \cdot \frac{1}{s^2} \right] &= \CL\inv[\CL[e^{-t}] \cdot \CL[t]] \\
        &= \CL\inv\left[ \CL[(e^{-t}*t)(t)] \right] \\
        &= (e^{-t}*t)(t) \\
        &= \int_{0}^{t} e^{-v}(t-v) \, dv \\
        &= e^{-t} +t-1.
    \end{aligned}\]
\end{mdexample}

\subsection{The Dirac delta function}

\begin{definition}
    The \textbf{Dirac delta function} is defined by the following properties
    \[\delta(t) = 0 \text{ when } t \neq 0,\]
    and 
    \[\int_{-\infty}^{\infty} \delta(t) \, dt =1.\]
\end{definition}

Given \(b>0\), define 
\[\begin{aligned}
    g_b(t) = \begin{cases}
        \frac{1}{2b} &\text{if } -b \leq t \leq b \\
        0 &\text{otherwise}.
    \end{cases}
\end{aligned}\]
Then, one can think of the \(\delta\)-function as
\[\delta(t) = \lim_{b \to 0} g_b(t)\]
and 
\[\delta(t-a) = \lim_{b \to 0} g_b(t-a).\]
This limit is zero for all values \(t\) except at \(t=a\), where it is infinite.

\begin{theorem}
    \[\int_{-\infty}^{\infty} \delta(t-a)f(t) \, dt := \lim_{b\to 0} \int_{-\infty}^{\infty} g_b(t-a)f(t) = f(a).\]
\end{theorem}

\begin{mdthm}
    The Laplace transform of the Dirac delta function (for \(a>0\)) is 
    \[\begin{aligned}
        \CL[\delta(t-a)](s) &:= \lim_{b \to 0} \CL[g_b(t-a)](s) \\
        &= e^{-as}.
    \end{aligned}\]
\end{mdthm}

\begin{mdcor}
    For \(a=0\) the Laplace transform of \(\delta(t)\) is defined as 
    \[\begin{aligned}
        \CL[\delta(t)](s) &:= \lim_{a \to 0} \CL[\delta(t-a)](s) \\
        &=1.
    \end{aligned}\]
\end{mdcor}

\subsection{\texorpdfstring{\(2^{\text{nd}}\)}{TEXT} order linear ODEs with constant coefficients}

\begin{definition}
    A second order linear differential equation with constant coefficients is one of the form 
    \[ay''+by'+cy=g(t)\]
    for \(a,b,c \in \RR\) and \(g: I \subset \RR \to \RR\).
\end{definition}

\begin{definition}
    The associated equation 
    \[ay''+by'+cy=0\]
    is called the \textbf{homogeneous equation}.
\end{definition}

\begin{definition}
    Let \(\xi(t)\) be the solution of the initial value problem 
    \[\begin{aligned}
        ay''+by'+cy=\delta(t) \qquad y(0)&=0 \\ 
        y'(0)&=0.
    \end{aligned}\]
    The function \(\xi(t)\) is called the \textbf{impulse response}.
\end{definition}

\begin{corollary}
    Let \(\xi(t)\) be the impulse response. Then 
    \[\CL[a\xi ''(t)+b\xi '(t)+c\xi(t)](s) = \CL[\delta(t)](s),\]
    and applying the properties of the Laplace transform, and the initial condition gives that 
    \[\CL[\xi(t)](s) = \frac{1}{as^2+bs+c}.\]
\end{corollary}

\begin{mdthm}
    Consider the following initial value problem,
    \[\begin{aligned}
        ay''+by'+cy =g(t) \qquad y(0)&=0 \\
        y'(0)&=0.
    \end{aligned}\]
    The unique solution is 
    \[\begin{aligned}
        y(t) &= (\xi * g)(t) \\
        &= \int_{0}^{t} \xi(t-k)g(k) \, dk.
    \end{aligned}\]
\end{mdthm}

\begin{proof}
    Applying the Laplace transform on both sides, we have:
    \[\begin{aligned}
        \CL[ay''+by'+cy] &= \CL[g(t)] \\
        s^2\CL[y]-sy(0)-y'(0)+s\CL[y]-y(0)+c\CL[y] &= \CL[g(t)] \\
        s^2\CL[y] +s\CL[y]+c\CL[y] &= \CL[g(t)] \\
        (s^2+s+c)\CL[y] &= \CL[g(t)].
    \end{aligned}\]
    Therefore,
    \[\begin{aligned}
        \CL[y] &= \CL[g(t)] \cdot \frac{1}{s^2+s+c} \\
        &= \CL[g(t)](s) \cdot \CL[\xi(t)](s).
    \end{aligned}\]
\end{proof}

\begin{mdcor}
    Consider the following the initial value problem 
    \[\begin{aligned}
        ay''+by'+cy =g(t) \qquad y(0) &=y_0 \\
        y'(0)&=y_0.
    \end{aligned}\]
    The solution is 
    \[\begin{aligned}
        y(t) &= (\xi * g)(t)+\wh{y}(t) \\
        &= \int_{0}^t \xi(t-k)g(k) \, dk + \wh{y}(t),
    \end{aligned}\]
    where \(\wh{y}(t)\) is the solution of 
    \[\begin{aligned}
        ay''+by'+cy = 0 \qquad y(0)&= y_0 \\
         y'(0)&= y'_0.
    \end{aligned}\]
\end{mdcor}

\subsection{\texorpdfstring{\(2^{\text{nd}}\)}{TEXT} order linear homogeneous ODEs with constant coefficients}

\begin{definition}
    Given a second order linear homogeneous equation with constant i.e.,
    \[ay''+by'+cy=0\]
    for \(a,b,c \in \RR\), the equation 
    \[ar^2+br+c = 0\]
    is called the \textbf{characteristic equation}.
\end{definition}

\begin{mdthm}
    Let \(r_1\) and \(r_2\) be the roots of the characteristic equation.
    \begin{enumerate}
        \item If \(r_1\) and \(r_2\) are distinct and real (when \(b^2-4ac>0\)) then, the characteristic equation has general solution 
        \[y= C_1e^{r_1 t}+C_2 e^{r_2 t}\]
        where \(C_1,C_2 \in \RR\).
        \item If \(r_1 = r_2\) (happens when \(b^2 -4ac=0\)), then the characteristic equation has the general solution 
        \[y=(C_1+C_2 t)e^{r_1 t}\]
        where \(C_1,C_2 \in \RR\).
        \item If \(r_1\) and \(r_2\) are complex roots of the form \(\alpha\pm i\beta\) (when \(b^2-4ac<0\)), then the general solution to the characteristic equation is 
        \[y = C_1 e^{\alpha x} \cos(\beta x)+C_2 e^{\alpha x} \sin(\beta x)\]
        where \(C_1,C_2 \in \RR\).
    \end{enumerate}
\end{mdthm}


\subsection{``Uniqueness'' of the Laplace transform}

\begin{mdthm}
    If \(f(t)\) is a \textbf{continuous} function with \(\CL[f(t)](s)=F(s)\), then \(f(t)\) is the \textbf{only} continuous function whose Laplace transform is \(F(s)\).
\end{mdthm}

\begin{theorem}
    If \(h\) and \(g\) are piecewise continuous functions with \(\CL[h] = \CL[g]\), then \(h=g\) except possibly at he points of discontinuity.
\end{theorem}

\pagebreak

\section{Picard's theorem}

\begin{mdthm}[Picard's theorem -- existence and uniqueness]
    Let \(R \subset \RR^2\) be a closed rectangle of the form 
    \[R := \{(t,y) : a \leq t, c \leq y \leq d\},\]
    for \(a,b,c,d \in \RR\) and let 
    \[\begin{aligned}
        f(t,y) &: R \to \RR \\
        \diffp{}{y}f(t,y) &: R \to \RR
    \end{aligned}\]
    be continuous functions. Let \((t_0,y_0) \in (a,b) \times (c,d)\) be a point in the open rectangle. Then there exists \(\eps>0\) and 
    \[y(t) : (t_0-\eps,t_0+\eps) \to \RR\]
    such that \(y\) is the unique solution of the initial value problem 
    \[\diff{y}{t}=f(t,y), \quad \text{for } y(t_0)=y_0\]
    in the interval \((t_0 - \eps,t_0 +\eps)\).
\end{mdthm}

\begin{mdnote}
    In essence, if \(f(t,y)\) and \(\diffp{}{y}f(t,y)\) are continuous functions then the initial value problem 
    \[\diff{y}{t}=f(t,y), \quad \text{for } y(t_0)=y_0\]
    has only one unique solution, \(y\).
\end{mdnote}

\subsection{Proof of the existence}

The proof of the existence part of Picard's theorem is also known as \textbf{Picard's Iteration method}, which provides us a method to find a solution.

\subsection{PLACEHOLDER TITLES BELOW}

\subsection{Picard's iteration method}

\begin{mdthm}[Picard's iteration]
    Suppose we have an initial value problem 
    \[\diff{y}{t} = f(t,y) \quad \text{for } y(t_0)=y_0.\]
    Then, the solutions are given by 
    \[\begin{aligned}
        y_0(t) &:= y_0 \\
        y_1(t) &:= y_0 +\int_{t_0}^{t} f(u,y_0(u)) \, du \\
        &\vdots  \\
        y_k(t) &:= y_0 + \int_{t_0}^{t} f(u,y_{k-1}(u)) \, du,
    \end{aligned}\]
    for any \(k \in \NN\).
\end{mdthm}

\begin{definition}
    Let \(I \subset \RR\) and let 
    \[\phi_n: I \to \RR \quad \text{for } n \in \NN,\]
    be a sequence of functions. We say that \(\phi_n\) is \textbf{uniformly convergent} on \(I\) to \(\phi :I \to \RR\) if for any \(M\) there exists \(n_M \in \NN\) such that 
    \[\sup_I \abs{\phi-\phi_n} < M.\]
\end{definition}

\begin{lemma}
    Let \(I \subset \RR\) be a bounded interval and let 
    \[\phi_n: I \to \RR \quad \text{for } n \in \NN,\]
    be a sequence of integrable functions uniformly converging on \(I\) to \(\phi:I \to \RR\). Then,
    \[\lim_{n\to \infty} \int_I \phi_n = \int_I \phi.\]
\end{lemma}

\section{SOME NONSENSE TO CHANGE ABOVE}

\section{Power series methods}

In this section we illustrate solution to the second order linear homogeneous ODE of the form 
\[P(x) y''+Q(x)y'+R(x)y=0\]
where \(P(x),Q(x)\) and \(R(x)\) are analytic functions at \(c \in \RR\). From now on unless stated otherwise, we will consider ODEs of this form.

\begin{mdnote}
    By analytic, we mean there exists a power series expansion of each respective function at the point \(x = c \in \RR\).
\end{mdnote}

\subsection{Ordinary points}

\begin{definition}
    Consider the ODE from above, if \(P(c)\neq0\) then \(c\) is called an \textbf{ordinary point}.
\end{definition}

\begin{definition}
    If \(P(c)=0\) (and either \(Q(c)\) or \(R(c)\) is different from zero) then \(c\) is called a \textbf{singular point}.
\end{definition}

\begin{definition}
    Two solution of an ODE \(y_1\) and \(y_2\) are said to be \textbf{linearly independent} if there are \(\alpha,\beta \in \RR\) such that 
    \[\alpha y_1(t)+\beta y_2(t) =0\]
    if and only if \(\alpha=\beta=0\).
\end{definition}

\begin{mdthm}
    Suppose \(x=c\) is an ordinary point of the ODE then, the ODE has \\ \underline{two linearly independent}  analytic solution of the form:
    \[y = \sum_{n=0}^{\infty} a_n (x-c)^n.\]
    Furthermore, the radius of convergence is at least as large as the distance from \(c\) to the nearest singular point (real or complex-valued) of the ODE.
\end{mdthm}

\begin{mdremark}
    If there are no singular points then the radius of convergence is infinite.
\end{mdremark}

% \begin{example}[Motivating example]
%     Consider Airy's equation:
%     \[y''-xy=0\]
%     near the point \(x_0=0\). Note that \(x_0=0\) is an ordinary point. Let
%     \[y = \sum_{k=0}^{\infty} a_k x^k\]
%     be a solution the ODE. Differentiating twice, we have 
%     \[y''=\sum_{k=2}^{\infty} k(k-1)a_k x^{k-2};\]
%     now substituting into the ODE we obtain:
%     \[\begin{aligned}
%         0 = y''-xy &= \sum_{k=2}^{\infty} k(k-1)a_k x^{k-2} - x\sum_{k=0}^{\infty} a_k x^k \\
%         &= \sum_{k=2}^{\infty} k(k-1)a_k x^{k-2} - \sum_{k=0}^{\infty} a_k x^{k+1}.
%     \end{aligned}\]
%     \begin{mdthm}
%         TO FINISH
%     \end{mdthm}
% \end{example}

\subsection{Regular singular points}

\begin{definition}
    Let \(x=c\) be a singular point of the ODE. If 
    \[\lim_{x\to c}(x-c) \frac{Q(x)}{P(x)} \quad \text{and} \quad \lim_{x\to c} (x-c)^2 \frac{R(x)}{P(x)}\]
    are both \underline{finite} then, \(x=c\) is called a \textbf{regular singular point}. Otherwise, is called an \textbf{irregular singular point}.
\end{definition}

\begin{mdremark}
    Since \(x=c\) is a singular point, at least one of the functions \(\frac{Q(x)}{P(x)}\) or \(\frac{R(x)}{P(x)}\) blows up at \(x=c\) and in particular they are not analytic.
\end{mdremark}

\begin{mdexample}
    Consider the equation 
    \[(x-2)^2(x-1)^2y''+(x-1)y'+5y=0.\]
    We can identify 
    \[\begin{aligned}
        P(x) &= (x-2)^2(x-1)^2 \\
        Q(x) &= x-1 \\
        R(x) &= 5.
    \end{aligned}\]
    Since \(P(1)=P(2)=0\) and \(R\) is never zero then \(x=1,2\) are the only singular points of the ODE. We have that 
    \[\begin{aligned}
        \lim_{x\to 2} (x-2)\frac{Q(x)}{P(x)} &= \lim_{x\to 2} (x-2)\frac{(x-1)}{(x-1)^2(x-2)^2} \\
        &=  \infty
    \end{aligned}\]
    so, \(x=2\) is an irregular singular point. Whereas, at \(x=1\) we have 
    \[\begin{aligned}
        \lim_{x\to 1} \frac{Q(x)}{P(x)} &= \lim_{x\to 1} (x-1)\frac{x-1}{(x-1)^2(x-2)^2} \\
        &= 1
    \end{aligned}\]
    and 
    \[\begin{aligned}
        \lim_{x\to 1} (x-1)^2 \frac{R(x)}{P(x)} &= \lim_{x\to 1} (x-1)^2 \frac{5}{(x-1)^2(x-2)^2} \\
        &=5.
    \end{aligned}\]
    Since they are both finite we have that \(x=1\) is a regular singular point.
\end{mdexample}

\subsection{Euler equations}

\begin{definition}
    The \textbf{Euler equation} is an ODE of the form 
    \[x^2y''+\alpha xy'+\beta y=0.\]
\end{definition}

\begin{mdremark}
    The solutions presented in this section to the Euler equation are only valid for \(x>0\).
\end{mdremark}

\begin{definition}
    Given an Euler equation the following equation 
    \[r(r-1)+\alpha r+\beta=0,\]
    is called the \textbf{indicial equation}.
\end{definition}

\subsubsection{Real and distinct roots}

\begin{mdthm}
    If the indicial equation has real and distinct roots, \(r_1\) and \(r_2\), then the general solution to the Euler equation is given by 
    \[y(x) = C_1 x^{r_1}+C_2 x^{r_2}\]
    where \(C_1,C_2 \in \RR\) and \(x>0\).
\end{mdthm}

\begin{example}
    Solve the following ODE
    \[2x^2y''+3xy'-y=0 \quad \text{for } x>0.\]
    Clearly, this is an Euler equation since we can write the ODE as 
    \[x^2y''+\frac{3}{2}xy'-\half y-0.\]
    The corresponding indicial equation is \(r(r-1)+\frac{3}{2}r-\half=0\). Equivalently,
    \[\begin{aligned}
        0= 2r(r-1)+3r-1 &= 2r^2+r-1 \\
        &= (2r-1)(r+1).
    \end{aligned}\]
    Therefore, the roots of the indicial equation are \(r_1 = \half\) and \(r_2=-1\), we conclude the general solution of the Euler equation is 
    \[y=C_1 x^{\half} +C_2 x\inv \quad \text{for } x>0.\]
\end{example}

\subsubsection{Complex roots}

\begin{mdthm}
    If the indicial equation has complex roots, \(r_1= \alpha +i\beta\) and \(r_2=\alpha-i\beta\), where \(\alpha,\beta \in \RR\). Then the general solution to the Euler equation is given by 
    \[y(x) = C_1 e^{\alpha \ln(x)}\cos(\beta\ln x)+C_2 e^{\alpha \ln(x)}\sin(\beta \ln x)\]
    where\(C_1,C_2 \in \RR\) and \(x>0\).
\end{mdthm}

\begin{example}
    Consider the Euler equation:
    \[x^2y''+xy+y=0 \quad \text{for } x>0.\]
    The corresponding indicial equation is 
    \[r(r-1)+r+1 = r^2+1 =0.\]
    Therefore, the roots are \(r_1=i\) and \(r_2 =-i\) which implies the general solution of the Euler equation is given by 
    \[y(x)=C_1 \cos(\ln x)+C_2\sin(\ln x) \quad \text{for } x>0.\]
\end{example}

\subsubsection{Equal roots}

\begin{mdthm}
    If the indicial equation has a repeated root \(r_1\) then the general solution to the Euler equation is given by
    \[y(x)=C_1 x^{r_1}+C_2 x^{r_1} \ln(x)\]
    where \(C_1,C_2 \in \RR\) and \(x>0\).
\end{mdthm}

\begin{example}
    Consider the Euler equation 
    \[x^2y''+5xy'+4y=0.\]
    The corresponding indicial equation is 
    \[\begin{aligned}
        0= r(r-1)+5r+4 &= r^2+4r+4 \\
        &= (r+2)^2.
    \end{aligned}\]
    Therefore, \(r_1=r_2=-2\) which implies the general solution is 
    \[y(x)=x^{-2}(C_1+C_2\ln x) \quad \text{for } x>0.\]
\end{example}

\subsection{Frobenius method}

\begin{definition}
    Let \(x=c\) be a regular singular point and let 
    \[p_0 := \lim_{x\to c}(x-c) \frac{Q(x)}{P(x)} \quad \text{and} \quad q_0:= \lim_{x\to c}(x-c)^2 \frac{R(x)}{P(x)}.\]
    The equation 
    \[r(r-1)+p_0r+q_0=0\]
    is called the \textbf{indicial equation} at \(x=c\).
\end{definition}

\begin{example}
    Consider the equation
    \[(x-2)^2(x-1)^2y''+(x-1)y'+5y=0.\]
    From a previous example we know that 
    \[\lim_{x \to 1}(x-1)\frac{Q(x)}{P(x)} =1 \quad \text{and} \quad \lim_{x\to 1} (x-1)^2\frac{R(x)}{P(x)}=5.\]
    Therefore, the indicial equation at \(x=1\) is 
    \[r(r-1)+r+5=0.\]
\end{example}

\begin{mdthm}
    Let \(x=0\) be a regular singular point of the ODE. Suppose that \(r_1,r_2 \in \RR\) with \(r_1 \geq r_2\) are solutions of the indicial equation. Then there exists a solution of the form 
    \[y(x) = x^{r_1} \sum_{k=0}^{\infty} a_k x^k \quad x>0,\]
    with \(a_0 \neq 0.\)
\end{mdthm}

\subsection{Power series}

\begin{definition}
    A \textbf{power series} is an expression of the form 
    \[\sum_{n=0}^{\infty} a_n (x-c)^n,\]
    where \(a_n\) and \(c\) are constants.
\end{definition}

\begin{mdremark}
    For \(x=c\) the series always converge to \(a_0\).
\end{mdremark}

\begin{definition}
    We say that a power series \textbf{converges absolutely} at \(x\) whenever the limit 
    \[\lim_{N\to \infty} \sum_{k=0}^{N} \abs{a_k} \abs{x-c}^k\]
    exists. That is the series \(\sum_{k=0}^{\infty} \abs{a_k} \abs{x-c}^k\) is convergent.
\end{definition}

\begin{definition}
    Given a power series there exists a number \(R\in[0,\infty)\) called the \textbf{radius of convergence} if the power series converges absolutely for any \(x \in (c-R,c+R)\). Otherwise, it does not converge absolutely.
\end{definition}

\begin{mdthm}[Cauchy-Hadamard formula]
    The radius of convergence is 
    \[\frac{1}{R} = \limsup_{n \to \infty} \abs{a_n}^{\frac{1}{n}}.\]
\end{mdthm}

\begin{mdremark}
    A key property of power series is that they can be differentiated term by term, added and multiplied together, within the radius of convergence.
\end{mdremark}

\begin{definition}
    Let \(f(x)\) be a smooth function at \(x=c\) and let 
    \[\sum_{k=0}^{\infty} \frac{f^{(k)}(c)}{k!}(x-c)^k\]
    be its Taylor expansion at \(c\). If its Taylor expansion has radius of converge \(R>0\) then, the function is said to be an \textbf{analytic function} at \(x=c\).
\end{definition}

\begin{mdexample}
    Let \(y = \sum_{n=0}^{\infty} a_n x^n\) be the power series solution about \(x=0\) of the initial value problem 
    \[\begin{aligned}
        2y''+xy'+y=0 \quad y(0) &= 1 \\
        y'(0) &=0.
    \end{aligned}\]
    Find the value of \(a_0,a_1,a_2\) and \(a_3\). 
    \begin{solution}
        Since the solution is a power series we know it must be a Taylor series. As such each 
        \[a_n = \frac{f^{(n)}(0)}{n!}.\]
        Using this we determine that, \(a_0 = y(0)=1\) and \(a_1=y'(0)=0\). From the ODE we can write 
        \[y''=\frac{-xy'-y}{2},\]
        which implies \(y''(0)=-\half\) hence, \(a_2 = -\frac{1}{4}\). To find \(a_3\) we take a derivative of the ODE and obtain 
        \[2y'''+y'+xy''+y'=0\]
        and conclude \(a_3=0\).
    \end{solution}
\end{mdexample}

\section{Heat equation}

\begin{definition}
    The \textbf{heat equation} for a wire of length \(L>0\) takes the form of 
    \[\diffp{}{t}u(x,t)=k\diffp[2]{}{x}u(x,t)\]
    where \(k>0\) and \(u(x,t)\) represents the temperature of the wire at the position \(x \in (0,L)\).
\end{definition}

\begin{mdremark}
    In \(\RR^3\) the heat equation becomes of the form:
    \[\diffp{}{t}u(x,y,z,t)=k\left( \diffp[2]{}{x}+\diffp[2]{}{y}+\diffp[2]{}{z} \right)u(x,y,z,t)\]
\end{mdremark}

\begin{theorem}
    If \(u(x,t)\) and \(v(x,t)\) are solutions to the heat equation then 
    \[\alpha u(x,t)+\beta v(x,t)\]
    for \(\alpha,\beta \in \RR\) is also a solution.
\end{theorem}

\begin{definition}
    Related to the heat equation there are also \textbf{boundary conditions} (BC). These take the form of:
    \begin{itemize}
        \item \textbf{homogeneous} BC, where 
        \[u(0,t)=u(L,t)=0 \quad \forall t\geq 0;\]
        \item \textbf{insulated ends} or \textbf{Neumann} BC, where 
        \[\diffp{}{x}u(0,t)=\diffp{}{x}u(L,t)=0 \quad \forall t\geq 0.\]
    \end{itemize}
\end{definition}

\begin{mdremark}
    We can have other types of boundary conditions which take on a more complicated form, for example:
    \[u(0,t) = e^t \quad \text{and} \quad u(L,t)=\sin(t) \quad \forall t\geq 0.\]
\end{mdremark}

\begin{definition}
    The \textbf{initial condition} of the heat equation is defined as 
    \[u(x,0)=f(x) \quad \text{for } x \in [0,L]\]
    for an \(f : [0,L] \to \RR\).
\end{definition}

\subsection{Homogeneous boundary conditions}

In this section we show methods to solve the heat equation with the following conditions:

\[\begin{aligned}
    \begin{cases}
        u_t = ku_{xx} & x \in (0,L), \quad t,k>0 \\
        u(0,t)=u(L,t)=0 & \text{(Homogeneous BC)} \\
        u(x,0)=f(x) & \text{(initial condition)},
    \end{cases}
\end{aligned}\]
where \(f:[0,L] \to \RR\) is a continuous function such that \(f'\) is piecewise conditions and \(f(0)=f(L)=0\).

\begin{mdthm}[Unique solution to HE with homogeneous BC]
    The \underline{\textbf{unique}} solution to the heat equations with the conditions specified above is 
    \[\begin{aligned}
        u(x,t) &= \sum_{n=1}^{\infty} c_n \sin\left( \frac{\pi n}{L} x \right)\exp\left( -k\left( \frac{\pi n}{L} \right)^2t \right) \\
        &=\sum_{n=1}^{\infty} c_n \sin\left( \frac{\pi n}{L} x \right) e^{-k\left( \frac{\pi n}{L} \right)^2 t}
    \end{aligned}\]
    where 
    \[c_n := \frac{2}{L} \int_{0}^{L} f(x) \sin\left( \frac{\pi n}{L} x \right) \, dx.\]
\end{mdthm}

\begin{proof}[Sketch proof]
    Need to check that we can differentiate the power series term by term in respect to both \(x\) and \(t\) i.e. the power series converges absolutely and uniformly on \([0,L]\).
\end{proof}

\begin{corollary}
    We have that \(u(x,t)=0\) is a solution to the heat equation with homogeneous BC \(\iff\) \(f(x)=0\). We call this the \textbf{trivial solution}.
\end{corollary}

\begin{mdexample}
    Find a solution to the following heat conduction problem:
    \[\begin{aligned}
        \begin{cases}
            u_t = 7u_{xx} & x \in (0,\pi)\quad t>0 \\
            u(0,t)=u(\pi,t)= 0 & \text{(homogeneous BC)}\\
            u(x,0)=3\sin(2x)-6\sin(5x) & \text{(initial condition)}.
        \end{cases}
    \end{aligned}\]
    \begin{solution}
        By the theorem there exists a unique solution to the problem and is given by 
        \[\begin{aligned}
            u(x,t) &= \sum_{n=1}^{\infty} c_n \sin(n x) \exp(-7n^2 t) \\
            &=\sum_{n=1}^{\infty} c_n \sin(n x) e^{-7n^2 t}.
        \end{aligned}\]
        To find the coefficients \(c_n\) we can evaluate the integral, but it is easier to impose the initial condition; we note that 
        \[\begin{aligned}
            u(x,0) &= \sum_{n=1}^{\infty} c_n \sin(nx) \\
            &= 3\sin(2x)-6\sin(5x),
        \end{aligned}\]
        by comparing the LHS and RHS we have that \(c_2 =3 , c_5 =-6\) and the remaining \(c_n =0\). Therefore, the solution to the problem is given by 
        \[u(x,t)= 3e^{-28t}\sin(2x)-6e^{-175t}\sin(5x).\]
    \end{solution}
\end{mdexample}

\subsection{Fourier series and the initial condition}

In this section we address the problems related to the convergence of the series 
\[\sum_{n=1}^{\infty} a_n \sin \left( \frac{n\pi}{L} x\right)\]
to the initial condition.

\begin{proposition}
    Let \(h(x) : [-L,L] \to \RR\) and let 
    \[\half a_0 + \sum_{n=1}^{\infty} \left[ a_n \cos\left( \frac{n\pi}{L}x \right)+ b_n \sin \left( \frac{n\pi}{L}x \right) \right]\]
    be a function series converging uniformly to \(h\). Then,
    \[a_n = \frac{1}{L} \int_{-L}^L h(x) \cos\left( \frac{n\pi}{L}x \right) \, dx \quad \text{for } n \in \NN \cup \{0\}\]
    and
    \[b_n =\frac{1}{L} \int_{-L}^L h(x) \sin \left( \frac{n\pi}{L}x \right) \, dx \quad \text{for } n \in \NN.\]
\end{proposition}

\begin{mdlemma}
    We have the following results.
    \begin{itemize}
        \item For \(n,m \in \NN \cup \{0\}\)
        \[\frac{1}{L} \int_{-L}^L \sin \left( \frac{n\pi}{L}x \right) \cos\left( \frac{m\pi}{L}x \right) \, dx =0.\]
        \item For \(m,n \in \NN\),
        \[\frac{1}{L} \int_{-L}^{L} \sin\left( \frac{n\pi}{L}x\right)\sin\left( \frac{m\pi}{L}x \right) \, dx= \begin{cases}
            0 & \text{if } n\neq m \\
            1 & \text{if } n=m.
        \end{cases}\]
        \item For \(m,n \in \NN \cup \{0\}\),
        \[\frac{1}{L} \int_{-L}^{L} \cos\left( \frac{n\pi}{L}x \right) \cos\left( \frac{m\pi}{L}x \right) \, dx =\begin{cases}
            0 & \text{if } n\neq m, \\
            1 & \text{if } n=m\neq 0, \\
            2 & \text{if } n=m=0.
        \end{cases}\]
    \end{itemize}
\end{mdlemma}

\begin{mdlemma}
    Some properties of the trigonometric functions:
    \begin{itemize}
        \item \(\cos(-x)=\cos(x)\) (\(\cos(x)\) is an even function);
        \item \(\sin(-x)=-\sin(x)\) (\(\sin(x)\) is an odd function);
        \item \(\cos(n\pi)=(-1)^n\);
        \item \(\sin(n\pi)=0\).
    \end{itemize}
\end{mdlemma}

\begin{mdremark}
    Even functions: \(f(x)=f(-x)\). \\
    Odd functions: \(-f(x)=f(-x)\).
\end{mdremark}

\begin{mdexample}
    Finding Fourier series of a function.
\end{mdexample}

\begin{definition}
    Let \(h(x):[-L,L] \to \RR\), the infinite sum
    \[\half a_0 + \sum_{n=1}^{\infty} \left[ a_n \cos\left( \frac{n\pi}{L}x \right) +b_n \sin\left( \frac{n\pi}{L}x \right)\right]\]
    with 
    \[a_n = \frac{1}{L} \int_{-L}^{L} h(x)\cos\left( \frac{n\pi}{L}x \right) \, dx \quad \text{for } n \in \NN \cup \{0\}\]
    and 
    \[b_n = \frac{1}{L} \int_{-L}^L h(x)\sin\left( \frac{n\pi}{L}x \right) \quad \text{for } n \in \NN,\]
    is called the \textbf{Fourier series} of \(h(x)\).
\end{definition}

\begin{definition}
    Let \(h(x) : [0,L] \to \RR\), the infinite sum 
    \[\sum_{n=1}^{\infty} a_n \sin \left( \frac{n\pi}{L}x \right)\]
    with 
    \[a_n = \frac{2}{L} \int_{0}^{L} h(x) \sin \left( \frac{n\pi}{L}x \right) \, dx \quad \text{for } n \in \NN,\]
    is called the \textbf{Fourier sine series} of \(h(x)\).
\end{definition}

\begin{definition}
    Let \(h(x):[0,L] \to \RR\), the infinite sum 
    \[\half a_0 + \sum_{n=1}^{\infty} a_n \cos \left( \frac{n\pi}{L}x \right)\]
    with 
    \[a_n = \frac{2}{L} \int_0^L h(x) \cos\left( \frac{n\pi}{L}x \right) \, dx \quad \text{for } n \in \NN \cup \{0\},\]
    is called the \textbf{Fourier cosine series} of \(h(x)\).
\end{definition}

\begin{theorem}
    Let \(h,h' : [-L,L] \to \RR\) be piecewise continuous functions and let 
    \[\half a_0 + \sum_{n=1}^{\infty} \left[ a_n \cos\left( \frac{n\pi}{L}x \right)+b_n \sin \left( \frac{n\pi}{L}x \right) \right]\]
    be the Fourier series of \(h\). Then, for any \(x \in (-L,L)\) we have 
    \[\half a_0 + \sum_{n=1}^{\infty} \left[ a_n \cos\left( \frac{n\pi}{L}x \right)+b_n \sin \left( \frac{n\pi}{L}x \right) \right] = \half \left[ h(x^+)+h(x^-) \right].\]
    For \(x = \pm L\), the series converges to 
    \[\half \left[ h((-L)^+)+h(L^-) \right].\]
    Here,
    \[h(x^+) = \lim_{\delta \to \infty} h\left( x+\frac{1}{\delta} \right) \quad \text{and} \quad h(x^-)=\lim_{\delta\to\infty} h\left( x-\frac{1}{\delta} \right).\]
    Furthermore, if \(h\) is continuous at \(x\) then, \(h(x^+)=h(x^-)\).
\end{theorem}

\begin{corollary}
    If \(h\) is continuous with \(h(0)=h(L)\) and \(h'':[-L,L] \to \RR\) is piecewise continuous then, the Fourier series of \(h'(x)\) can be obtained by term wise differentiation. Namely, the Fourier series of \(f'(x)\) is 
    \[\sum_{n=1}^{\infty} \frac{\pi n}{L} \left[ -a_n \sin\left( \frac{n\pi}{L}x \right)+b_n \cos\left( \frac{n\pi}{L}x \right)\right].\]
\end{corollary}

\begin{definition}
    Given \(h:[0,L] \to \RR\), let \(\wh{h}_{\text{odd}}:[-L,L] \to \RR\) be the \textbf{odd extension} of \(h\). That is, 
    \[\wh{h}_{\text{odd}}(x) =\begin{cases}
        h(x) & \text{for } x \in [0,L] \\
        -h(-x) & \text{for } x \in [-L,0).
    \end{cases}\]
\end{definition}

\begin{proposition}
    The Fourier series of \(\wh{h}_{\text{odd}}\) is equal to the Fourier Sine series of \(h\). That is, the Fourier series of \(\wh{h}_{\text{odd}}\) is 
    \[\sum_{n=1}^{\infty} b_n \sin \left( \frac{n\pi}{L}x \right).\]
    where 
    \[b_n = \frac{2}{L} \int_0^L \wh{h}_{\text{odd}}(x)\sin \left( \frac{n\pi}{L}x \right) \, dx \quad \text{for } n \in \NN.\]
\end{proposition}

\begin{corollary}
    Let \(h : [0,L] \to \RR\) be a continuous function such that \(h(0)=h(L)=0\) and such that \(h'\) is piecewise continuous. Let \(\sum_{n=1}^{\infty} a_n \sin \left( \frac{n\pi}{L}x \right)\) be the Fourier sine series of \(h\). Then,
    \[h(x) =\sum_{n=1}^{\infty} a_n \sin \left( \frac{n\pi}{L}x \right).\]
\end{corollary}

\begin{definition}
    Given \(h:[0,L] \to \RR\), let \(\wh{h}_{\text{even}}:[-L,L] \to \RR\) be the \textbf{even extension} of \(h\). That is,
    \[\wh{h}_{\text{even}}(x) = \begin{cases}
        h(x) & \text{for } x \in [0,L] \\
        h(-x) & \text{for } x \in [-L,0).
    \end{cases}\]
\end{definition}

\begin{proposition}
    The Fourier series of \(\wh{h}_{\text{even}}\) is equal to the Fourier cosine series of \(h\). That is, the Fourier series of \(\wh{h}_{\text{even}}\) is 
    \[\half a_0 + \sum_{n=1}^{\infty} a_n \cos\left( \frac{n\pi}{L}x \right),\]
    where 
    \[a_n =\frac{2}{L} \int_{0}^L \wh{h}_{\text{even}}(x) \cos\left( \frac{n\pi}{L}x \right) \quad \text{for } n \in \NN \cup \{0\}.\]
\end{proposition}

\begin{corollary}
    Let \(h:[0,L] \to \RR\) be a continuous function such that \(h'\) is piecewise continuous. Let \(\half a_0 +\sum_{n=1}^{\infty} a_n \cos\left( \frac{n\pi}{L}x \right)\) be the Fourier cosine series of \(h\). Then,
    \[h(x) = \half a_0 +\sum_{n=1}^{\infty} a_n \cos\left( \frac{n\pi}{L}x \right).\]
\end{corollary}

\subsection{Separation of variables}

In this section we solve the following problem 
\[\begin{aligned}
    \begin{cases}
        u_t=ku_{xx} & x\in(0,L) \quad t,k>0 \\
        u(0,t)=u(L,t)=0,
    \end{cases}
\end{aligned}\]

with the method of \textbf{separation of variables} i.e. when 
\[u(x,t) = X(x)T(t).\]

By plugging in the boundary conditions we obtain the trivial solution of the heat equation. We note that 
\[\begin{aligned}
    \diffp{}{t}u(x,t) &= \diffp{}{t} X(x)T(t) \\
    &= X(x)T'(t),
\end{aligned}\]

and 

\[\begin{aligned}
    \diffp{}{t}u(x,t) &= \diffp{}{x} X(x)T(t) \\
    &= X''(x)T(t).
\end{aligned}\]

Therefore, 

\[\begin{aligned}
    \diffp{}{t}u(x,t) &= k\diffp[2]{}{x}u(x,t) \\
    \iff X(x)T'(t) &= kX''(x)T(t) \\
    \iff \frac{T'(t)}{kT(t)} &= \frac{X''(x)}{X(x)}.
\end{aligned}\]

The only way this equation holds if both the RHS and LHS are equal to a constant. Let this constant be \(\lambda \in \RR\) such that 
\[\frac{T'(t)}{kT(t)} = -\lambda = \frac{X''(x)}{X(x)}.\]

\begin{mdnote}
    We use \(-\lambda\) as a convention.
\end{mdnote}

This can be rephrased as the following: there exists \(\lambda \in \RR\) such that 
\[T'(t)= -\lambda k T(t) \quad \text{and} \quad X''(x)=-\lambda X(x).\]

\subsubsection{Solution of time ODE}

Consider the time dependent ODE:
\[T'(t)=-\lambda k T(t).\]

With the standard methods of solving first order ODE we obtain the solution 

\[T(t) = Ce^{-\lambda k t}\]
for \(C \in \RR\).

\subsubsection{Solution of `position' ODE}

Consider the position dependent ODE:
\[\begin{aligned}
    \begin{cases}
        X''(x)= -\lambda X(x) & x\in[0,L] \\
        X(0)=X(L)=0,
    \end{cases}
\end{aligned}\]

clearly, the trivial solution \(X(x)=0\) for all \(t\geq 0\) is a valid solution, but we are interested in the non-trivial solutions.

\begin{definition}
    The value \(\lambda\) for which a non-trivial solution of the ODE above is called an \textbf{eigenvalue} for the Dirichlet problem with homogeneous BC. A non-zero solution related to this value of \(\lambda\) is called an \textbf{eigenfunction}.
\end{definition}

\begin{mdthm}
    The ODE above: 
    \begin{itemize}
        \item does \textbf{NOT} have a non-zero solution when \(\lambda<0\);
        \item does \textbf{NOT} have a non-zero solution when \(\lambda=0\);
    \end{itemize}
\end{mdthm}

\begin{proof}
    We prove each bullet point in turn.
    \begin{itemize}
        \item Proof when \(\lambda<0\). \\
        The general solution to the ODE is of the form 
        \[X(x)= C_1e^{x\sqrt{-\lambda}}+C_2 e^{-x\sqrt{-\lambda}}\]
        for \(C_1,C_2\in \RR\). Imposing the boundary condition \(X(0)=0\) gives \(C_2 =-C_1\); imposing \(X(L)=0\) gives 
        \[C_1 \left( e^{L \sqrt{-\lambda}}-e^{-L\sqrt{-\lambda}} \right) =0,\]
        if \(C_1=0\) then \(X(x)=0\). Therefore, we can assume \(C_1 \neq 0\). In this case we have a non-zero solution when \(\lambda<0\) satisfies 
        \[\begin{aligned}
            0 &= e^{L \sqrt{-\lambda}}-e^{-L\sqrt{-\lambda}} \\
            &= \left( e^{2L \sqrt{-\lambda}} -1\right)e^{-L \sqrt{-\lambda}} \\
        \end{aligned}\]
        This is \(0\) if and only if \(e^{2L \sqrt{-\lambda}} -1=0\) that is, if \(2L\sqrt{-\lambda}=0\). However, since both \(\lambda\) and \(L\) are assumed to be non-zero we have that \(2L \sqrt{-\lambda} \neq 0\) always.
        \item Proof when \(\lambda=0\). \\
        The general solution to the ODE is of the form 
        \[X(x)=C_1x+C_2\]
        for \(C_1,C_2 \in \RR\). By imposing the boundary conditions we have that \(C_1=C_2=0\) hence, \(X(x) =0\).
    \end{itemize}
\end{proof}

\begin{mdprop}
    The ODE above a non-zero solution when \(\lambda = \left( \frac{\pi n}{L} \right)^2\) for \(n \in \NN\). The solution is given by the function 
    \[X_n(x) = \sin \left(\frac{\pi n}{L} \right) .\]
    Furthermore, any other solution can be obtained by multiplying \(X_n(x)\) by a constant.
\end{mdprop}

\begin{proof}
    Suppose \(\lambda>0\) then 
    \[X(x) = C_1\cos(x\sqrt{\lambda})+C_2\sin(x\sqrt{\lambda})\]
    for \(C_1,C_2 \in \RR\). Imposing the initial condition \(X(0)=0\) gives that \(C_1=0\) and imposing \(X(L)=0\) we have 
    \[X(x)=C_2\sin(L\sqrt{\lambda}).\]
    This is true when
    \[\begin{aligned}
        L\sqrt{\lambda} &= n\pi \\
        \lambda &= \left( \frac{n\pi}{L} \right)^2
    \end{aligned}\]
    for \(n \in \NN\). Furthermore, for \(\lambda = \left( \frac{n\pi}{L} \right)^2\) the function 
    \[X_n(x)=\sin \left( \frac{n\pi}{L} x\right)\]
    is the non-zero solution and, any other solution can be obtained by multiplying \(X_n(x)\) by a constant.
\end{proof}

\subsubsection{The general solution}

We know solutions exists for \(\lambda = \left( \frac{n\pi}{L}\right)^2\) for \(n\in \NN\) so, a solution to 
\[T'(t)= - k\left( \frac{n\pi}{L}\right)^2T(t)\]
is 
\[T_n(t) = e^{-k \left( \frac{n\pi}{L} \right)^2 t}.\]

In conclusion, the method of separation of variables gives that the solution to 

\[\begin{aligned}
    \begin{cases}
        u_t=k u_{xx} \quad x\in(0,L) \; t,k>0 \\
        u(0,t)=u(L,t)=0,
    \end{cases}
\end{aligned}\]

is given by 
\[\begin{aligned}
    u_n(x,t) &= X_n(x)T_n(t) \\
    &= \sin\left( \frac{n\pi}{L}x \right)e^{-k \left( \frac{n\pi}{L} \right)^2 t}
\end{aligned}\]

\begin{mdremark}
    The set 
    \[S = \left\{ \sin\left( \frac{n\pi}{L}x \right)e^{-k \left( \frac{n\pi}{L} \right)^2 t} : n \in \NN \right\}\]
    is a countable set of solution of the heat equation which satisfies the homogeneous BC. Therefore, any finite linear combination of the elements in \(S\) is a solution to the ODE. That is, the function series 
    \[u(x,t) = \sum_{n=1}^{\infty} a_n \sin \left( \frac{n\pi}{L}x \right) e^{-k \left( \frac{n\pi}{L} \right)^2 t} \quad \text{for } a\in \RR\]
    is a solution. By the theory of Fourier series we have that the coefficients are given by 
    \[a_n = \frac{2}{L} \int_0^L f(x)\sin\left( \frac{n\pi}{L}x \right) \, dx.\]
\end{mdremark}

\subsection{Uniqueness of the solution}

\begin{mdprop}
    The function \(v(x,t)=0\) is the \underline{\textbf{unique}} solution to the heat conduction problem:
    \[\begin{aligned}
        \begin{cases}
            v_t =kv_{xx}  \quad x \in [0,L] \text{ and } t,k>0 \\
            v(L,t)\diffp{}{x}v(L,t)-v(0,t)\diffp{}{x}v(0,t)= 0 &\text{(BC)} \\
            v(x,0)=0.
        \end{cases}
    \end{aligned}\]
\end{mdprop}

\begin{proof}
    Clearly, \(v(x,t)=0\) is a solution the problem. It remains to prove that it is a unique solution. Let 
    \[I(t) = \half \int_{0}^{L} v(x,t)^2 \, dx.\]
    Note that \(I(t)\geq 0\) for all \(t \in [0,\infty)\) and that \(I(0) =0\). It follows that 
    \[\begin{aligned}
        \diff{}{t}I(t) &= \int_{0}^L v(x,t) \diffp{}{t}v(x,t) \\
        &= k \int_{0}^L v(x,t) \diffp[2]{}{x}v(x,t),
    \end{aligned}\]
    where the last equality is achieved since the heat conduction problem tells us \(v_t = k v_{xx}\). 
    Using integration by parts we have that 
    \[\begin{aligned}
        k \int_0^L v(x,t) \diffp[2]{}{x}v(x,t) \, dx &= k\left[ v(x,t)\diffp{}{x}v(x,t) \right]_0^L - k \int_0^L \left( \diffp{}{x} v(x,t) \right)^2 \, dx \\
        &= -k \int_{0}^{L} \left( \diffp{}{x} v(x,t) \right)^2 \, dx.
    \end{aligned}\]
    Notice that \(\int_{0}^{L} \left( \diffp{}{x} v(x,t) \right)^2 \, dx \geq 0\) and \(k>0\) so,
    \[\begin{aligned}
        \diff{}{t}I(t) &= -k\int_{0}^{L} \left( \diffp{}{x} v(x,t) \right)^2 \, dx \\
        &\leq 0.
    \end{aligned}\]
    That ism \(I(t)\) is a non-increasing function of \(t\) and since \(I(0)=0\) and \(I(t)\geq0\) we must have \(I(t)=0\) i.e.
    \[\half \int_0^L v(x,t)^2 \, dx =0.\]
    Clearly, this implies that \(v(x,t)=0\).
\end{proof}

\begin{mdthm}
    The solution to the heat equation with homogeneous BC is \textbf{unique}.
\end{mdthm}

\begin{proof}
    For the sake of contradiction assume the solution is not unique and let \(u_1(x,t)\) and \(u_2(x,t)\) be two solutions of the heat equation with homogeneous BC. We have that 
    \[v(x,t)=u_1(x,t)-u_2(x,t)\]
    is a solution to the heat equation above hence, by applying the proposition above we have that \(v(x,t)=0\) is the unique solution. We conclude,
    \[u_1(x,t)=u_2(x,t).\]
\end{proof}

\subsection{Insulated ends}

In this section we want to solve the following heat conduction problem:

\[\begin{aligned}
    \begin{cases}
        u_t=ku_{xx} & x \in (0,L), \quad t,k>0 \\
        u_x(0,t)=u_x(L,t)=0 & \text{(Neumann BC)} \\
        u(x,0)=f(x) & \text{(initial condition)},
    \end{cases}
\end{aligned}\]

where \(f:[0,L] \to \RR\) is a continuous function such that \(f'\) is piecewise continuous and \(f'(0)=f'(L)=0\).

\begin{mdthm}[Unique solution to HE with insulated ends]
    The \underline{\textbf{unique}} solution to the heat equation with the conditions specified above is 
    \[\begin{aligned}
        u(x,t)&= \half c_0 + \sum_{n=1}^{\infty} c_n \cos \left( \frac{\pi n}{L} x \right) \exp\left( -k \left( \frac{\pi n}{L} \right)^2 t \right) \\
        &= \half c_0 + \sum_{n=1}^{\infty} c_n \cos \left( \frac{\pi n}{L} x \right) e^{ -k \left( \frac{\pi n}{L} \right)^2 t},
    \end{aligned}\]
    where 
    \[c_n := \frac{2}{L} \int_{0}^L f(x) \cos \left( \frac{\pi n}{L}x \right) \, dx.\]
\end{mdthm}

\begin{mdexample}
    Consider the following heat conduction initial boundary value problem:
    \[\begin{aligned}
        \begin{cases}
            u_t = 7u_{xx} & x\in (0,\pi), \quad t>0, \\
            u_x(0,t)=u_x(\pi,t)=0 &\text{(Neumann BC)} \\
            u(x,0)=5+\cos(2x)-2\cos(3x) &\text{(initial condition)}.
        \end{cases}
    \end{aligned}\]
    By the theorem there exists a unique solution of the form 
    \[u(x,t) = \half c_0+  \sum_{n=1}^{\infty} c_n \cos(nx) e^{ -7n^2 t}.\]
    To find the coefficients \(c_n\) we can evaluate the integral, but it is easier to impose the initial condition; we note that 
    \[\begin{aligned}
        u(x,0) &= \half c_0+  \sum_{n=1}^{\infty} c_n \cos(nx) \\
        &= 5+\cos(2x)-2\cos(3x).
    \end{aligned}\]
    Therefore, \(\half c_0 =5 ,c_1=1,c_3=-2\) and the remaining \(c_n=0\). We conclude, the general solution to this problem is 
    \[u(x,t)=5+e^{-28 t}\cos(2x)-2e^{-63 t}\cos(3x).\]
\end{mdexample}

\subsubsection{Solution for `position' ODE}

Assume \(u(x,t)=X(x)T(t)\) by our previous discussion of separation of variables we now consider the ODE 
\[\begin{aligned}
    \begin{cases}
        X''(x)=-\lambda X(x) \quad x \in [0,L] \\
        X'(0)=X'(L)=0
    \end{cases}
\end{aligned}\]

and when it has solution.

\begin{definition}
    The value \(\lambda\) for which a non-trivial solution of the ODE with the specified condition above exists is called an \textbf{eigenvalue} for the Dirichlet problem with \textbf{Neumann boundary conditions}. A non-zero solution related to this value of \(\lambda\) is called \textbf{eigenfunction}.
\end{definition}

\begin{mdthm}
    The ODE with the specified conditions above does \underline{\textbf{NOT}} have a non-zero solution when \(\lambda<0\).
\end{mdthm}

\begin{proof}
    In this case the solution is of the form 
    \[X(x)=C_1 e^{x\sqrt{-\lambda}}+C_2 e^{-x\sqrt{-\lambda}}\]
    for \(C_1,C_2 \in \RR\) so,
    \[X'(x)= \sqrt{-\lambda}\left( C_1 e^{x\sqrt{-\lambda}}-C_2 e^{-x\sqrt{-\lambda}} \right).\]
    Imposing the condition \(X'(0)=0\) gives us \(C_1=C_2\); imposing \(X'(L)=0\) gives 
    \[ \sqrt{-\lambda}\left( C_1 e^{L\sqrt{-\lambda}}-C_2 e^{-L\sqrt{-\lambda}} \right)=0 \iff C_1 \left( e^{L\sqrt{-\lambda}}-e^{-L\sqrt{-\lambda}} \right)=0.\]
    Since we are not interested in the trivial solution we can assume \(C_1 \neq 0\) therefore, 
    \[\begin{aligned}
        0 &= e^{L \sqrt{-\lambda}}-e^{-L\sqrt{-\lambda}} \\
        &= \left( e^{2L \sqrt{-\lambda}} -1\right)e^{-L \sqrt{-\lambda}} \\
    \end{aligned}\]
    This is \(0\) if and only if \(e^{2L \sqrt{-\lambda}} -1=0\) that is, if \(2L\sqrt{-\lambda}=0\). However, since both \(\lambda\) and \(L\) are assumed to be non-zero we have that \(2L \sqrt{-\lambda} \neq 0\) always.
\end{proof}

\begin{mdthm}
    The ODE with the specified conditions has a non-zero solution when \(\lambda =\left( \frac{n\pi}{L} \right)^2\) for \(n \in \NN \cup \{0\}\). The solution is given by
    \[X_n(x)= \cos\left( \frac{n\pi}{L} x\right).\]
    Furthermore, any other solution can be obtained by multiplying \(X_n(x)\) by a constant.
\end{mdthm}

\begin{proof}
    There are two special cases.
    \begin{itemize}
        \item If \(\lambda>0\) then, 
        \[X(x) = C_1 \cos(x\sqrt{\lambda})+C_2 \sin(x\sqrt{\lambda})\]
        which implies 
        \[X'(x)=-C_1\sqrt{\lambda}\sin(x\sqrt{\lambda})+C_2 \sqrt{\lambda}\cos(x\sqrt{\lambda}).\]
        Imposing the condition that \(X'(0)=0\) gives \(C_2 =0\) so,\(X(x)=C_1\cos(x\sqrt{x})\); imposing the condition \(X'(L)=0\) gives 
        \[-C_1\sqrt{\lambda}\sin(L\sqrt{\lambda})=0.\]
        This is true when 
        \[L\sqrt{L} = n \pi \quad \text{for } n \in \NN,\]
        that is when,
        \[\lambda = \left( \frac{n\pi}{L}\right)^2.\]
        Hence, the solution is given by 
        \[X_n(x)= \cos\left( \frac{n\pi}{L} x\right),\]
        and any other solution can be obtained by multiplying \(X_n(x)\) by a constant.
        \item If \(\lambda =0\) then,
        \[X_0(x) = \cos(0)=1\]
        which is a non-zero solution.
    \end{itemize}
\end{proof}

\begin{mdremark}
    The set 
    \[S = \left\{ \cos\left( \frac{n\pi}{L}x \right)e^{-k \left( \frac{n\pi}{L} \right)^2 t} : n \in \NN \cup \{0\} \right\}\]
    is a countable set of solution of the heat equation which satisfies the Neumann BC. Therefore, any finite linear combination of the elements in \(S\) is a solution to the ODE. That is, the function series 
    \[u(x,t) = \half a_0 + \sum_{n=1}^{\infty} a_n \cos \left( \frac{n\pi}{L}x \right) e^{-k \left( \frac{n\pi}{L} \right)^2 t} \quad \text{for } a\in \RR\]
    is a solution. By the theory of Fourier series we have that the coefficients are given by 
    \[a_n = \frac{2}{L} \int_0^L f(x)\cos\left( \frac{n\pi}{L}x \right) \, dx.\]
\end{mdremark}

\subsection{Constant boundary conditions}

In this section we show methods to solve the heat equation with the following conditions:

\[\begin{aligned}
    \begin{cases}
        u_t = ku_{xx} & x \in (0,L), \quad t,k>0 \\
        u(0,t)=U_1 \\
        u(L,t)=U_2  \\
        u(x,0)=f(x) & \text{(initial condition)},
    \end{cases}
\end{aligned}\]
where \(f:[0,L] \to \RR\) is a continuous function such that \(f'\) is piecewise conditions and \(f(0)=U_1\) and \(f(L)=U_2\).

\begin{definition}
    Given a heat conduction problem, the \textbf{equilibrium solution} or the \textbf{steady-state solution} is a solution \(u(x,t)\) of the problem (not including the initial condition) that does not depend on \(t\). That is, 
    \[\diffp{}{t}u(x,t) =0 \quad \forall t \geq 0.\]
    We denote such solution by \(u_e(x)\).
\end{definition}

\begin{mdremark}
    Requiring that \(\diffp{}{t}u_e(x) =0\) and that it satisfies the heat equation gives that \(\diffp[2]{}{t}u_t(x) = 0\). Therefore, \(u_e(x)\) must be of the form \(Ax+B\). By imposing the initial condition we obtain the values of \(A\) and \(B\).
\end{mdremark}

\begin{mdthm}[Equilibrium solution]
    The equilibrium solution of the heat conduction problem with the specified conditions above is 
    \[u_e(x)=U_1 +\frac{(U_2-U_1)x}{L}.\]
\end{mdthm}

\begin{mdthm}[Unique solution to HE with constant BC]
    The \underline{\textbf{unique}} solution to the heat equation with the conditions specified above is 
    \[u(x,t):= w(x,t)+u_e(x)\]
    where \(u_e(x)\) is the equilibrium solution and \(w(x,t)\) is the solution of the following problem:
    \[\begin{aligned}
        \begin{cases}
            u_t =ku_{xx} & x\in (0,L) \quad t,k>0 \\
            u(0,t)=u(L,t)=0 &\text{(homogeneous BC)}\\
            u(x,0) =f(x)-u_e(x).
        \end{cases}
    \end{aligned}\]
\end{mdthm}

\subsection{Maximum principle}

\begin{mdnote}
    Refer to this video \href{https://www.youtube.com/watch?v=nDpwwEBj4ME}{Maximum principle} at minute \(8:00\).
\end{mdnote}

\begin{mdnote}
    Let \(u(x,t)\) be a solution to the heat equation \(u_t-ku_{xx}\leq 0\) for \(k>0\) then, 
    \[\max u(x,t) = u(0,0) \text{ or } u(L,0).\]
\end{mdnote}

\begin{mdthm}[Maximum principle]
    Suppose that \(u(x,t)\) satisfies 
    \[u_t -ku_{xx} \leq 0 \quad \text{for } k>0\]
    in the spacetime rectangle \(\Omega_T = (0,L) \times (0,T]\).
    Then,
    \[\max_{\overline{\Omega}_T = [0,L]\times[0,T]} u = \max_{\overline{\Omega}_T \backslash \Omega_T}.\]
    In particular,
    \[\sup_{[0,L]\times[0,\infty]} u = \max_{\overline{\Omega}_{\infty} \backslash \Omega_{\infty}}.\]
\end{mdthm}

\begin{mdnote}
    We can interpret the Maximum principle as follows: \\
    Suppose that \(u(x,t)\) satisfies 
    \[u_t -ku_{xx} \leq 0 \quad \text{for } k>0\]
    in the spacetime rectangle \(0 \leq x \leq L\), \(0 \leq t \leq T\). Then, the maximum value of \(u\) occurs at some point on the boundary lines 
    \[t=0, x=0 \text{ or } x=L \]
    Below we have an illustration of the rectangle.
    \begin{figure}[H]
         \begin{center}
             \include{./Resources/Maximum principle.tex}
         \end{center}
    \end{figure}
\end{mdnote}

\begin{lemma}
    If \(u\) attains its maximum over \(\overline{\Omega}_T\) at a point \((x_0,t_0) \in \Omega_T\) then,
    \[u_t(x_0,t_0) \geq 0 \quad \text{and} \quad u_{xx}(x_0,t_0) \leq 0.\]
    In particular, 
    \[u_t(x_0,t_0)-ku_{xx}(x_0,t_0) \geq 0.\]
\end{lemma}

\begin{mdprop}
    Let \(u_1(x,t)\) and \(u_2(x,t)\) be two solutions of 
    \[\begin{cases}
        u_t=ku_{xx} \quad x \in (0,L), \, t,k>0 \\
        u(0,t)=u(L,t)=0 \\
        u_1(x,0)=f_1(x) \\
        u_2(x,0)=f_2(x).
    \end{cases}\]
    Then,
    \[\max_{\overline{\Omega}_T} \abs{u_1-u_2} \leq \max_{[0,L]} \abs{f_1-f_2}.\]
\end{mdprop}

\section{The wave equation}

Given a length \(L>0\) of a `perfectly flexible' elastic string stretched between two points at distance \(L\), the \textbf{wave equation} says that the displacement \(u(t,x)\) for \(x \in (0,L)\) and time \(t>0\) changes according to the following problem.

\[\begin{aligned}
    \begin{cases}
        u_{tt}=\alpha^2 u_{xx} & x\in(0,L), \; t>0 \\
        u(0,t)=u(L,t) =0 & \text{(Homogeneous BC)} \\
        u(x,0) =f(x) & \text{(initial displacement condition)} \\
        u_t(x,0)=g(x) & \text{(Initial velocity condition)},
    \end{cases}
\end{aligned}\]

where \(\alpha^2\) depends on the properties of the string and \(f,g:[0,L] \to \RR\) are smooth functions with \(f(0)=f(L)=0\) and \(g(0)=g(L)=0\).

\begin{mdremark}
    This is a second order linear PDE.
\end{mdremark}

\begin{mdthm}
    The \underline{\textbf{unique}} solution to the ODE above with the specified conditions is 
    \[u(x,t) = \sum_{n=1}^{\infty} \left[ a_n \cos\left( \frac{\pi n \alpha}{L}t \right)+b_n \sin\left( \frac{\pi n \alpha}{L}t \right) \right] \sin\left( \frac{n \pi}{L}x \right)\]
    where 
    \[a_n = \frac{2}{L} \int_0^L f(x) \sin\left( \frac{\pi n}{L}x \right) \, dx,\]
    and 
    \[\frac{n \pi \alpha}{L} b_n = \frac{2}{L} \int_0^L  g(x) \sin \left( \frac{\pi n}{L}x \right) \, dx.\]
\end{mdthm}

\begin{mdexample}
    Solve the following problem:
    \[\begin{cases}
        u_{tt}=9u_{xx} \quad x \in (0,\pi), \, t>0 \\
        u(0,t)=u(\pi,t)=0 \\
        u(x,0)=4\sin(3x) \\
        u_t(x,0)=14\sin(7x).
    \end{cases}\]
    By the theorem above, there exists a unique solution of the form 
    \[u(x,t) = \sum_{n=1}^{\infty} \left[ a_n \cos(3nt)+b_n \sin(3nt) \right]\sin(nx).\]
    To find the coefficients we impose the initial conditions:
    \[\begin{aligned}
        u(x,0) &= \sum_{n=1}^{\infty} a_n \sin(nx) \\
        &= 4\sin(3x)
    \end{aligned}\]
    and 
    \[\begin{aligned}
        u_t(x,0) &= \sum_{n=1}^{\infty} 3n b_n \sin(nx) \\
        &= 14\sin(7x).
    \end{aligned}\]
    These conditions imply that \(a_3=4\) and all other \(a_n=0\), it also implies \(b_7 = \frac{14}{21} =\frac{2}{3}\) and all other \(b_n=0\). Therefore, the solution is 
    \[u(x,t) = 4\cos(9t)\sin(3x)+\frac{2}{3}\sin(21t)\sin(7x).\]
\end{mdexample}

\subsection{Zero initial velocity}

In this section we consider the problem 
\[\begin{cases}
    u_{tt} = \alpha^2 u_{xx} & x\in(0,L), t>0 \\
    u(0,t)=u(L,t)=0 \\
    u(x,0)=f(x) \\
    u_t(x,0)=0 & \text{(Zero initial velocity)}.
\end{cases}\]

\begin{mdthm}
    The \underline{\textbf{unique}} solution to the problem above is 
    \[u(x,t) = \sum_{n=1}^{\infty} a_n \cos\left( \frac{n\pi \alpha}{L}t \right)\sin\left( \frac{n\pi}{L}x \right)\] 
    with 
    \[a_n = \frac{2}{L} \int_{0}^L f(x) \sin \left( \frac{n\pi}{L}x \right) \, dx.\]
\end{mdthm}

\subsubsection{Separation of variables}

In order to prove the theorem above we will consider a simpler problem. Consider the problem 
\[\begin{cases}
    u_{tt} = \alpha^2 u_{xx} \quad x\in (0,L), \, t>0 \\
    u(0,t)=u(L,t)=0 \\
    u_t(x,0)=0.
\end{cases}\]

Suppose, \(u(x,t)\) is separable i.e.
\[u(x,t)=X(x)T(t),\]
once again \(u(x,t)=0\) is a solution, but we are interested in non-trivial solutions. Imposing the boundary conditions, and we have 
\[u(0,t)=X(0)T(t)=0 \quad \text{and} \quad u(L,t)=X(L)T(t)=0\]
for all \(t>0\). In order to have \(X(0)T(t)=0\) for all \(t>0\) we either have \(X(0)=0\) or \(T(t)=0\) for all \(t>0\) however, the second option leads back to the trivial solution thus, we must have 
\[X(0)=0.\]
By a similar argument \(X(L)=0\). Now imposing the initial condition, \(u_t(x,0)=0\), we have that 
\[\diffp{}{t}u(x,0)=X(x)T'(0)=0\]
by a similar reasoning as above this gives that \(T'(0)=0\). Substituting, \(u(x,t)=X(x)T(t)\) into the problem implies, there exist \(\lambda \in \RR\) such that 
\[T''(t)=-\lambda \alpha^2 T(t) \quad \text{and} \quad X''(x)=-\lambda X(x).\]

Consider the position dependent ODE, 
\[\begin{cases}
    X''(x)=-\lambda X(x) \\
    X(0)=X(L)=0.
\end{cases}\]

We have seen that this ODE has non-zero solutions only when 
\[\lambda = \left( \frac{n\pi}{L} \right)^2 \quad \text{for } n \in \NN\]
and, when that is the case, the solution is given by 
\[X_n(x) = \sin\left( \frac{n\pi}{L}x \right).\]
Any other solution can be obtained by multiplying \(X_n(x)\) by a constant.

With this in mind, for a given \(\lambda = \left( \frac{n\pi}{L} \right)^2\) the time dependent ODE becomes
\[T''(t) =-\left( \frac{n\pi\alpha}{L} \right)^2 T(t)\]
and, the general solution is 
\[T_n(t)=a_n \cos\left( \frac{n\pi\alpha}{L} t\right)+b_n\sin\left( \frac{n\pi\alpha}{L} t\right).\]

We have that 
\[T'_n(t) = -a_n \left( \frac{n\pi\alpha}{L} \right)\sin\left( \frac{n\pi\alpha}{L} t\right)+b_n\left( \frac{n\pi\alpha}{L} \right)\cos\left( \frac{n\pi\alpha}{L} t\right)\]
and imposing the initial condition gives 
\[T'_n(0)=b_n \left( \frac{n\pi\alpha}{L} \right)=0.\]
This implies that \(b_n=0\) and
\[T_n(t)=a_n \cos\left( \frac{n\pi\alpha}{L} t\right).\]

We have that 
\[u(x,t) = a_n \sin \left( \frac{n\pi}{L}x \right)\cos\left( \frac{n\pi\alpha}{L} t\right).\]

Consider the set 
\[S = \left\{ \sin \left( \frac{n\pi}{L}x \right) \cos\left( \frac{n\pi\alpha}{L} t\right) : n \in \NN \right\},\]

is a countable set of solutions and since the wave equation is linear, any finite linear combination of elements in \(S\) is a solution. Therefore, the function series 
\[u(x,t) = \sum_{n=1}^{\infty} a_n \sin \left( \frac{n\pi}{L}x \right) \cos\left( \frac{n\pi\alpha}{L} t\right) \quad \text{for } a_n \in \RR,\]
is the solution to the problem.

\subsection{Zero initial displacement}

In this section we consider the problem 
\[\begin{cases}
    u_{tt} = \alpha^2 u_{xx} & x\in(0,L), t>0 \\
    u(0,t)=u(L,t)=0 \\
    u(x,0)=0 &\text{(Zero initial displacement)} \\
    u_t(x,0)=g(x).
\end{cases}\]

\begin{mdthm}
    The \underline{\textbf{unique}} solution to the problem above is 
    \[u(x,t) = \sum_{n=1}^{\infty} b_n \sin\left( \frac{n\pi \alpha}{L}t \right)\sin\left( \frac{n\pi}{L}x \right)\] 
    with 
    \[\frac{n\pi \alpha}{L}b_n = \frac{2}{L} \int_{0}^L g(x) \sin \left( \frac{n\pi}{L}x \right) \, dx.\]
\end{mdthm}

\subsubsection{Separation of variables}

In order to prove the theorem above we will consider a simpler problem. Consider the problem 
\[\begin{cases}
    u_{tt}=\alpha^2 u_{xx} &x\in(0,L), \, t>0 \\
    u(0,t)=u(L,t)=0 \\
    u(x,0)=0.
\end{cases}\]

Suppose, the solution \(u(x,t)\) is separable i.e. it can be written as 
\[u(x,t)=X(x)T(t).\]

Once again \(u(x,t)=0\) is a solution, but we are interested in non-trivial solutions.

The boundary conditions for \(X(x)\) are \(X(0)=X(L)=0\) and imposing the condition \(u(x,0) = X(x)T(0)=0\) which implies \(T(0)=0\).

As in a previous section, we obtain that there must exist \(\lambda \in \RR\) such that 
\[T''(t)=-\lambda \alpha^2 T(t) \quad \text{and} \quad X''(x)=-\lambda X(x).\]

Considering the position dependent ODE 
\[\begin{cases}
    X''(x)=-\lambda X(x) x\in [0,L] \\
    X(0)=X(L)=0,
\end{cases}\]

as shown previously this has a solution when 
\[\lambda = \left( \frac{n\pi}{L} \right)^2 \quad \text{for } n \in \NN\]
and the solution is the function 
\[X_n(x)=\sin\left( \frac{n\pi}{L} x\right).\]
Any other solution can be obtained by multiplying \(X_n(x)\) by a constant.

With this in mind, for a given \(\lambda = \left( \frac{n\pi}{L} \right)^2\) the time dependent ODE becomes 
\[T''(t)=-\left( \frac{n\pi\alpha}{L} \right)^2 T(t),\]
where the general solution is then,
\[T_n(t)=a_n \cos \left( \frac{n\pi\alpha}{L} t\right)+b_n \sin \left( \frac{n\pi\alpha}{L}t \right).\]

Imposing the initial condition gives
\[T_n(0)=a_n=0,\]
which implies that \(a_n=0\) and 
\[T_n(t)=b_n \sin \left( \frac{n\pi\alpha}{L}t \right).\]

We have that 
\[u(x,t) =b_n \sin\left( \frac{n\pi}{L} x\right)\sin \left( \frac{n\pi\alpha}{L}t \right).\]

Consider the set 
\[S = \left\{ \sin\left( \frac{n\pi}{L} x\right)\sin \left( \frac{n\pi\alpha}{L}t \right): n \in \NN \right\},\]
which is a countable set of solutions and since the wave equation is linear, any finite linear combinations of elements in \(S\) is a solution. Therefore, the function series 

\[u(x,t) = \sum_{n=1}^{\infty} b_n \sin\left( \frac{n\pi \alpha}{L}t \right)\sin\left( \frac{n\pi}{L}x \right)\] 
is the solution.

\subsection{Uniqueness of the solution}

In this section we prove that the solution to the wave equation and its variants is unique.

\begin{mdthm}
    Consider the following initial-boundary value problem for the wave equation:
    \[\begin{cases}
        v_{tt}=\alpha^2 v_{xx} \quad x\in(0,L) \, t>0 \\
        v(L,t)=v(0,t)=0 \\
        v(x,0)=0\\
        v_t(x,0)=0.
   \end{cases}\]
   The function \(v(x,t) =0\) is the unique solution to this problem.
\end{mdthm}

\begin{proof}
    Clearly, \(v(x,t)=0\) is a solution to the problem. First note that the conditions \(v(L,t)=v(0,t)=0\) and \(v(x,0)=0\) imply that \(v_t(L,t)=v_t(0,t)=0\) and \(v_x(x,0)=0\). Let 
    \[E(t) = \half \int_0^L \alpha^2 v_x^2 +v_t^2 \, dx \quad (\text{the energy}).\]
    We have that 
    \[\begin{aligned}
        \diff{}{t}E(t) &= \int_0^L \diffp{}{t} \left( \alpha^2 v_x^2 +v_t^2 \right) \, dx \\
        &= \int_0^L \alpha^2 v_x v_{xt}+v_t v_{tt} \, dx \\
        &=\int_0^L \alpha^2 v_x v_{xt} \, dx + \int_0^L v_t v_{tt} \, dx.
    \end{aligned}\]
\end{proof}

\begin{mdthm}
    FINISH PROOF
\end{mdthm}

\begin{mdthm}
    Solution is unique 
\end{mdthm}

\begin{proof}
    Let \(u_1(x,t)\) and \(u_2(x,t)\) be two solutions of the wave equation. 
    \begin{mdthm}
        TO FINISH.
    \end{mdthm}
\end{proof}

\subsection{Cracking the whip}

In this section we consider the problem 
\[\begin{cases}
    u_{tt} = \alpha^2 u_{xx} , & x \in (0,\infty), \, t>0 \\
    u(0,t)=h(t) & \lim_{x\to\infty} \sup_{t \geq 0} \abs{u(x,t)}=0\\
    u(x,0)=0 \\
    u_t(x,0)=0.
\end{cases}\]

\begin{mdthm}
    The \underline{\textbf{unique}} solution to the problem above is given by 
    \[u(x,t) = g_{\frac{x}{\alpha}}(t) \cdot h\left( t-\frac{x}{\alpha} \right),\]
    where the function 
    \[g_{\frac{x}{\alpha}}(t) = \begin{cases}
        1 & t \geq \frac{x}{\alpha} \\
        0 & \text{otherwise},
    \end{cases}\]
    is a step function.
\end{mdthm}

\begin{mdexample}
    Let 
    \[h(t) = \begin{cases}
        \sin(t) & t \in [0,\pi] \\
        0 & t\geq \pi.
    \end{cases}\]
    Then the solution to the problem above is given by 
    \[u(x,t) = g_{\frac{x}{\alpha}}(t) \cdot h\left( t-\frac{x}{\alpha} \right).\]
    Note that \(h(t) = (1-g_{\pi}(t))\sin\left( t-\frac{x}{\alpha} \right)\), as such we can write 
    \[\begin{aligned}
        u(x,t) &= g_{\frac{x}{\alpha}}(t) \left( 1- g_{\pi} \left( t-\frac{x}{\alpha} \right)\right)\sin\left( t-\frac{x}{\alpha} \right) \\
        &= \begin{cases}
            \sin\left( t-\frac{x}{\alpha} \right) & t\in \left[ \frac{x}{\alpha},\frac{x}{\alpha}+\pi \right] \\
            0 & \text{otherwise}.
        \end{cases}
    \end{aligned}\]
\end{mdexample}

\subsection{Can you hear the shape of a drum?}

NO

\subsection{The 2-dimensional wave equation}

Let \(\Omega\) be a bounded domain in \(\RR^2\) and given a function \(u(x,y,t)\), let \(\Delta u = u_{xx}+u_{yy}\). Then the \(2\)-dimensional wave equation
\[\begin{cases}
    u_{tt}(x,y,t) = \alpha^2 \Delta u(x,y,t) & (x,y) \in \Omega \, t>0 \\
    u(x,y,t)=0 & (x,y) \in \partial \Omega \quad (\text{Homogeneous BC}) \\
    u(x,y,0)=f(x,y) & (\text{Initial displacement}) \\
    u_t(x,y,0)=g(x,y) & (\text{Initial velocity}).
\end{cases}\]

\begin{mdremark}
    The notation \(\partial \Omega\) means the boundary of the set \(\Omega\).
\end{mdremark}

\subsubsection{Separation of variables}

However, in this section we are going to investigate this problem without the initial displacement and velocity conditions, namely the problem: 
\[\begin{cases}
    u_{tt}(x,y,t) = \alpha^2 \Delta u(x,y,t) & (x,y) \in \Omega \, t>0 \\
    u(x,y,t)=0 & (x,y) \in \partial \Omega \quad(\text{Homogeneous BC}).
\end{cases}\]

Assume the separation is separable, assume that \(u(x,y,t)=X(x,y)T(t)\). Now, checking the boundary condition 
\[u(x,y,t)= X(x,y)T(t)=0 \quad \text{for } (x,y) \in \partial \Omega.\]

In order to have this we must either have \(X(x,y)=0\) or \(T(t) =0\) for all \(t>0\). The second option leads us to the trivial solution so, if we are interested in the non-trivial solution we assume:
\[X(x,y)=0 \quad \text{for } (x,y) \in \partial \Omega.\]
Calculating the second derivatives we have that 
\[\begin{aligned}
    u_{tt} &= XT'', \\
    u_{xx} &= X_{xx}T, \\
    u_{yy} &= X_{yy}T.
\end{aligned}\]

Therefore, wave equation 
\[u_{tt} = \alpha^2 \Delta u \] 
becomes 
\[\begin{aligned}
    XT'' &= \alpha^2 (u_{xx}+u_{yy}) \\
    &=\alpha^2 \left( X_{xx} +X_{yy} \right)T \\
    &= \alpha^2 T \cdot \Delta X,
\end{aligned}\]

This is equivalent to 
\[\frac{T''}{\alpha^2 T} = \frac{\Delta X}{X},\]
for this to be true there must exist \(\lambda \in \RR\) such that 
\[\frac{T''}{\alpha^2 T} = -\lambda = \frac{\Delta X}{X}.\]



\subsection{The Dirichlet eigenvalues of a disk}
























\pagebreak

\appendix

\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}

\section{Links}

\begin{itemize}
    \item \href{https://tutorial.math.lamar.edu/Classes/DE/SeriesIntro.aspx}{Series}
\end{itemize}

\section{Laplace transform table}

\begin{table}[H]
    \begin{center}
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Function} & \textbf{L-Transform} \\ \hline
            \(y(t)\) & \(Y(s) = \CL[y(t)](s)\) \\ \hline
            \(e^{at}\) & \(\frac{1}{s-a} \text{ for } s>a\) \\ \hline
            \(\sin(\omega t)\) & \(\frac{\omega}{s^2+\omega}\) 
        \end{tabular}
    \end{center}
\end{table}

\section{Techniques of integration}

\subsection{Integration by parts}

\begin{mdthm}[Integration by parts]
    Let \(f,g \in C[a,b]\) with \(f',g' \in C[a,b]\); then
    \[\int_a^b f(x)g'(x) \, dx =[f(x)g(x)]^b_a -\int_a^b f'(x)g(x)\, dx\]
\end{mdthm}

\begin{mdnote}
    The acronym \textbf{LIATE} can be used to choose which function to differentiate. (The I stands inverse trigonometric/hyperbolic functions).
\end{mdnote}

\section{Tricks}

\subsection{Step function}

\begin{example}
    Let \(f\) satisfy \(f(T+t) =f(t)\) for all \(t \geq 0\) and for some fixed \(T>0\). Show that 
    \[\CL[f(t)](s) = \frac{\int_{0}^{T} f(t) e^{-st}\, dt}{1-e^{-sT}}.\]
    \begin{solution}
        Notice that 
        \[\begin{aligned}
            u_T(t) = \begin{cases}
                0 &\text{if } t < T\\
                1 &\text{if } t \geq T.
            \end{cases}
        \end{aligned}\]
        Therefore, 
        \[\begin{aligned}
            1- u_T(t) = \begin{cases}
                1 &\text{if } t < T\\
                0 &\text{if } t \geq T.
            \end{cases}
        \end{aligned}\]
        Now, we can write 
        \[\begin{aligned}
            \int_{0}^{T} y(t)e^{-st} \, dt &= \int_{0}^{T} 1 \cdot y(t)e^{-st} \, dt + \int_{0}^{\infty} 0 \cdot y(t)e^{-st} \, dt \\
            &= \int_{0}^{\infty} (1-u_T(t)) y(t)e^{-st} \, dt.
        \end{aligned}\]
    \end{solution}
\end{example}

\begin{example}
    Compute \(\CL\inv\left[ \frac{e^{-4s}}{2s-1} \right](t)\).
    \begin{solution}
        Recall 
        \begin{itemize}
            \item \(\CL[e^{at}]= \frac{1}{s-a}\) for \(s>a\).
            \item \(\CL[u_a(t)y(t-a)](s)=e^{-as}\CL[y(t)](s)\).
        \end{itemize}
        We know 
        \[\CL\left[e^{\half t}\right](s) = \frac{1}{s-\frac{1}{2}}.\]
        Therefore, we have
        \[\begin{aligned}
            \CL\inv\left[ \frac{e^{-4s}}{2s-1} \right] &= \half \CL\inv \left[ \frac{e^{-4s}}{s-\frac{1}{2}} \right] \\
            &= \half \CL\inv \left[ e^{-4s} \CL\left[e^{\half t}\right] \right] \\
            &= \half \CL\inv \left[ \CL\left[ u_4(t) e^{\half (t-4)} \right] \right] \\
            &= \half u_4(t) e^{\half (t-4)}.
        \end{aligned}\]
    \end{solution}
\end{example}

\section{Hessian matrix}

\href{https://en.wikipedia.org/wiki/Second_partial_derivative_test#Notes}{Hessian matrix}

\end{document}