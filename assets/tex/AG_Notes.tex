\documentclass[12pt, a4paper]{article}
\usepackage{francesco}
\usepackage[colorlinks=true,
            urlcolor=RubineRed,
            linktoc=all,
            linkcolor=black,
            pdfauthor={Francesco N. Chotuck},
            pdftitle={Algebraic Geometry Notes}
            ]{hyperref}
%\usepackage[none]{hyphenat}
\usepackage%[disable]
{todonotes}
\renewcommand{\AA}{\mathbb A}
\newcommand{\PP}{\mathbb P}
\declaretheorem[style=thmredbox,name=Question,numbered=yes]{mdQ}

\pagestyle{fancy}
\lhead{Francesco Chotuck}
\rhead{7CCMMS20 Algebraic Geometry Notes}
\setlength{\headheight}{15pt}

\title{Algebraic Geometry Notes}
\date{}
\author{Francesco Chotuck}
\begin{document}
\maketitle

\begin{abstract}
    \noindent This is KCL undergraduate module 7CCMMS20, instructed by Dr.\ Dmitri Panov. The formal name for this class is ``Algebraic Geometry''.
\end{abstract}

\tableofcontents

\pagebreak

\section{homogeneous coordinates}

We can redefine a circle to be a conic that passes through the points \((1:-i:0)\) and \((1:i:0)\). 

\textbf{Claim}: any theorem that holds for a collections of circles and lines holds for parabolas.

The space of all conics in \(\PP^2_{K}\) is \(\PP^5\) (where \(K\) is algebraically closed field).

\todo[inline]{AG 
Point of infinity in projective space is when we set z=0. }

\section{Rings}

The idea of this section. Let \(K\) be an algebraically closed field and an affine variety \(V \subset K^n\) (by this we mean set of polys =0 or something). Consider polynomials in \(K[X_1,\ldots,X_n]\) as functions on \(K^n\). restrict these functions (polys) ot \(V\). We get a ring and this ring knows everything about \(V\). 

\begin{example}
    Suppose \(x^2+y^2-1=0\) in \(\RR^2\). Now restrict all poly on this function (a circle). What kind of functions do we get? For example if we restrict \(x\) to the circle we get \(\sin\).
\end{example}

We only consider commutative rings in this course.

\begin{example}
    Example of maximal ideal in the ring \(\CC[X]\). First of all we conisder an interesting ideal for this ring. consider we have a set points \(x_1,\ldots,x_n \in \CC\), the set of polys which vanish at each of these points form an ideal. However, not all ideals of these type of maximal, only ideal which vanish at one point are maximal ideal in \(\CC[X]\). 
\end{example}

\begin{example}
    Example of radical. Consider the ring \(\ZZ\) and take \(I = 24\ZZ = 2^3\cdot 3 \ZZ\). We have that \(6^3 \in 24\ZZ\) thus \(Rad(I)=6\ZZ\).
\end{example}

\begin{proposition}
    The radical is an ideal.
\end{proposition}

\section{Zarinski topology}

\begin{definition}
    Let \(X \subset \AA^n_k\) be any subset. The \textbf{Zarinski closure} of \(X\) is the minimal closed set containing \(X\). (i.e. affine variety).
\end{definition}

\begin{mdnote}
    We can find such a minimal set since we are allowed to take the intersection of closed sets.
\end{mdnote}

\begin{lemma}
    The Zarinski closure of \(X \subset \AA^n_k\) is equal to \(V(I(X))\) where \(I(X)\) is the ideal of all polynomials which vanish at \(X\).
\end{lemma}

\begin{example}
    The Zarinski closure of \([0,1]\) is \([0,1]\) adn the Zarinski closure of \(\ZZ\) is \(\ZZ\). 
\end{example}

\begin{example}
    Example fo Def 7.6 \(I(\left\{ 0 \right\} \cup \left\{ 1 \right\}) = \langle x(x-1) \rangle\).
\end{example}

\todo[inline]{The above is to keep notes}

\todo[inline]{This would be the actual notes}

\section{Bezout's theorem}

\subsection{Definitions}

\begin{definition}
    An \textbf{affine space} over a field \(K\), is the vector space \(K^n\) where the point \(0\) does not play a special role. We denote affine space by \(\AA^n_K\) or \(\AA^n\).
\end{definition}

\begin{mdnote}
    An affine space captures the geometric properties of an object without the need for a fixed coordinate system or origin. It is a more flexible framework that allows for studying geometric properties independently of specific coordinate systems.
\end{mdnote}

\begin{mdremark}
    In this course \(K\) is often \(\CC\) or \(\RR\) and sometimes a finite field.
\end{mdremark}

\begin{definition}
    The set \(K[X_1,\ldots,X_n]\) is the ring of polynomials in \(n\) variables with coefficients in the field \(K\). 
\end{definition}

\begin{definition}
    The \textbf{degree} of a polynomial is the maximal degree of its monomials.
\end{definition}

\begin{mdremark}
    For example the degree of the monomial \(4xy\) is \(2\), since \(x\) and \(y\) have exponent of \(1\). 
\end{mdremark}

\begin{definition}
    By \(K(X_1,\ldots,X_n)\) we denote the field of rational functions. 
\end{definition}

\begin{definition}
    A subset \(V \subset \AA_K^n\) is an \textbf{affine variety} if there exists polynomials \(f_1,\ldots,f_M \in K[X_1,\ldots,X_n]\) such that the point \((x_1,\ldots,x_n) \in V \iff f_i(x_1,\ldots,x_n)=0\) for all \(i \in \left\{ 1,\ldots,M \right\}\).
\end{definition}

\begin{mdnote}
    An affine variety is like the set of points where different polynomial equations agree. It's a way to understand the solutions to systems of equations using geometry.
\end{mdnote}

\begin{mdexample}
    The simplest example of an affine variety is the affine space of dimension \(n\) over a field \(K\).
\end{mdexample}

\begin{definition}
    Let \(F \in K[X_1,\ldots,X_n]\) be a non-constant polynomial, then the subset \(\left\{ x \in \AA^n_K : F(x)=0 \right\} \subset \AA^n_K\) is called a \textbf{hypersurface}. 
\end{definition}

\begin{definition}
    Let \(F \in K[X_1,\ldots,X_n]\) be a non-constant polynomial.
    \begin{itemize}
        \item If \(F(x)=0\) we say that \(F\) \textbf{vanishes} at \(x\).
        \item (More generally) if \(S \subset \AA^n_K\) and \(F(x)=0\) of any \(x\in S\) we say that \(F\) \textbf{vanishes} at \(S\). 
    \end{itemize}
\end{definition}

\begin{corollary}
    If \(F\) vanishes on \(S\) then \(S\) belongs to the hypersurface \(F(x)=0\). 
\end{corollary}

\begin{mdthm}
    Each polynomial \(F\in K[X_1,\ldots,X_n]\) can be represented as a product 
    \[F= G_1G_2 \cdots G_k\]
    of irreducible polynomials. Such a representation is unique up to changing the order of the factors \(G_i\) and multiplying them by constants.
\end{mdthm}

\begin{mdnote}
    The polynomial \(f\) is irreducible over \(K\) if it \ul{cannot} be expressed as the product of two polynomials of lower degree.
\end{mdnote}

\begin{mdremark}
    That is, the polynomial ring is a unique factorisation domain.
\end{mdremark}

\subsection{Resultant}

\begin{mdnote}
    Given two polynomials \(f,g \in K[X]\) the resultant of \(f\) and \(g\) is used to determine if \(f\) and \(g\) have a common root, or more generally if they have a common factor.
\end{mdnote}

\begin{definition}
    Given two polynomials:
\[\begin{aligned}
    f(x) &= a_nx^n + a_{n-1}x^{n-1} + \ldots + a_1x + a_0 \\
    g(x) &= b_mx^m + b_{m-1}x^{m-1} + \ldots + b_1x + b_0
\end{aligned}\]
The \textbf{Sylvester} matrix is given by 
\[S(f,g) = 
    \left(
    \begin{array}{ccccccccc}
    a_n & a_{n-1} & a_{n-2} & \cdots & a_0 & 0 & \cdots & 0 \\
    0 & a_n & a_{n-1} & \cdots & a_1 & a_0 & \cdots & 0 \\
    \vdots & \vdots & \vdots & & \vdots & \vdots & & \vdots \\
    0 & 0 & 0 & \cdots & a_n & a_{n-1} & \cdots & a_1 & a_0 \\
    b_m & b_{m-1} & b_{m-2} & \cdots & b_0 & 0 & \cdots & 0 \\
    \vdots & \vdots & \vdots & & \vdots & \vdots & & \vdots \\
    0 & 0 & 0 & \cdots & b_m & b_{m-1} & \cdots & b_1 & b_0 \\
    \end{array}
    \right)
    \]
\end{definition}

\begin{mdexample}
    Say we have two polynomials:
\[\begin{aligned}
    f(x) &= 3x^2 - 2x + 1 \\
    g(x) &= 2x^3 + x^2 - 3x + 4
\end{aligned}\]

Then, the Sylvester matrix would be:
\[
S(f, g) = \begin{pmatrix}
    3 & -2 & 1 & 0 & 0 \\
    0 & 3 & -2 & 1 & 0 \\
    0 & 0 & 3 & -2 & 1 \\
    2 & 1 & -3 & 4 & 0 \\
    0 & 2 & 1 & -3 & 4 \\
\end{pmatrix}
\]
\end{mdexample}

\begin{definition}
    The determinant of the Sylvester matrix is called the \textbf{resultant}. For two polynomials \(f\) and \(g\) it is denoted by \(R[f,g]\).
\end{definition}

\begin{mdlemma}
    The polynomials \(f\) and \(g\) have a common factor if and only if \(R[f,g]=0\). In particular, if \(K\) is algebraically closed and \(R[f,g]=0\) then \(f\) and \(g\) have a common root.
\end{mdlemma}

\begin{definition}
    A \textbf{hyperplane} is a flat surface in \(n\)-dimensional space of dimension \(n-1\). It is defined by an equation of the form \(a_1x_1 + a_2x_2 + \ldots + a_nx_n = b\).
\end{definition}

\begin{proposition}
    Let \(F(x_1,\ldots,x_n)\) be a polynomial vanishing on the hyperplane \(a_0 + a_1x_1 + a_2x_2 + \ldots + a_nx_n=0\). Then \(F(x_1\ldots,x_n)\) is divisible by \(a_0 + a_1x_1 + a_2x_2 + \ldots + a_nx_n\).
\end{proposition}

\begin{mdremark}
    We take \(a_0 = -b\).
\end{mdremark}

\begin{mdprop}[Resultant in terms of roots]
    Suppose 
    \[\begin{aligned}
        f(t)&= \alpha_0(t-\alpha_1) \cdots (t-\alpha_n) \\
        g(t)&= \beta_0(t-\beta_1) \cdots (t-\beta_m) \\
    \end{aligned}\]
    then 
    \[R[f,g]= \alpha_0^m \beta_0^n \prod_{i,j} (\alpha_i-\beta_j).\]
\end{mdprop}

\begin{mdcor}
    Let \(f,g \in K[X]\) of degrees \(n\) and \(m\) respectively. We have the following. 
    \begin{enumerate}
        \item \(R[t^n,(t-1)^m]=(-1)^{mn}\).
        \item \(R[\alpha f,\beta g] = \alpha^m \beta^n R[f,g]\).
        \item \(R[f(\alpha x),g(\alpha x)] = \alpha^{mn}R[f,g]\).
    \end{enumerate}
\end{mdcor}

\subsection{Pascal and Bézout's theorems}

\begin{mdnote}
    Before we state Bézout's theorem we state a lemma necessary for its proof.
\end{mdnote}

\begin{mdlemma}
    We have the following.
    \begin{enumerate}
        \item Let \(K\) be an infinite field. If the number of solutions of \(F(x,y)=G(x,y)=0\) in \(K^2\) is finite, then it is at most \(\deg(F)\deg(G)\).
        \item Let \(K\) be a field, let \(f \in K[X,Y]\) be an irreducible polynomial and let \(g\in K[X,Y]\) be an arbitrary polynomial. If \(g\) is NOT divisible by \(f\) then the system of equations \(f(x,y)=g(x,y)=0\) has only a finite number of solutions.
    \end{enumerate}
\end{mdlemma}

\begin{mdnote}
    Bézout's theorem is a generalisation of the following.
\end{mdnote}

\begin{proposition}
    Let \(K\) be a field and let \(F \in K[X]\) be of degree \(d\). Then the equation \(F=0\) has at most \(d\) solutions.
\end{proposition}

\begin{mdthm}[Bézout]
    Let \(F,G \in K[X,Y]\) without common factors. Then the number of points in \(K^2\) where \(F(x,y)=0\) and \(G(x,y)=0\) \ul{is at most} \(\deg(F)\deg(G)\).
\end{mdthm}

\begin{proof}
    Let \(F\) and \(G\) be two polynomials without common factors. It follows from the lemma above (2) that the system of equations \(F=G=0\) has only a finite number of solutions. Indeed, we can decompose \(F\) into a product \(F = H_1\cdots H_k\) of irreducible polynomials and apply the lemma (2) to each pair \(H_i,G\). Hence, we can apply lemma (1) to \(F\) and \(G\).
\end{proof}

\begin{mdremark}
    In the lemma we assume that \(K\) is an infinite field so the proof of Bézout's theorem only works for infinite fields. However, the theorem also holds in finite fields. To prove this we extend \(K\) to its algebraic closure which is always infinite.
\end{mdremark}

\begin{definition}
    We define a few things.
    \begin{itemize}
        \item A \textbf{plane curve} (or \textbf{curve}) is the set of points for which a polynomial \(F(x,y)=0\) in \(K^2\).
        \item The \textbf{degree} of a curve \(F(x,y)=0\) is the degree of \(F\).
        \item A curve \(F(x,y)=0\) is called \textbf{irreducible} if \(F\) is an irreducible polynomial.
        \item If \(\deg(F)=2,3\) the curve \(F(x,y)=0\) is called a \textbf{conic} and a \textbf{cubic} respectively.
    \end{itemize}
\end{definition}

\begin{mdthm}[Pascal]
    Suppose that the points \(A_1,\ldots,A_6 \in \RR^2\) lie on a conic. Denote by \(A_iA_j\) the line containing \(A_i\) and \(A_j\). Then the three points 
    \begin{itemize}
        \item \(A_1A_2 \cap A_4A_5\),
        \item \(A_2A_3\cap A_5A_6\) and 
        \item \(A_3A_4\cap A_6A_1\)
    \end{itemize}
    lie on one line.
\end{mdthm}

\begin{proof}
    We assume the conic is \ul{irreducible} and given by an equation \(F=0\). 
    
    Let \({L_1,M_1,L_2,M_2,L_3,M_3}\) be linear functions on the plane vanishing on the lines \(A_1A_2, A_2A_3,\ldots,A_6A_1\) respectively. We consider the following one-parameter family of \ul{degree \(3\)} polynomials:
    \[G_{\lambda} = L_1L_2L_3 + \lambda M_1M_2M_3.\]
    We note that \(A_1,\ldots,A_6\) and the three points of intersections that we study lie on the cubic curve \(G_{\lambda}=0\) for any \(\lambda\). We choose a point \(p\) on the conic different from \(A_1,\ldots,A_6\) and choose \(\lambda_0\) such that \(G_{\lambda_0}(p)=0\). 

    The intersection of the conic \(F=0\) and the cubic \(G_{\lambda_0}=0\) contains at least \(7\) points: six points are \(A_i\) and then \(p\). For the sake of contradiction, suppose \(F\) and \(G_{\lambda_0}\) do not have any common factors, then we can apply Bézout's theorem and find that there are at most \(\deg(F)\deg(G)=2\cdot 3=6\) points of intersection, but we have \(7\) hence, \(F\) and \(G\) must have a common factor. We assume \(F\) is irreducible so, it follows that \(F\) is a factor of \(G_{\lambda_0}\), so we can write 
    \[G_{\lambda_0} = F \cdot L\]
    where \(\deg(L)=1\) (since \(\deg(G)=3\) and \(\deg(F)=2\)). The intersection points of the theorem lie on the curve 
    \[\left\{ G_{\lambda_0}=0 \right\} = \left\{ F\cdot L=0 \right\}\]
    but they do not lie on the conic \(F=0\), so they must lie on \(L=0\).
\end{proof}

\section{Polynomial method}

\begin{mdnote}
    The idea of this section: let \(E\) be a finite subset of a vector space \(K^n\). To obtain information about \(E\) we study non-zero polynomials of minimal possible degree that vanish on \(E\). We will study two problems which make use of this method.
\end{mdnote}

\begin{mdexample}
    If \(\abs{E}\leq n\) there is a polynomial of degree \(1\) that vanishes on \(E\). On the other hand, if \(K\) is finite and \(\abs{E}>\abs{K}^{n-1}\) then there is no non-zero polynomial of degree \(\leq 1\) that vanishes at \(E\). Indeed, any degree \(1\) polynomial is vanishing at a hyperplane contains exactly \(\abs{K}^{n-1}\) points.
\end{mdexample}

\begin{definition}
    A polynomial is \textbf{identically zero} if it is zero as an element of \(K[X_1,\ldots,X_n]\) that is, all of its coefficients are zero.
\end{definition}

\begin{mdexample}
    Any polynomial \(F \in K[X_1,\ldots,X_n]\) with \(\deg(F)=\abs{K}\) that vanishes at all points of \(K^n\) is identically zero.
\end{mdexample}

\subsection{Kakeya conjecture}

\begin{definition}
    A \textbf{Kakeya set} is a subset of a Euclidean space that contains a unit line segment in every direction.
\end{definition}

\begin{mdnote}
    The question regarding Kakeya sets is how big are they. Dvir proved the following conjecture. 

    For any \(n\) there exists \(c_n>0\) such that any Kakeya set \(E \subset K^n\) has cardinality at least \(c_n\abs{K}^n\).
\end{mdnote}

\begin{mdprop}
    We have that 
    \begin{itemize}
        \item The vector space of polynomials in \(K[X_1,\ldots,X_n]\) of degree \(\leq d\) has dimension \(\binom{n+d}{d}\).
        \item The dimension of the space of homogeneous degree \(d\) polynomials \(\binom{n+d}{d}\).
    \end{itemize}
\end{mdprop}

\begin{mdnote}
    We note that the \(n\) is the one from \(K[X_0,\ldots,X_n]\), so the extra coordinate of \(x_0\) is not counted.
\end{mdnote}

\begin{proof}[Sketch of proof]
    We do this by induction.
    \begin{itemize}
        \item \textbf{Base Case:} For \(n = 1\), the vector space of polynomials in \(K[X_1]\) of degree at most \(d\) has dimension \(d+1\).
        \item \textbf{Inductive Step:} Assume the statement holds for \(n = k\). For \(n = k+1\), the dimension of the vector space is \(\binom{k + d + 1}{d + 1}\).
    \end{itemize}
\end{proof}

\begin{mdlemma}
    Let \(L\) be a linear subspace of \(K[X_1,\ldots,X_n]\) and let \(E\) be a finite subset of \(K^n\) with \(\abs{E}<\dim(L).\)
    \begin{itemize}
        \item Then there is a non-zero polynomial \(P \in L\) vanishing on \(E\).
        \item If we denote by \(M\) the subspace of polynomials from \(L\) that vanish on \(E\) then \(\dim(M) \geq \dim(L)-\abs{E}\).
    \end{itemize}
\end{mdlemma}

\begin{proof}
    Denote by \(K^E\) the vector space of \(K\)-valued functions on \(E\) (i.e.\ elements in \(K^E\) are functions \(f:E \to K\)). We have the evaluation map 
    \[\text{ev}:L \to K^E.\]
    This map associates to a polynomial \(F \in L\) the function \(F(x_1,\ldots,x_n)\) on \(E\) (i.e.\ \({(x_1,\ldots,x_n) \in E}\)).
    The kernel of the evaluation map is \(M\). The inequality follows from the rank-nullity theorem:
    \[\begin{aligned}
        \dim(\ker(\text{ev}))&=\dim(L)-\dim(\text{Im}(\text{ev})) \\
        &\geq \dim(L)-\abs{E}.
    \end{aligned}\]
\end{proof}

\begin{mdcor}
    Some corollaries.
    \begin{enumerate}
        \item Let \(E \subset K^n\) be a subset such that \(\abs{E} < \binom{n+d}{d}\). Then there exists a non-zero polynomial \(F \in K[X_1,\ldots,X_n]\) vanishing on \(E\) with \(\deg(F)\leq d\).
        \item For any subset \(E \subset K^n\) there exists a non-zero polynomial \(F \in K[X_1,\ldots,X_n]\) vanishing on \(E\) with \(\deg(F) \leq n \abs{E}^{1/n}\).
    \end{enumerate}
\end{mdcor}

\begin{proof}
    We prove each statement in turn.
    \begin{enumerate}
        \item Denote by \(V_d\) the space of polynomials in \(K[X_1,\ldots,X_n]\) of degree at most \(d\). By a proposition above we have that 
        \[\dim(V_d) = \binom{n+d}{d}\]
        i.e. \(\dim(V_d) > \abs{E}\) (by the hypothesis). Then applying the lemma above with \(L=V_d\), the statement is proven.
        \item In the proof above set \(d = \floor{n \abs{E}^{1/n}}\) then, 
        \[\begin{aligned}
            \binom{n+d}{d} &= \frac{(n+d)!}{d!n!} = \frac{(d+1)\cdots(n+d)}{n!} \\
            &> \frac{(n \abs{E}^{1/n}) \cdots (n \abs{E}^{1/n}+n-1)}{n!} \\
            &\geq \frac{n^n}{n!}\abs{E} \\
            &\geq \abs{E}.
        \end{aligned}\]
    \end{enumerate}
\end{proof}

\begin{mdlemma}
    Let \(P \in K[x_1,\ldots,x_n]\) be a polynomial of degree at most \(\abs{K}-1\) which vanishes on a Kakeya set \(E \subset K^n\). Then \(P\) is identically zero.
\end{mdlemma}

\begin{proof}
    For the sake of contradiction, assume that \(P\) is not identically zero. Then it must have positive degree, we can write 
    \[P = \sum_{i=0}^{d} P_i \quad \text{where } 1\leq d \leq \abs{K}-1\]
    for \(d>0\), and \(P_i\) is the \(i^\text{th}\) homogeneous component. We will come to a contradiction by showing that \(P_d\) is identically zero. Fix a vector \(\bm{v} \in K^n \setminus \left\{ 0 \right\}\). We will prove that \(P_d(\bm{v})=0\). Since \(E\) is a Kakeya set, \(E\) contains a line with direction of \(\bm{v}\) i.e. for some \(\bm{x} \in K^n\) the set \(E\) contains the line \(\left\{ \bm{x}+t\bm{v} : t\in K \right\}\). We restrict \(P\) to this line. We obtain a polynomial of one variable in \(t\) namely, \(P(\bm{x}+t\bm{v})\). We have that \(P(\bm{x}+t\bm{v})=0\) for all \(t\in K\). Since \(P(\bm{x}+t\bm{v})\) is a polynomial in \(t\) of degree at most \(\abs{K}-1\) all its coefficients are zero by the Lemma preceding Bezout's theorem. We claim the coefficient of \(P(\bm{x}+t\bm{v})\) in front of \(t^d\) is \(P_d(\bm{v})\) and is \(0\). Indeed,
    \[\begin{aligned}
        P(\bm{x}+t\bm{v}) &= P_d(\bm{x}+t\bm{v})+\sum_{i=0}^{d-1}P_i(\bm{x}+t\bm{v})  \\
        &= \sum_{i\leq d} c_i t_i +t^d P_d(\bm{v}).
    \end{aligned}\]
    Hence, \(P_d\) is identically zero.
\end{proof}

\begin{mdprop}[Kakeya conjecture]
    For nay \(n\) there exists \(c_n>0\) such that any Kakeya set \(E \subset K^n\) is such that \(\abs{E}\geq c_n \abs{K}^n\).
\end{mdprop}

\begin{proof}
    We show that \(c_n = (2n)^{-n}\) works. For the sake of contradiction, we assume \(\abs{E} <(2n)^{-n}\abs{K}^n\). By a corollary above there is non-zero polynomial \(F\) with \(\deg(F) \leq \frac{\abs{K}}{2} <\abs{K}\) vanishing on \(E\). By the above lemma this is identically zero which is a contradiction.
\end{proof}

\subsection{The joints problem}

\begin{definition}
    Let \(\mathcal{L}\) denote a finite collection of lines in \(\RR^3\). The number of lines in \(\mathcal{L}\) is denoted by \(\abs{\mathcal{L}}\).
\end{definition}

\begin{definition}
    A point \(p \in \RR^{3}\) is a \textbf{joint} of \(\mathcal{L}\) if \(p\) lies in (at least) three lines of \(\mathcal{L}\) that are not coplanar. The number of joints of \(\mathcal{L}\) is denoted \(J(\mathcal{L})\).
\end{definition}

\begin{mdexample}
    \textbf{The joints problem}. For a given number \(\ell\in \NN\) we want to find 
    \[J(\ell)=\max_{\abs{\mathcal{L}}=\ell} J(\mathcal{L}).\]
    For example, we can consider \(\ZZ^3 \subset \RR^3\). Let \(S =\left\{ 01,\ldots,n-1 \right\}\). We define 
    \[\mathcal{L} = \text{lines parallel to \(x,y,z\)-axes intersecting \(S \times S \times S\)}.\]
    We have that \(J(\mathcal{L}) = n^3\) and \(\abs{\mathcal{L}}=3n^2\).
\end{mdexample}

\begin{mdnote}
    This allows us to make a guess to the joints problem: \(\abs{J(\ell)} \approx \ell^{3/2}\).
\end{mdnote}

\begin{lemma}
    If \(\mathcal{L}\) is a set of lines in \(\RR^3\) that determines \(J\) joints, then one of the lines contains at most \(3J^{1/3}\) joints.
\end{lemma}

\begin{proof}
    Let \(P\) be a polynomial of the \ul{lowest degree} that vanishes at all joints of \(\mathcal{L}\). By some previous corollary we have that \(\deg(P) \leq 3J^{1/3}\). For the sake of contradiction, suppose that each line in \(\mathcal{L}\) contains \(> 3J^{1/3}\) joints. Then \(P\) vanishes at all lines in \(\mathcal{L}\). It follows that \(\nabla P\) vanishes at all joints. However, at the same time for some \(i \in \left\{ 1,2,3 \right\}\) we have that \(\diffp{P}{{x_i}} \neq 0\) and so \(\diffp{P}{{x_i}}\) has degree \(\deg(P)-1\) which contradicts the minimality of \(P\).
\end{proof}

\begin{mdthm}
    We have that \(J(\ell) \leq 10 \ell^{3/2}\).
\end{mdthm}

\begin{proof}
    Let \(\mathcal{L}\) be a collection of \(\ell\) lines with \(J(\ell)\) joints. By the lemma above there is a line in \(\mathcal{L}\) that contains at most \(3J(\ell)^{1/3}\) joints. The number of joints of \(\mathcal{L}\) that do not lie on this line is at most \(J(\ell-1)\). Thus, we have
    \[J(\ell) \leq J(\ell-1)+3J(\ell)^{1/3}.\]
    Iterating this argument and using the fact that \(J(\ell_1\leq J(\ell_2))\) for \(\ell_1 \leq \ell_2\) we see that 
    \[\begin{aligned}
        J(\ell) &\leq J(\ell-1)+3J(\ell)^{1/3} \\
                &\leq J(\ell-2)+2\cdot 3J(\ell)^{1/3}\\
                &\leq \ell \cdot 3J(\ell)^{1/3}.
    \end{aligned}\]
    Hence, \(J(\ell)^{2/3}\leq 3\ell\).
\end{proof}

\section{Projective space}

\begin{mdnote}
    In Euclidean plane geometry, we need to separate the cases of pairs of lines which meet and parallel line which do not. Geometry becomes a lot simple if any two lines met possible ``at infinity''. There are various ways of arranging this and the most convenient method is to embed the \(\AA^2\) into \(3\)-dimensional space. To each point \(p \in \AA^2\) we can associate the line \(OP\). The lines parallel to the plane correspond to the points at infinity.
\end{mdnote}

\begin{definition}
    An \(n\)-dimensional \textbf{projective space} over a field \(K\) is the set of \(1\)-dimensional subspaces of the vector space \(K^{n+1}\). Equivalently, it is the set of lines in \(K^{n+1}\) which contain the origin. We denote this by \(\PP^n_K\) or just \(\PP^n\).
\end{definition}

\begin{proposition}
    The projective space associated to a vector space \(V\) is the set of \(1\)-dimensional subspaces of \(V\), and we denote it by \(\PP(V)\).
\end{proposition}

\begin{mdremark}
    We can make this notion more precise. For any \(K\)-vector space \(V\) we can define the projective space 
    \[\PP(V)= \left( V\setminus \left\{ 0 \right\} \right) / (\text{\(K^*\)-rescaling action } v \mapsto \lambda v, \text{ for all \(\lambda \in K^*\)}).\]
    This comes equipped with a quotient map \(\pi:V\setminus \left\{ 0 \right\} \to V\) such that \(v \mapsto [v]\) where \([v]=[\lambda v]\). By picking a basis for \(V\) we can suppose \(V= K^{n+1}\) and say 
    \[\begin{aligned}
        \PP^n &= \PP(K^{n+1}) \\
        &=\text{(space of straight lines in \(K^{n+1}\) through \(0\))}.
    \end{aligned}\]
\end{mdremark}

\begin{mdnote}
    As such we can think of points in the projective space as lines in the affine space.
\end{mdnote}

\begin{mdexample}
    We provide some examples of projective space.
    \begin{enumerate}
        \item \(\mathbb{P}^0 = \left\{ \text{one point} \right\}\). In the affine space this is a line therefore, is represented by a single point in projective space. It has dimension \(0\).
        \item We can think of \(\PP^1_{\RR}\) as a topological circle. This space is the set one-dimensional subsets of \(\RR^2\) which is the set of lines through the origin. 
        \item \(\PP^2_{\RR}\) is the sphere \(x^2+y^2+z^2=1\) quotient by the central symmetry. This symmetry maps a point \(P=(x,y,z)\) to \(P'=(-x,-y,-z)\).
        \item \(\PP^1_{\CC}\) can be identified as the two-sphere.
        \item \(\PP^3_{\RR}\) can be identified with \(SO(3,\RR)\).
    \item \(\PP^1_{\ZZ/p\ZZ}\) has \(p+1\) points. The projective line over a finite field $\zmod{p}$ consists of all one-dimensional subspaces of the vector space $(\zmod{p})^2$, excluding the zero vector. Each non-zero vector in $(\zmod{p})^2$ defines a unique one-dimensional subspace and thus a point on the projective line. However, scalar multiples of a vector define the same subspace, so each point corresponds to $p-1$ distinct vectors. Thus, the total number of points on the projective line is $\frac{p^2 - 1}{p - 1}$, which simplifies to $p+1$. 
    \end{enumerate}
\end{mdexample}

\begin{mdlemma}
    We have that \(\PP^n = \AA^n \cup \PP^{n-1}\).
\end{mdlemma}

\begin{proof}
    Consider the vector space \(V^{n+1}\) and let \(\AA^n\) be an affine subspace of \(V^{n+1}\) that does contain \(0\). Consider the subset of \(\PP^n\) consisting of lines in \(V^{n+1}\) that pass through \(0\) and intersect \(\AA^n\). Clearly, this subset is isomorphic to \(\AA^n\) as for each point \(x\in \AA^n\) we can take the line through \(x\) and \(0\). The piece of \(\PP^n\) not contained in this subset consists of all lines through \(0\) contained in the vector subspace \(V^n \subset V^{n+1}\) parallel to \(\AA^n\). This is precisely \(\PP^{n-1}\) so \(\PP^n = \AA^n \cup \PP^{n-1}\).
\end{proof}

\begin{definition}
    The described hyperplane \(\PP^{n-1}\subset \PP^n\) is called the \textbf{infinity} of \(\AA^n\).
\end{definition}

\begin{corollary}
    \(\PP^n = \AA^n \cup \cdots \cup \AA^0\).
\end{corollary}

\begin{definition}
    Let \( V^{n+1} \) be a vector space and \( W^{k+1} \subseteq V^{n+1} \) be a vector subspace. The set of lines through \(0\) in \( W^{k+1} \) is a subset of \( \mathbb{P}^n \) isomorphic to a projective space \( \mathbb{P}^k \). Such subsets of \( \mathbb{P}^n \) are called \textbf{linear subspaces}. If \( k = 1 \) it is called a \textbf{projective line}. If \( k = n - 1 \) it is called a \textbf{hyperplane}.
\end{definition}

\begin{mdprop}
    Assume \(l+m\geq n\). We have that \(\mathbb{P}^l \cap \mathbb{P}^m \subset \mathbb{P}^n\) is a linear subspace of dimension at least \(l+m-n\).
\end{mdprop}

\begin{proof}
    Let \(V^{n+1}\) be the vector space corresponding to \(\mathbb{P}^n\). Consider vector subspaces \(V^{l+1},V^{m+1} \subseteq V^{n+1}\) corresponding to \(\mathbb{P}^l\) and \(\mathbb{P}^m\). Then the intersection \(V^{l+1} \cap V^{m+1}\) is a vector subspace of \(V^{n+1}\) of dimension at least \(l + m + 1 - n\). All lines in this vector subspace going through 0 form a linear subspace of \(\mathbb{P}^n\) of dimension at least \(l + m - n\) and this is exactly the intersection \(\mathbb{P}^m \cap \mathbb{P}^l\).
\end{proof}

\begin{mdprop}
    Suppose \(k \leq n\). For any points \(x_1,\ldots,x_k \subset \mathbb{P}^n\) there is a linear subspace \(\mathbb{P}^{k-1} \subset \mathbb{P}^n\) containing \(x_1,\ldots,x_k\).
\end{mdprop}

\begin{proof}
    Consider lines \(l_1, l_2, \ldots, l_k\) in \(V^{n+1}\) that correspond to points \(x_1, \ldots, x_k\) in \(\mathbb{P}^n\). Then these lines are contained in a certain linear subspace \(V^k \subseteq V^{n+1}\). All lines in \(V^k\) passing through 0 correspond to a linear subspace \(\mathbb{P}^{k-1}\) in \(\mathbb{P}^n\) containing \(x_1, \ldots, x_k\).
\end{proof}

\begin{mdthm}[Desargues theorem]
    Let \(a,b,c,A,B,C\) be six points in \(\PP^3\) not contained in one plane and such that no three of these points lie on one line. Suppose that the lines \(aA, bB\) and \(cC\) intersect in one point. Then the points 
    \begin{itemize}
        \item \(ab\cap AB\),
        \item \(bc \cap BC\) and 
        \item \(ac\cap AC\)
    \end{itemize}
    lie on one line.
\end{mdthm}

\begin{proof}
    By our assumption and the proposition above there are unique \(\PP^2\) containing points \(a,b,c\) and points \(A,B,C\) respectively i.e.\ \(\PP^2(abc)\) and \(\PP^2(ABC)\). Then the line \(abc\cap ABC\) contains all three intersection points. Indeed, \(ab \subset abc\) and \(AB \subset ABC\) hence, \(ab \cap AB \subset abc\cap ABC\). The same reasoning is used for the other two intersections.
\end{proof}

\begin{figure}[H]
     \begin{center}
         \includegraphics[width=\textwidth]{./Resources/Desargues theorem.png}
     \end{center}
\end{figure}


\begin{mdthm}
    Suppose \(\mathbb{P}^k,\mathbb{P}^l,\mathbb{P}^m\) are linear subspaces in \(\mathbb{P}^n\) and that \(k+l+m\geq n-1\). Then there exists a projective line \(\mathbb{P}^1\subset \mathbb{P}^n\) that intersects all three subspaces.
\end{mdthm}

\begin{proof}
    If \(\mathbb{P}^l \cap \mathbb{P}^m \neq \emptyset\) then the statement is trivial (since there is a projective line that connects a point from this intersection with a point on \(\mathbb{P}^k\)).
    
    Suppose \(\mathbb{P}^l \cap \mathbb{P}^m = \emptyset\). Consider the minimal linear subspace of \(\mathbb{P}^n\) that contains \(\mathbb{P}^l\) and \(\mathbb{P}^m\). Then the following two statements hold
    
    \begin{enumerate}
        \item This subspace is the union of all projective lines that join a point of \(\mathbb{P}^l\) with a point of \(\mathbb{P}^m\).
        
        \item Moreover, this space has dimension \(l + m + 1\).
    \end{enumerate}
    
    We know that in \(\mathbb{P}^n\) we have \(\mathbb{P}^{l+m+1} \cap \mathbb{P}^k \neq \emptyset\) since \(((m + l + 1) + k) - n \geq 0\) by the assumptions of the theorem. Hence, there is a projective line intersecting \(\mathbb{P}^m\), and \(\mathbb{P}^k\).
\end{proof}

\subsection{Projective transformations}

Let \( V, W \) be vector spaces and \( T: V \rightarrow W \) be a linear transformation with \( \ker T = 0 \). Then any one-dimensional subspace in \( V \) is sent to a one-dimensional subspace in \( W \). Hence, \( T \) gives us a well-defined map

\[ \tau : \PP(V) \rightarrow \PP(W). \]

\begin{definition}
    A projective transformation from \( \PP(V) \) to \( \PP(W) \) is the map \( \tau \) defined by an invertible linear transformation \( T: V \rightarrow W \).
\end{definition}

\begin{example}
    Take \( \mathbb{P}^2_K \), take two projective lines \( L_1, L_2 \) in it and a disjoint point \( p \). The projection of \( L_1 \) to \( L_2 \) from \( p \) is the map that associates to a point \( x \in L_1 \) the intersection point of the unique projective line \( xp \) with \( L_2 \). This map is a projective transformation.
\end{example}

\subsection{Homogeneous coordinates}

\begin{definition}
    Consider the vector space \(K^{n+1}\) with coordinate \((x_0,\ldots,x_n)\) and let \(\PP^n_K = \PP(K^{n+1})\). 
    
    Let \(L \subset K^{n+1}\) be a line passing through \(0\). Denote \(p(L)\) the corresponding point in \(\PP^n_K\). We say that \([a_0:\cdots:a_n]\) are \textbf{homogeneous coordinates} of the point \(p(L)\) if \((a_0,\ldots,a_n) \in L\) and \(a_0,\ldots,a_n \neq 0\).
\end{definition}

\begin{mdnote}
    Recall that \(\PP^n\) is the quotient \(K^{n+1}\setminus \left\{ (0,\ldots,0) \right\}\) by the equivalence relation 
    \[(x_0,\ldots,x_n) \sim (\lambda x_0,\ldots,\lambda x_n) \quad \text{where } \lambda \in K\setminus \{0\}.\]
    We call a representative for an equivalence the \textbf{homogeneous coordinates} of that point in \(\PP^n\).
\end{mdnote}

\begin{mdremark}
    If \((a_0,\ldots,a_n) \in L\) then points on \(L\) can be written as \((\lambda a_0,\ldots,\lambda a_n)\) for \(\lambda\in K\). So for \(\lambda\neq 0\) we have that \([a_0:\cdots:a_n]\) is the same point of \([\lambda a_0:\cdots:\lambda a_n]\).
\end{mdremark}

\begin{mdthm}
    We can embed \(\AA^n\) into \(\PP^n\) by the map 
    \[(x_0,\ldots,x_n) \mapsto [x_0:\cdots:x_n:1].\]
    Any other homogeneous coordinates where the first coordinate is non-zero can be re-scaled to have first coordinate \(1\). Points with last coordinate equal to \(0\) are ``\textbf{points at infinity}''. 
\end{mdthm}

\begin{mdremark}
    A point \([x_0:\cdots:x_n:0]\) can be seen as a point in \(\PP^{n-1}\) by dropping the last coordinate.
\end{mdremark}

\subsubsection{Homogeneous equations}

\begin{definition}
    The \textbf{degree} of a polynomial is the highest of the degrees of the polynomial's monomials (individual terms) with non-zero coefficients. 
\end{definition}

\begin{mdremark}
    In a multivariate polynomial, we have that \(x_1^{\eps_1}\cdots x_n^{\eps_n}\) is also a monomial and the degree is given by the sum of the \(\eps_i\). For example, consider the polynomial \(P(x, y, z) = 3x^2y + 2xyz^2 + z^3\).
    \begin{itemize}
        \item The degree of the monomial \(3x^2y\) is \(2 + 1 = 3\).
        \item The degree of the monomial \(2xyz^2\) is \(1 + 1 + 2 = 4\).
        \item The degree of the monomial \(z^3\) is \(3\).
    \end{itemize}
    The highest degree among these monomials is \(4\). Therefore, the degree of the polynomial \(P(x, y, z)\) is \(4\).
\end{mdremark}

\begin{definition}
    A polynomial \(F \in K[X_0,\ldots,X_n]\) is called \textbf{homogeneous of degree \(d\)} if all its \ul{monomials} have degree \(d\). 
\end{definition}

\begin{mdexample}
    Example and counter example.
    \begin{itemize}
        \item The polynomial \(F=x+y^2\) is not homogeneous as the monomial \(x\) is of degree \(1\) and the monomial \(y\) is of degree \(2\).
        \item The polynomial \(G = ab+c^2+(d+e)^2\) is homogeneous, because all monomials have degree \(2\).
    \end{itemize}
\end{mdexample}

\begin{mdthm}
    If \([x_0 : \ldots : x_n]\) and \([y_0 : \ldots : y_n]\) represent the same point \(p \in \mathbb{P}^n\), then
\[
(x_0, \ldots, x_n) = \lambda(y_0, \ldots, y_n)
\]
with \(\lambda \in k \setminus \{0\}\). Hence, if \(f \in k[X_0, \ldots, X_n]\) is a homogeneous polynomial of degree \(d\), then
\[
f(x_0, \ldots, x_n) = \lambda^d f(y_0, \ldots, y_n)
\]

Thus, the actual value of \(f\) at \(p\) is not well-defined, but it is well-defined whether \(f\) is zero at \(p\).

\end{mdthm}

\begin{definition}
    Let \(F \in K[X_0,\ldots,X_n]\) be a degree \(d\) homogeneous polynomial. Define the subset
    \[X_F = \left\{ [a_0:\cdots:a_n] \in \PP^n :F(a_0,\ldots,a_n)=0 \right\} \subset \PP^n.\]
    We call \(X_F\) a \textbf{hypersurface} of degree \(d\).
\end{definition}

\begin{mdremark}
    If \(F(a_0, \ldots,a_n)=0\) then \(F\) is zero on the whole line \((\lambda a_0,\ldots,\lambda a_n)\) for \(\lambda \in K\). 
\end{mdremark}

\begin{mdexample}
    Some examples.
    \begin{itemize}
        \item If \(\text{deg}(F) = 1\), then \(\{F = 0\} = \mathbb{P}^{n-1}\). This is a \textbf{hyperplane}.
        \item Consider \(\PP^1_{\CC}\). Then \(F = \sum a_i z_0^i z_1^{d-i}\). In this case \(F\) can be decomposed as a product \(F = \prod_{k=1}^d (b_k z_0 + c_k z_1)\). So the equations \(F = 0\) defines the collection of points \((c_k : -b_k)\) in \(\mathbb{P}^1_{\mathbb{C}}\).
    \end{itemize}
\end{mdexample}

\begin{definition}
    A subset \(V \subset \PP^n_K\) is said to be a \textbf{projective variety} if there exist homogeneous polynomials \(f_1,\ldots,f_N \in K[X_0,\ldots,X_n]\) such that 
    \[[x_0:\cdots:x_n] \in V \iff f_i(x_0,\ldots,x_n)=0\quad \forall i\in \left\{ 1,\ldots,N \right\}.\]
\end{definition}

\begin{mdexample}
    Hypersurfaces in \(\PP^n\).
\end{mdexample}

\subsection{Switching coordinates}

\begin{mdnote}
    We saw that \(\mathbb{P}^n \setminus \mathbb{P}^{n-1} = \mathbb{A}^n\). This permits us to look at any hypersurface \(X_F \subseteq \mathbb{P}^n\) from ``different angles''. Namely, we can choose any hyperplane \(\mathbb{P}^{n-1} \subseteq \mathbb{P}^n\) and study the piece of \(X_F\) that lies in the corresponding \(\mathbb{A}^n\), i.e., \(X_F \cap (\mathbb{P}^n \setminus \mathbb{P}^{n-1})\).
    
    We explain next how to derive an equation for \(X_F \cap \mathbb{A}^n\) from the equation \(F=0\) defining \(X_F\).
\end{mdnote}

\begin{definition}
    The subset in \(\PP^n_K\) such that \(x_i \neq 0\) is denoted by \(U_i\). We can identify with \(\AA^n_K\) by the following: 
    \[[x_0:\cdots:x_i:\cdots:x_n] \in \PP^n_K \longleftrightarrow \left( \frac{x_0}{x_i},\ldots,\frac{x_n}{x_i} \right) \in \AA^n_K.\]
\end{definition}

\begin{definition}
    The subset \(U_0 \subset \PP^n\) where \(x_0 \neq 0\) is given by \(\AA^n\). The points in \(\PP^n\setminus U_0\) are \textbf{points at infinity} of \(U_0 = \AA^n\). 
\end{definition}

\begin{mdcor}[Switching coordinates]
    We relate homogeneous equations in \(\PP^n\) and non-homogeneous equations in \(U_0\) as follows.
    \begin{itemize}
        \item \(\PP^n \to U_0\) we do the following substitution \(x_0 \mapsto 1\).
        \item \(U_0 \to \PP^n\) we replace each variable by \(\frac{x_i}{x_0}\) then multiply by an appropriate power of \(x_0\) to make the polynomial homogeneous.
    \end{itemize}
\end{mdcor}

\begin{mdexample}
    We show some examples:
    \begin{itemize}
        \item The homogeneous polynomial \(x^2_0+x_0x_1+x_0x_2+x_1x_2\) is transformed into a non-homogeneous polynomial by setting \(x_0 =1\).
        \item The non-homogeneous polynomial \(f(x_1,x_2,x_3)=x_1^2x_2^2+x_3^3+1\) can be turned into a homogeneous by introducing a new variable \(x_0\) and setting each \(x_i\) to \(\frac{x_i}{x_0}\) then multiplying by \(x_0^4\) to make it homogeneous. 
        
        We have that \(f\left(\frac{x_1}{x_0},\frac{x_2}{x_0},\frac{x_3}{x_0} \right) = \frac{x_1^2x_2^2}{x_0^4}+\frac{x_3^3}{x_0^3}+1\). To make this polynomial homogeneous we multiply by \(x_0^4\) and obtain \(f(x_0,x_1,x_2,x_3)=x_1^2x_2^2+x_0x^3_3+x_0^4\).
    \end{itemize}
\end{mdexample}

\subsection{Quadratic forms and conics}

\begin{mdnote}
    In this section we study equations of degree \(2\). We assume that the field \(L\) is algebraically closed and the characteristic of \(K\) is not equal to \(2\).
\end{mdnote}

\begin{definition}
    A \textbf{quadratic form} on a vector space \(V\) is a homogeneous polynomial of degree \(2\).
\end{definition}

\begin{definition}
    A quadratic form \(F\) is \textbf{diagonal} in coordinates \(x_1,\ldots,x_n\) if \(F = \sum_i a_i x_i^2\). We say \(F\) is \textbf{diagonalisable} if it is diagonal in some linear coordinates.
\end{definition}

\begin{definition}
    A \textbf{symmetric bilinear form} \(Q\) on \(V\) is a function in two variables \(u,v \in V\) satisfying 
    \[\begin{aligned}
        Q(u,v) &=Q(v,u) \quad \text{and} \\
        Q(\alpha u_1+\beta u_2,v) &= \alpha Q(u_1,v) +\beta Q(u_2,v).
    \end{aligned}\]
\end{definition}

\begin{mdthm}
    Let \(F\) be a quadratic form on \(V\). We can associate a symmetric bilinear form to \(F\):
    \[Q(u,v) = \frac{F(u+v)-F(u)-F(v)}{2}.\]
    Furthermore, we have \(F(v)=Q(v,v)\).
\end{mdthm}

\begin{mdlemma}
    For any quadratic form \(F \neq 0\) on \(K^n\) there exists a basis \(v_1,\ldots,v_n\) such that 
    \[F(x_1v_1+\cdots+x_n v_n)=x_1^2+\cdots+x_i^2 \quad \text{for some } i\in\left\{ 1,\ldots,n \right\}.\]
\end{mdlemma}

\subsubsection{Conics}

\begin{definition}
    Let \(F \in K[X,Y,Z]\) be a homogeneous polynomial of degree \(2\). Then the curve \(\left\{ F=0 \right\} \subset \PP^2_K\) is called a \textbf{conic}. 
\end{definition}

\begin{definition}
    The conic \(\left\{ F=0 \right\}\) is called \textbf{irreducible} if \(F\) is an irreducible polynomial.
\end{definition}

\begin{mdlemma}[Characterisation of conics]
    Let \(F=0\) be an irreducible conic in \(\PP^2_K\). Then in some homogeneous coordinates \([x:y:z]\) the conic is given by the equation \(xz-y^2=0\).
\end{mdlemma}

\begin{proof}
    By a lemma above in some coordinates the equation for \(F\) is 
    \begin{itemize}
        \item \(x'^2=0\),
        \item \(x'^2+y'^2=0\) or 
        \item \(x'^2+y'^2+z'^2=0\).
    \end{itemize}
    The first two equations correspond to reducible conics, so we are in the third case. Now consider the new coordinates
    \begin{itemize}
        \item \(x'=(x-z\sqrt{-1})\),
        \item \(y'=y\sqrt{-1}\),
        \item \(z'=(x+z\sqrt{-1})\).
    \end{itemize}
\end{proof}

\begin{mdcor}
    Any irreducible conic in \(\PP^2_K\) can be parametrised by \(\PP^1_K\).
\end{mdcor}

\begin{proof}
    By the lemma above we can assume that the conic \(C\) can be given by \(xz-y^2=0\). Then the parametrisation \(\phi:\PP^1_K \to C\) can be given as follows:
    \[\phi(u:v)=[u^2:uv:v^2].\]
    This map is called the \textbf{rational parametrisation} of the conic. Clearly the image of this map belongs to \(C\) and the map is injective. Moreover, the map is surjective. 
\end{proof}

\begin{mdlemma}
    Let \(F_d\in K[X,Y,Z]\) be a homogeneous polynomial of degree \(d\) and let \(C \subset \PP^2_K\) be a conic. We have the following either 
    \[F_d \equiv 0 \quad\text{OR}\quad \{F_d = 0\} \cap C \text{ contains no more than \(2d\) points.}\]
\end{mdlemma}

\begin{mdremark}
    By \(F \equiv 0\) we mean that all coefficients are \(0\).
\end{mdremark}

\begin{mdnote}
    This is an alternative proof to Bezout's theorem.
\end{mdnote}

\begin{proof}
    We choose coordinates \([x:y:z]\) so that the equation of \(C\) is given by \(xz-y^2=0\). We consider the rational parametrisation 
    \[\begin{aligned}
        \PP^1 &\to C \\
        [u:v] &\mapsto [u^2:uv:v^2].
    \end{aligned}\]
    We have that \(F_d(u^2,uv,v^2)\) is a homogeneous polynomial of degree \(2d\) in \(u\) and \(v\). The points of intersection of \(C\) with \(F_d=0\) correspond to the points \([u:v] \) on \(\PP^1_\CC\) such that \(F_d(u^2,uv,v^2)=0\). The last polynomial is a homogeneous polynomial of degree \(2d\) unless it is identically zero. If it is non-zero then it factors into the product of \(2d\) linear polynomials and so \(\#\left\{ C \cap \left\{ F_d=0 \right\} \right\} \leq 2d\), otherwise clearly \(C \subset F_d  =0\).
\end{proof}

\begin{mdlemma}
    For any five points in \(\PP^2\) there is a conic in \(\PP^2\) that contains them.
\end{mdlemma}

\begin{mdremark}
    We have that
    \begin{enumerate}
        \item The set of non-zero homogeneous polynomials of degree \(2\) in \(K[X,Y,Z]\) up to multiplication by a constant is \(\PP^5\).
        \item Each conic defines a point in \(\PP^5\).
        \item All conics passing through a point in \(\PP^2\) form a hyperplane in \(\PP^5\).
        \item Since any five hyperplanes in \(\PP^5\) intersect, there is a conic through any \(5\) points in \(\PP^5\).
    \end{enumerate}
\end{mdremark}

\begin{proof}
    Let \(\AA^3\) be the vector space corresponding to \(\PP^2\). Pick \(5\) non-zero points \(a_1,\ldots,a_5\) on the lines in \(\AA^3\) corresponding to point \(A_1,\ldots,A_5\). By a lemma in the Kakeya conjecture section, there exists a homogeneous polynomial \(F(x,y,z)\) of degree \(2\) vanishing \(a_i\) (the space of such polynomials has dimension \(6\)). Therefore, \(F=0\) defines on \(\PP^2\) a conic passing through \(A_i\).
\end{proof}

\begin{definition}
    Let \(V\) be a vector space and \(V^*\) be its dual i.e.\ the space of all linear functions on \(V\). Then the space \(\PP(V^*)\) is called \textbf{dual} to \(\PP(V)\).
\end{definition}

\begin{mdprop}
    Let \(A_1,\ldots,A_5\) be points in \(\PP^2\).
    \begin{enumerate}
        \item If no \(4\) points out of \(A_1,\ldots,A_5\) lie on one line then the conic passing through \(A_1,\ldots,A_5\) is unique.
        \item If no \(3\) of the \(A_i\) lie on one line then the conic is irreducible.
    \end{enumerate}
\end{mdprop}

\begin{proof}
    We prove each statement.
    \begin{enumerate}
        \item We prove each direction.
        \begin{itemize}
            \item Proof of \((\lthen).\) Suppose \( A_1, \ldots, A_4 \) lie on one line \( L = 0 \). Then the number of conics containing \( A_1, \ldots, A_5 \) is infinite. Indeed, for any line \( L' = 0 \) that contains \( A_5 \) the degenerate conic \( LL' = 0 \) contains all five points.
            \item Proof of \((\then).\) Let \( M \) be the space of polynomials of degree at most 2, vanishing at \( A_i \). By the Lemma (Kakeya?) from the course \(\dim(M) \geq 6 - 5 \geq 1 \). So there is at least one conic containing \( A_1, \ldots, A_5 \).

            Assume now that no four points out of \( A_1, \ldots, A_5 \) lie on one line. We will prove that the conic is unique, in other words \(\dim(M) = 1\). We will treat separately two cases.
            \begin{itemize}
                \item \textit{Generic case, no three point out of} \( A_1, \ldots, A_5 \) \textit{lie on one line}. Assume by contradiction that \(\dim(M) \geq 2\). Pick a point \( p \) on the line \( A_1A_2 \). Then for some non-zero \( F \in M \) we have \( F(p) = 0 \). So the intersection of the conic \( F = 0 \) with the line \( A_1A_2 \) contains at least three point. From Bezout theorem it follows that if \( F \) is reducible, and \( F = L_1 \cdot L_2 \) where \( L_1 = 0 \) defines the line \( A_1A_2 \) and \( L_2 \) defines a second line. Since none of \( A_3, A_4, A_5 \) lie on \( A_1A_2 \), these three points lie on \( L_2 = 0 \), and we get a contradiction.
                \item \textit{Non-generic case, three points, say} \( A_1, A_2, A_3 \) \textit{lie on one line} \( L = 0 \). Let \( Q = 0 \) be any conic, containing \( A_1, \ldots, A_5 \). From Bezout theorem it follows that \( Q = L \cdot L_1 \), where \( L_1 \) has degree one. We see, that \( L_1 = 0 \) is the line passing through \( A_4 \) and \( A_5 \). So the conic \( Q \) is unique.
            \end{itemize}
        \end{itemize}
        \item Take the conic \( Q = 0 \) that contains \( A_1, \ldots, A_5 \). If \( Q \) is reducible, i.e., \( Q = L_1L_2 \), then it is clear that at least one of the lines \( L_1 = 0 \) or \( L_2 = 0 \) contains 3 points out of \( A_1, \ldots, A_5 \). And also, if three points out of 5 are contained on a line \( L = 0 \), then by Bezout theorem \( L \) divides \( Q \), i.e., \( Q \) is reducible.
    \end{enumerate}
\end{proof}

\subsubsection{Bitangent lines and Steiner's problem}

\begin{definition}
    We say that a line is \textbf{tangent} to an irreducible conic if it intersects it in exactly one point.
\end{definition}

\begin{mdthm}
    There are at most \(4\) lines which are tangent to two distinct irreducible conics in \(\PP^2_{\CC}\).
\end{mdthm}

\begin{proof}
    The set of all lines in \(\PP^2_{\CC}\) is the dual \((\PP^2_{\CC})^*\). To find the number of bitangent lines we claim that the set of lines tangent to an irreducible conic \(C \subset \PP^2\) is a conic in \((\PP^2_{\CC})^*\). By Bézout's theorem two conics in \((\PP^2_{\CC})^*\) intersect in at most \(4\) points. It follows that the number of bitangents lines is \(\leq 4\). Indeed, we can write the equation of a line with \(ax+by+cz=0\) which is tangent to the conic \(xz-y^2=0\) if and only if the equation \(au^2+buv+cv^2\) in \([u:v]\) has only one root. It follows that \(b^2-4ac=0\) which is a conic in \((\PP^2_{\CC})^*\).
\end{proof}

\section{Cubic curves}

\begin{mdnote}
    Throughout this section we assume \(K\) is an algebraically closed field of characteristic \(0\). The main goal will be to study curves of degree \(3\) in \(\PP^2_K\).
\end{mdnote}

\begin{mdthm}
    Any two curves in \(\PP^2_K\) have a point of intersection.
\end{mdthm}

\subsection{Weierstrass normal form of cubic curves}

\begin{mdnote}
    In this section, we will show that every irreducible cubic in \(\PP^2_K\) can be put in \textit{Weierstrass form}.
\end{mdnote}

\begin{definition}
    Let \(F \in K[X_1,\ldots,X_n]\) be a homogeneous polynomial. A point \(P\) on the hypersurface \(F=0\) is called \textbf{singular} if 
    \[\diffp{}{{x_i}} F(P)= 0 \quad \text{for all } i =0,1,\ldots,n.\]
    Furthermore, 
    \begin{itemize}
        \item the point \(P\) is called \textbf{smooth} or \textbf{non-singular} if \(\nabla F(P)\neq 0\).
        \item The hypersurface \(F=0\) is called \textbf{smooth} if all of its points are smooth.
        \item The \textbf{tangent plane} to a hypersurface \(F=0\) at a smooth point \(P \in \left\{ F=0 \right\}\) is the hyperplane defined by 
        \[\sum_{i=0}^N \diffp{}{{x_i}}F(P)x_i =0.\]
    \end{itemize}
\end{definition}

\begin{definition}
    A projective line \(L\) is called \textbf{tangent} to a hypersurface \(F=0\) at a point \(p\) if the restriction of \(F\) to \(L\) has a double root at \(p\). 
\end{definition}

\begin{theorem}
    Smooth curves in \(\PP^2_\CC\) are irreducible.
\end{theorem}

\begin{definition}
    Let \(F \in K[X_0,\ldots,X_n]\) be a polynomial. The \textbf{Hessian} of \(F\) is the determinant of the \((n+1)^2\) matrix \((\bm{H})_{i,j} = \diffp[2]{F}{{x_i x_j}}\).
\end{definition}

\begin{definition}
    A point \(P\) on a curve \(F=0\) is called an \textbf{inflection point} if \(\text{Hess}(F)(P)=0\). 
\end{definition}

\begin{mdnote}
    To find points of inflection on \(F\), we need to find the intersection of the Hessian curve of \(F\) with \(F\).
\end{mdnote}

\begin{mdthm}
    Let \(C\) be an irreducible cubic in \(\PP^2_K\). Then for some homogeneous coordinates \([x:y:z]\) on \(\PP^2_K\) the equation for \(C\) is the following:
    \[xz^2=y^3+ax^2y+bx^3\]
    where \(z^2 = f(y)\).
\end{mdthm}

\begin{mdnote}
    This theorem is saying that a cubic of the form 
    \[y^2 = ax^3+bx^2+cx+d \quad \text{in  \(\AA^3\)} \quad  \sim \quad y^2z=ax^3+bx^2z+cxz^2+dz^3 \quad \text{in \(\PP^2\)}.\]
    In particular, the only point at infinity is \([0:1:0]\).
\end{mdnote}

\subsection{A lemma on 8 points}

\begin{mdthm}
    Let \(p_1,\ldots,p_8\) be points in \(\PP^2\) such that no \(4\) lie on one line and no \(7\) lie on one conic. Let \(M\) be the space of homogeneous polynomials of degree \(3\) in \(K[X,Y,Z]\) vanishing at \(p_1,\ldots,p_8\). Then \(\dim(M)=2\). 
\end{mdthm}

\begin{proof}
    Without loss of generality, we can assume that the \(p_i's\) are not in the plane \(x=0\). So, we consider \(x\neq 0\) and also consider polynomials of degree \( \leq 3\) in \(K[Y,Z]\). By some lemma in the Kakeya conjecture section we have \(\dim(M)\geq 2\). 
    
    We will assume that \(\dim(M)>2\) and will get a contradiction. We consider \(3\) situations.
    \begin{itemize}
        \item Generic case: no \(3\) points on one line and no \(6\) points lie on one conic. Let \(L=0\) be the line passing through \(p_1\) and \(p_2\). We pick two distinct points from \(p_1\) and \(p_2\), say \(q\) and \(r\). By a lemma in the Kakeya conjecture section there is an \(F \in M\) vanishing at \(q\) and \(r\). Then the cubic \(F=0\) intersects \(L=0\) in four points hence, by Bezout we have \(F= L\cdot Q\), where \(Q\) is a conic i.e.\ \(p_3,\ldots,p_8\) lie on \(Q\) which is a contradiction

        \item \(3\) points  lie one line \(L=0\). Let \(Q=0\) be a conic passing through \(p_4,\ldots, p_8\) by a lemma this conic is not unique. We also, note that \(F =L\cdot Q\) belongs to \(M\).  
        
        Now, we consider polynomials \(F_1,F_2\in M\) such that \(F,F_1,F_2\) are linearly independent. We pick a point \(p\) on \(L=0\). Then a linear combination \(aF_1+bF_2\) is vanishing on \(p\) and by Bezout \(aF_1+bF_2\) is divisible by \(L\). Hence, is proportional to \(L \cdot Q=F\) which is a contradiction.
        \item \(6\) points lie on a conic \(Q=0\). This is similar to \((2)\), and we should consider \(F= Q\cdot L\) where \(L=0\) joins \(p_7\) and \(p_8\).
    \end{itemize}
\end{proof}

\subsection{A group law on cubic curves}

Let \(C \subset \PP^2_K\) be a cubic curve, we define on \(C\) a structure of abelian group.
\begin{itemize}
    \item The identity element is a fixed point which we denote \(O \in C\). 
    \item The operation (addition) is defined as follows. We first need to define a different operation: take points \(P\) and \(Q\) on \(C\) and pass a line through these points. The third point of intersection on the curve is denoted by \(P*Q\). TO define the addition we construct a line through \(O\) and \(P*Q\) then \(P+Q\) is defined as the third point of intersection of the line with \(C\) by \(P+Q\). That is, \(P+Q = O*(P*Q)\). 
\end{itemize}

\begin{figure}[H]
     \begin{center}
         \includegraphics[scale=1]{./Resources/Addition on curve.png}
     \end{center}
\end{figure}

There are some special cases we need to consider when defining \(P*Q\):
\begin{itemize}
    \item If the third point of intersection does not exist then \(P*Q=Q\).
    \item If \(P=Q\) then we draw the tangent line at \(P\) then the intersection with the curve is \(P*Q\).
\end{itemize}

\begin{mdlemma}
    The operation of addition is commutative and associative.
\end{mdlemma}

\begin{proof}
    We prove each property.
    \begin{itemize}
        \item Commutativity is clear since \(P*Q\) is uniquely defined by \(P\) and \(Q\). 
        \item Associativity. We need to consider this figure.
        \begin{figure}[H]
             \begin{center}
                 \includegraphics[scale=0.8]{./Resources/Associativity on curve.png}
             \end{center}
        \end{figure}
        Let \(F=0\) be the equation of \(C\) and let \(P,Q,R\) be three points on \(C\) it suffices to show that \((P+Q)*R=P*(Q+R)\) since this is equivalent to \((P+Q)+R=P+(Q+R)\).

        Let \(G=0\) be the cubic composed of the lines \((P,Q),(O,Q+R),(R,P+Q)\) and let \(H=0\) be the cubic composed of the lines \((Q,R),(O,P+Q),(P,Q+R)\). We note that the \(8\) points 
        \[O,P,Q,R,(P*Q),(P+Q),(Q*R),(Q+R)\]
        lie on cubics, \(C,G=0,H=0\). We assume that all of these points are different, so we can use the lemma on \(8\) points. This theorem implies that \(F,G,H\) are linearly dependent: \(H = aF + bG\). We note that \(F\) and \(G\) vanish at \((P+Q)*R\) so by the linear dependence \(H\) vanishes as it as well. We also assume \((P+Q)*R\) is different from the \(8\) points. Then by the linear dependence \((P,Q+R)\) contains \((P+Q)*R\). 
    \end{itemize}
\end{proof}

\begin{mdthm}
    If \(O\) is a point at infinity (i.e.\ \([0:1:0]\)) then for a point \(P=(u,v)\) on the cubic we have \(-P=(u,-v)\).
\end{mdthm}

\begin{proof}
    Lines passing through \(xy\)-plane are vertical i.e. \(P\) and \(-P\) lie on a single vertical line.
\end{proof}

\subsection{Harnack's curve theorem}

\begin{mdnote}
    The main result is the Harnack's curve theorem which gives an exact upper bound on the number of connected components of a smooth curve of fixed degree.
\end{mdnote}

\begin{definition}
    Let \(F \in \RR[X,Y,Z]\) be a homogeneous polynomial of degree \(d\). Suppose that the curve \(F=0\) in \(\RR P^2\) is smooth. Then this curve is composed of several connected components (in the usual topology) which are all homeomorphic to a circle. We call such connected components \textbf{ovals}.
\end{definition}

\begin{definition}
    There are two types of ovals:
    \begin{itemize}
        \item Even ovals cut \(\RR P^2\) into a union of a disc and a Mobius band.
        \item Odd ovals do not cut it. The complement of such curves in \(\RR P^2\) is connected. 
    \end{itemize}
\end{definition}

\begin{example}
    An example of an odd oval is the real projective line.
\end{example}

\begin{mdthm}
    The number of ovals of an irreducible smooth plane curve of degree \(d\) is \(\leq \frac{(d-1)(d-2)}{2}+1\). 
\end{mdthm}

\begin{mdremark}
    From topology it follows that a smooth curve can have at most one odd oval. Indeed, any two odd ovals intersect, and so a curve with two odd ovals cannot be smooth.
\end{mdremark}

\begin{mdexample}
    We can ask how many ovals there can be in a smooth curve of degree \(d\). 
    \begin{itemize}
        \item If \(d=1\) corresponds to a line so, there is \(1\) oval.
        \item If \(d=2\) the number is \(1\) or \(0\) (for the latter a pair of distinct lines which do not intersect).
        \item If \(d=3\) the number is \(1\) or \(2\). 
    \end{itemize}
\end{mdexample}

\section{Higher dimensional projective varieties}

\begin{definition}
    We call the \textbf{Segre map} the map 
    \[\begin{aligned}
        \sigma: \PP^n \times \PP^m &\to \PP^{(n+1)(m+1)-1} \\
        \left( [x_0:\cdots:x_n],[y_0:\cdots:y_m] \right) &\mapsto [x_0y_0 : x_0y_1 : \cdots : x_0y_m : x_1y_0 : \cdots : x_iy_j:\cdots x_ny_m]
    \end{aligned}\]
    The image of the embedding is called the \textbf{Segre variety} \(\Sigma_{n,m}\).
\end{definition}

\begin{mdthm}
    The image of the Segre embedding can be identified with the set of \ul{rank one} \((m+1)\times(n+1)\) matrices (up to proportionality). Indeed, for each rank \(1\) matrix \((z_{ij})\) there exists vectors \((x_0,\ldots,x_n) \in K^{n+1}\) and \((x_0,\ldots,x_m) \in K^{m+1}\) such that \(z_{ij}=x_iy_j\) for all \(i\) and \(j\).
\end{mdthm}

\begin{proposition}
    The image of the Segre map is a projective variety in \(\PP^{(n+1)(m+1)-1}\) given by the system of homogeneous quadratic equations \(z_{ij}z_{kl}-z_{il}z_{kj}=0\).
\end{proposition}

\begin{corollary}
    Let \(X \subset \PP^n\) and \(Y \subset \PP^m\) be projective varieties. Then the image \(\sigma(X \times Y) \subset \PP^{(n+1)(m+1)-1}\) of the Segre map is a projective variety.
\end{corollary}

\section{Rings recap}

\begin{mdnote}
    The idea of section is to consider an algebraically closed field \(K\) and an affine variety \(V \subset K^n\). We will consider polynomials from \(K[X_1,\ldots,X_n]\) as functions on \(K^n\) and restrict these functions to \(V\). These restricted functions form a ring.
    We \textbf{claim} that if we understand this ring we understand \(V\).
\end{mdnote}

\subsection{Rings and ideals}

\begin{mdremark}
    We will only consider commutative rings in this course.
\end{mdremark}

\begin{definition}
    For a field \(K\) a \(K\)-\textbf{algebra} is a commutative ring containing \(K\) as a subring.
\end{definition}

\begin{mdexample}
    A typical example of a \(K\)-algebra is \(K[X_1,\ldots,X_n]\).
\end{mdexample}

\begin{definition}
    Let \(R\) be a ring and \(I \subseteq R\) be a proper ideal.
    \begin{itemize}
        \item The ideal \(I\) is said to be a \textbf{maximal ideal} of \(R\) if there is no ideal \(J\) of \(R\) such that \(I \subsetneq J \subsetneq R\).
        \item The ideal \(I\) is called \textbf{prime} if for any \(a,b\) with \(ab \in I\) we have that \(a \in I\) or \(b \in I\).
    \end{itemize}
\end{definition}

\begin{mdexample}
    We provide an example of a maximal ideal in the ring \(\CC[X]\). Consider the set points \(x_1,\ldots,x_n \in \CC\), now the set of polynomials which vanish at each of these points forms an ideal. However, not all ideals of this type are maximal; we have a maximal ideal when we consider the set of polynomial which vanish at one point --- this is a maximal ideal.
\end{mdexample}

\begin{definition}
    The \textbf{radical} of an ideal \(I\) in a commutative ring \(R\) is defined as 
    \[\text{Rad}(I) = \sqrt{I} =\left\{ r\in R : r^n \in I \text{ for some positive integer } n \right\}.\]
    An ideal \(I\) is called \textbf{radical} if \(I = \text{Rad}(I).\)
\end{definition}

\subsection{Finitely generated / Noetherian}

\begin{definition}
    Let \(R\) be a commutative ring and let \(A \neq \varnothing\) be a subset. The \textbf{ideal generated} by \(A\) is the smallest ideal of \(R\) containing \(A\). Equivalently, it is the collection of all finite linear combinations of elements of \(A\): 
    \[ (A) =\left\{ \sum_{i=1}^N b_ix_i : b_i \in R, x_i \in A \right\}.\]
    An ideal generated by the elements \(\left\{ x_1,\ldots,x_n \right\}\) is denoted as \((x_1,\ldots,x_n)\).
\end{definition}

\begin{definition}
    An ideal \(I \subset R\) is said to be \textbf{finitely generated} if there exists a finite subset \(A = \left\{ x_1,\ldots,x_n \right\} \subset R\) such that \(I = (x_1,\ldots,x_n)\).
\end{definition}

\begin{definition}
    A ring \(R\) is called \textbf{Noetherian} if every ideal of \(R\) is finitely generated.
\end{definition}

\begin{mdlemma}[Characterisation of Noetherian rings]
    Let \(R\) be a ring. The following conditions are equivalent:
    \begin{enumerate}
        \item The ideals of \(R\) satisfy the \textbf{Ascending chain condition} (ACC) i.e.\ if for the chain \({I_1 \subset I_2 \subset \cdots}\) are proper ideals of \(R\), then there exists an \(N\) such that for all \(n>N\) we have \(I_n =I_N\).
        \item Every ideal of \(R\) is finitely generated.
    \end{enumerate}
\end{mdlemma}

\begin{proof}
   We prove each direction in turn.  We prove the \textbf{contrapositive} statements.
   \begin{itemize}
    \item Proof of \((1) \then (2)\). Suppose an ideal \(I \subset R\) is not finitely generated. We will construct by induction an infinite sequence of elements \(x_1,\ldots,x_n,\ldots \in I\) such that the ideals \(I_n=(x_1,\ldots,x_n)\) form an infinite strictly increasing chain (which will contradict the ACC condition). Suppose we have already constructed \(x_1,\ldots,x_N\). Since \((x_1,\ldots,x_n) \neq I\) we can find \(x_{n+1} \in I\) such that \(x_{n+1} \not\in (x_1,\ldots,x_n)\).
    \item Proof of \((2)\then (1)\). If \(I_1 \subsetneq I_2 \subsetneq \cdots\) are proper ideal in \(R\) then \(I = \bigcup_{i\in \NN} I_i\) is a proper ideal in \(R\) (since it does not contain \(1\)). This ideal cannot be finitely generated. Indeed, for any collection of elements \(x_1,\ldots,x_n \subset I\) there exists a \(k\) such that \(x_1,\ldots,x_n \in I_k\). Therefore, \(x_1,\ldots,x_n\) do not generate \(I_{k+1}\). Hence, they do not generate \(I\). 
   \end{itemize}
\end{proof}

\begin{mdexample}
    We provide some examples and counterexamples of Noetherian rings.
    \begin{itemize}
        \item The ring \(\ZZ\) and \(K[X]\) are Noetherian since they are principal ideal domains, i.e.\ every ideal is generated by one element.
        \item The ring of continuous functions on \([0,1]\) is not Noetherian. Indeed, we can consider the sequence of ideals \(I_n\) of functions vanishing on \(\left[ 0,\frac{1}{n} \right]\).
        \item The ring of analytic functions in one variable is not Noetherian. Indeed, we can consider the chain of ideals \((\sin x), \left( \frac{\sin x}{x} \right), \left( \frac{\sin x}{(x(x-\pi))} \right),\ldots\), this clearly does not stabilise.
    \end{itemize}
\end{mdexample}

\subsection{Hilbert's basis theorem}

\begin{mdthm}
    Let \(R\) be a commutative ring. If \(R\) is Noetherian then so is \(R[X]\).
\end{mdthm}

\begin{proof}
    Suppose \( I \) is an ideal of \( R[X] \). We will prove that \( I \) is finitely generated.

First, we construct a sequence of embedded ideals in \( I_0 \subseteq I_1 \subseteq \ldots \subseteq I_n \subseteq \ldots \subseteq R \). The ideal \( I_i \) is generated by the leading coefficients of all polynomials of degree \( i \) in \( I \), i.e., if \( a_ix^i + a_{i-1}x^{i-1} + \ldots + a_0 \in I \), then \( a_i \in I_i \).

Let's show that \( I_i \subseteq I_{i+k} \) for \( k \geq 0 \). Indeed, if we multiply \( a_ix^i + a_{i-1}x^{i-1} + \ldots + a_0 \) by \( x^k \), the leading coefficient doesn't change. Also, since \( R \) is Noetherian, the chain of ideals \( I_i \) stabilizes for some \( n \) so that \( I_n = I_{n+1} = \ldots \).

We will now construct a finite set of polynomials, generating \( I \).

For each \( i = 0, \ldots, n \) choose \( S_i \) to be a finite set of polynomials in \( I\subset R[X] \) of degree \( i \) whose leading coefficients generate \( I_i \). We claim that the finite union \( S = S_0 \cup \ldots \cup S_n \) generates \( I \), in other words \( (S) = I \).

Proof of \( I = (S) \). Indeed, take a polynomial \( f = a_mx^m + a_{m-1}x^{m-1} + \ldots \in I \). Let's first show that there exists a polynomial \( g \in (S) \), of degree \( m \) with leading coefficient \( a_m \). If \( m \leq n \), then \( a_m \in I_m \) and so there is \( g \in (S_m) \) of degree \( m \) and with leading coefficient \( a_m \). If \( m > n \) then \( a_m \in I_n \), so we can find \( a_mx^n + \ldots + a_0 \in (S_n) \) and set \( g = x^{m-n}(a_mx^n + \ldots + a_0) \).

Now, \( f - g \) has degree at most \( m - 1 \) and \( g \in (S) \). Repeating the above procedure \( m \) times, we see \( f \in (S) \). I.e., \( (S) = I \).
\end{proof}

\begin{corollary}
    For any Noetherian ring \(R\) the ring \(R[X_1,\ldots,X_n]\) is Noetherian. In particular, we can take \(R\) to be equal to any field \(K\).
\end{corollary}

\begin{proof}
    We prove this by induction on \(n\). The base case is trivial since \(R\) is Noetherian. Suppose \(R[X_1,\ldots,X_n]\) is Noetherian we have an isomorphism 
    \[R[X_1,\ldots,X_{n+1}]=R[X_1,\ldots,X_n][X_{n+1}],\]
    i.e.\ the polynomial ring in \(n+1\) variables with coefficients in \(R\) can be seen as the polynomial ring in \(X_{n+1}\) with coefficient in \(R[X_1,\ldots,X_n]\). Hence, it is Noetherian by induction.
\end{proof}

\begin{definition}
    A commutative ring \(R\) is \textbf{finitely generated over its subring} \(A\) if there exists elements \(r_1,\ldots,r_n \subset R\) such that for any \(r \in R\) there is a polynomial \(P \in A[X_1,\ldots,X_n]\) such that \(r=P(x_1,\ldots,x_n)\).
\end{definition}

\begin{corollary}
    Let \(A\) be a Noetherian ring, and let \(R\) be a ring finitely generated over \(A\), then \(R\) is Noetherian.
\end{corollary}

\begin{proof}
    Suppose that \(R\) is generated by elements \((r_1,\ldots,r_n)\). Take the ring \(A[X_1,\ldots,X_n]\) (which is Noetherian) and consider the surjective homomorphism \(\phi:A[X_1,\ldots,X_n] \to R\) such that \(x_i \mapsto r_i\). Since, the ACC holds for ideals in \(A[X_1,\ldots,X_n]\) it must also hold for ideals in \(R\) because \(\phi\) is surjective and the pre-image of an ideal under \(\phi\) is an ideal.
\end{proof}

\section{Varieties}

\begin{mdnote}
    The goal of this section is to discuss the following equivalence for an algebraically closed field \(K\):
\[\text{Affine varieties in \(K^n\)} \iff \text{Radical ideals in \(K[X_1,\ldots,X_n]\)}.\]
\end{mdnote}

\subsection{From ideals to affine varieties}

\begin{mdnote}
    We construct a map that associates each ideal \(I \subset K[X_1,\ldots,X_n]\) the affine variety \(V(I)\) in \(\AA_K^n\). 
\end{mdnote}

\begin{definition}
    For each ideal \(I \subset K[X_1,\ldots,X_n]\) define 
    \[V(I) = \left\{ p \in \AA_K^n:f(p)=0 \; \forall f\in I \right\}.\]
\end{definition}

\begin{proposition}
    We have that \(V(I)\) is an affine variety.
\end{proposition}

\begin{proof}
    By Hilbert's basis theorem each ideal in \(K[X_1,\ldots,X_n]\) is generated by a finite number of elements \(f_1,\ldots,f_N\).
\end{proof}

\begin{mdprop}
    The map \(I \to V(I)\) from ideals in \(R = K[X_1,\ldots,X_n]\) to affine varieties in \(\AA_K^n\) has the following properties:
    \begin{enumerate}
        \item \(V(\left\{ 0 \right\})=\AA_K^n\) and \(V(R)=\varnothing\);
        \item If \(I \subset J \) then \(V(J)\subset V(I)\);
        \item \(V(I)\cup V(J)=V(I\cap J)\);
        \item \(\bigcap_m V(I_m)=V(\sum_m I_m)\) where \(\sum_m I_m\) denotes the ideal consisting of finite linear combinations of elements in \(I_m\) with coefficients in \(R\).
    \end{enumerate}
\end{mdprop}

\begin{proof}
    We prove each statement in turn.
    \begin{enumerate}
        \item We have.
        \begin{itemize}
            \item Let \( I = \{0\} \), the ideal consisting only of the zero polynomial. Any point in affine space \( \mathbb{A}^n \) satisfies all polynomials in \( I \), including the zero polynomial. Therefore, \( V(\{0\}) = \mathbb{A}^n \).
            \item Let \( I = K[X_1,\ldots,X_n] \), the ideal consisting of all polynomials with coefficients in field \( K \). Since \( I \) includes the constant polynomial \( 1 \), no point in affine space \( \mathbb{A}^n \) can satisfy all polynomials in \( I \), including the constant polynomial \( 1 \). Thus, \( V(K[X_1,\ldots,X_n]) = \varnothing \).
        \end{itemize}
        \item The more constraints in the form of equations we add, then we will have fewer solutions. 

        Given \( I \subset J \), let \( P \) be a point in \( V(J) \). This means that every polynomial in \( J \) vanishes at \( P \). Since \( I \) is a subset of \( J \), every polynomial in \( J \) is also in \( I \). Therefore, every polynomial in \( I \) vanishes at \( P \), which implies that \( P \) is also in \( V(I) \). Thus, we've shown that if \( P \) is in \( V(J) \), then \( P \) is in \( V(I) \), which means \( V(J) \subset V(I) \).
        \item We have to prove the double inclusion.
        \begin{itemize}
            \item We prove \(V(I)\cup V(J) \subset V(I \cap J)\). Since \(I \cap J \subset I\) by \((2)\) we have \(V(I) \subset V(I\cap J)\) and similarly \(V(J) \subset V(I \cap J)\).
            \item We prove \(V(I \cap J) \subset V(I) \cup V(J)\). Take a point \(p \in V(I \cap J)\). If \(p \not\in V(I)\) then \(f(p)\neq 0\) for some element \(f\in I\). Similarly, if \(p \not\in V(J)\) then \(g(p)\neq 0\) for some element \(g\in J\). Then \(f\cdot g \in I\cap J\) and \((f\cdot g)(p)\neq 0\), which contradicts the assumption that \(p\in V(I\cap J)\).
        \end{itemize}
        \item Take a point \(p \in \bigcap_m V(I_m)\) then \(f_m(p)=0\) for all \(f_m \in I_m\). It follows that \(f(p)=0\) for all finite linear combinations of elements of \(I_m\). So, \(p \in V\left( \sum I_m \right)\) and we conclude \(\bigcap_m V(I_m) \subset V(\sum I_m)\). 
        
        On the other hand, \(I_n \subset \sum I_m\) for every \(n\), hence \(V(\sum_m I_m) \subset V(I_n)\) for every \(n\) and therefore \(V(\sum_m I_m)\subset \bigcap_n V(I_n)\).
    \end{enumerate}
\end{proof}

\subsection{From varieties to ideals: the vanishing ideal}

\begin{definition}
    Let \(V \subset \AA^n\) be an affine variety. The \textbf{vanishing ideal} of \(V\) in \(K[X_1,\ldots,X_n]\) is 
    \[I(V)=\left\{ f\in K[X_1,\ldots,X_n] : f(p)=0 \; \forall p\in V \right\}.\]
\end{definition}

\begin{lemma}
    For any affine variety \(V\) the ideal \(I(V)\) is radical.
\end{lemma}

\begin{proof}
    If \(f \in \text{Rad}(I(V))\) then \(f^n \in I(V)\) for some \(n\) i.e.\ \(f^n\) vanishes at \(V\) for some \(n\). Therefore, \(f\) must vanish at \(V\) which implies that \(f\in I(V)\) hence, \(I(V)\) is radical.
\end{proof}

\begin{mdlemma}
    We have the following.
    \begin{enumerate}
        \item For any affine variety \(V\) we have \(V(I(V))=V\).
        \item For any ideal \(I\) we have \(I\subset I(V(I))\).
        \item If \(I\) is not radical \(I\subsetneq I(V(I))\).
    \end{enumerate}
\end{mdlemma}

\begin{proof}
    We prove each statement in turn.
    \begin{enumerate}
    \item We have to prove the double inclusion.
        \begin{itemize}
            \item \(\subset\). Take a point \(p \not\in V\) since \(V\) is an affine variety we have that \(V\) is given by a set of equations \(f_1=\cdots=f_n =0\). So, for some \(i\) we have that \(f_i(p)\neq 0\). Since \(f_i \in I(V)\) it follows that \(p\not\in V(I(V))\).
            \item \(\supset\). By definition of \(I(V)\) any \(f\in I(V)\) vanishes on \(V\). 
        \end{itemize}
    \item Any \(f\in I\) vanishes on \(V(I)\).
    \item This follows from \((2)\) and some lemma.
    \end{enumerate}
\end{proof}

\subsection{Hilbert's Nullstellensatz}

\begin{mdthm}[Hilbert's Nullstellensatz]
    If \(K\) is an algebraically closed field, then for each ideal \(J\) in \(K[X_1,\ldots,X_n]\) we have \(I(V(J)) = \text{Rad}(J)\).
\end{mdthm}

\begin{mdnote}
    This theorem provides a bijection between affine varieties and radical ideals.
\end{mdnote}

\begin{mdcor}
    The following two equivalent statements are called the \textbf{weak Nullstellensatz theorem}.
    \begin{enumerate}
        \item If \(K\) is an algebraically closed field, then for any ideal \(J \subset K[X_1,\ldots,X_n]\) we have that \(V(J)=\varnothing \iff J = K[X_1,\ldots,X_n]\).
        \item Every maximal ideal of \(K[X_1,\ldots,X_n]\) is of the form \(m_a=(X_1-a_1,X_2-a_2,\ldots,X_n-a_n)\), where \(a_1,\ldots,a_n \in K\). 
    \end{enumerate}
\end{mdcor}

\begin{proof}
    We prove each statement in turn.
    \begin{enumerate}
        \item We prove each direction.
            \begin{itemize}
                \item Proof of \((\then)\). Suppose \(V(J)= \varnothing\) then by the Nullstellensatz we have that \(\text{Rad}(J)=I(V(J))=I(\varnothing) =K[X_1,\ldots,X_n]\). Therefore, \(1 \in \text{Rad}(J)\) which implies \(1\in J\) as well.
                \item Proof of \((\lthen)\). If \(1 \in J\) then \(V(J)=\varnothing\).
            \end{itemize}
        \item Consider the evaluation map \(K[X_1\ldots,X_n] \to K\) that sends any polynomial \(f\) to its value at \(a\). The kernel of this map is precisely \(m_a\). It follows that the quotient \(K[X_1,\ldots,X_n]/m_a = K\) is a field hence, \(m_a\) is maximal.
    \end{enumerate}
\end{proof}

\begin{mdexample}
    Take the polynomials \(p, q\in \CC[X_1,\ldots,X_n]\). If \(p\) is irreducible and \(q\) vanishes at the hypersurface \(p(x_1,\ldots,x_n)=0\) then \(p\) divides \(q\). Indeed, \(q\) vanishes on \(p=0\) so by the Nullstellensatz \(q\in \text{Rad}((p))\) i.e.\ \(q^n \in (p)\) for some \(n\). At the same time \(\CC[X_1,\ldots,X_n]\) is a UFD so \(q\in (p)\).
\end{mdexample}

\subsection{The coordinate ring \texorpdfstring{\(K[V]\)}{TEXT}}

\begin{mdnote}
    We want to associate a ring to every variety as we did with ideals and varieties.
\end{mdnote}

\begin{definition}
    Let \(V \subset \AA_K^n\) be an affine variety. We define the \textbf{coordinate ring} of \(V\) as the quotient \(K[V] = K[X_1,\ldots,X_n]/I(V)\). The quotient map is \(\varphi : K[X_1,\ldots,X_n] \to K[V]\).
\end{definition}

\begin{mdexample}
    The coordinate ring \(\CC[\left\{ 0 \right\}]= \CC\). Clearly \(I(\left\{ 0 \right\}) = (x)\) so \(\CC[X]/(x)\cong \CC\).
\end{mdexample}

\begin{definition}
    A \(K\)-valued function \(f\) on \(V\) is \textbf{regular} if there is a polynomial \(F \in K[X_1,\ldots,X_n]\) such that restriction of \(F\) to \(V\) is \(f\), i.e. \(F\vert_V =f\).
\end{definition}

\begin{proposition}
    Let \(V \subset \AA_K^n\) be an affine variety. Then regular functions on \(V\) form a ring which is isomorphic to \(K[V]\).
\end{proposition}

\begin{mdprop}
    Let \(V\) be an affine variety. Points of \(V\) are in one-to-one correspondence with maximal ideals in \(K[V]\).
\end{mdprop}

\begin{proof}
    We prove each correspondence in turn.
    \begin{itemize}
        \item Points to maximal ideals. Take a point \(a\in V\) and let \(m_a \subset K[V]\) be the ideal of elements vanishing at \(a\). Consider the homomorphism \(K[V] \to K\) that takes a regular function \(f \in K[V]\) to \(f(a)\). Clearly, \(m_a\) is the kernel of the homomorphism, so \(K[V]/m_a\cong K\) and \(m_a\) is maximal.
        \item Maximal ideals to points. For any maximal ideal \(m \subset K[V]\) we find a point \(a\in V\) where all elements of \(m\) vanish. Since \(\varphi : K[X_1,\ldots,X_n] \to K[V]\) is surjective, we can consider the composition of the quotient homomorphism \(K[X_1,\ldots,X_n] \to K[V] \to K[V]/m\). The kernel of the map \({K[X_1,\ldots,X_n] \to K[V]/m}\) is given by \(\phi\inv(m)\) (the pre-image) so, by the isomorphism theorem we have that 
        \[K[X_1,\ldots,X_n]/\varphi\inv(m)\cong K[V]/m.\]
        Since \(m\) is maximal we have that \(K[V]/m\) is a field which implies the following ring \({K[X_1,\ldots,X_n]/\varphi\inv(m)}\) is also a field hence, \(\varphi\inv(m)\) is maximal. Using the weak Nullstellensatz we know that 
        \[\varphi\inv(m)=((X_1-a_1),\ldots,(X_n-a_n)),\]
        for some \(a\in K^n\). 

        We prove that \(a\in V\). Indeed, the above, all element of \(\varphi\inv(m)\) vanish at \(a\). At the same time, we have \(I(V)=\varphi\inv(0)\subset \varphi\inv(m)\). So all elements of \(I(V)\) vanish at \(a\) which implies \(a\in V\). Finally, \(m \subset K[V]\) is generated by restrictions of \(X_i-a_i\) to \(V\) so all the elements of \(m\) vanish at \(a\in V\).
    \end{itemize}
\end{proof}

\subsection{Regular maps}

\begin{definition}
    Let \(X \subset \AA_K^n\) and \(Y \subset \AA^m_K\) be affine varieties. A map \({f:X \to Y}\) is \textbf{regular} if there exists \(m\) polynomials \(f_1,\ldots,f_m \in K[X_1,\ldots,X_n]\) such that \(f(x)=\left( f_1(x),\ldots,f_m(x) \right)\) for all \(x\in X\).
\end{definition}

\begin{definition}
    A regular map \(f:X\to Y\) is called an \textbf{isomorphism} if it has a regular inverse. That is there exists a regular map \(g:Y \to X\) such that \(f\circ g=\id_X\) and \(g\circ f=\id_Y\). In this case we say that \(X\) and \(Y\) are isomorphic.
\end{definition}

\begin{mdexample}[Exam Question]
    Is there an isomorphism between \(\CC^1\) and the conic \(xy=1 \in \CC^2\).
    \begin{solution}
        It suffices to establish that \(\CC[X]\not\cong \CC[X,Y]/(xy=1)\). In the RHS we have that \(xy=1\) if this was an isomorphism then in LHS there must exist two polynomials whose product is \(1\) which is not possible unless they are constants.
    \end{solution}
\end{mdexample}

\begin{definition}
    Let \(f: X \to Y\) be a regular map between algebraic varieties \(X\) and \(Y\). Then for any regular function \(u\) on \(Y\), define the \textbf{pull-back} of \(u\), denoted \(f^*(u)\), as a function on \(X\) by the rule \(f^*(u)(x) = u(f(x))\), for each \(x \in X\). 
    
    It follows that \(f^*(u)\) is also a regular function on \(X\).
\end{definition}

\begin{mdexample}
    Let \(X \subset \AA_K^n\) be an affine variety and let \(f=(f_1,\ldots,f_m) : X \to \AA^m_K\) be a regular map. Let \(y_1,\ldots,y_m\) be coordinates on \(\AA_K^m\) then \(f^*(y_i)=f_i\).
\end{mdexample}

\begin{lemma}
    Let \(X\) and \(Y\) be affine varieties. Any homomorphism \(\varphi:K[Y] \to K[X]\) (as \(K\)-algebras) is of the form \(\varphi=f^*\) for some regular map \(f:X\to Y\).
\end{lemma}

\begin{proof}
    Let \(y_1,\ldots,y_m\) be coordinates in the space \(\AA^m_K\) of \(Y\). We have that \(y_i \in K[Y]\) hence \(\varphi(y_i) \in K[X]\). Set \(\varphi(y_i)=f_i\) and consider the map \(f(x)=(f_1(x),\ldots,f_m(x))\). The map \(f:X \to \AA^m_K\) is regular, and it satisfies the property \(f^*(y_i)=f_i=\varphi(y_i)\). 

    We now prove that \(f(X)\subset Y\). Take any point \(x\in X\) and any polynomial \(H \in I(Y)\). We show that \(H(f(x))=0\). Since \(H \in I(Y)\) we have that \(H(y_1,\ldots,y_m)\) is the zero element in \(K[Y]\). Then, clearly \(0=\varphi(H(y_1,\ldots,y_n))=H(f_1,\ldots,f_m) \in K[X]\) i.e.\ this expression vanishes on \(X\). We conclude, 
    \[H(f(x))=H(f_1(x),\ldots,f_m(x))=0.\]
    The claim holds for any \(H \in I(Y)\) so \(f(x)\in Y\).
\end{proof}

\begin{mdthm}
    Let \(X \subset \AA_K^n\) and \(Y\subset \AA_K^m\) be affine varieties. We have that \(X\) and \(Y\) are isomorphic if and only if their coordinate rings \(K[X]\) and \(K[Y]\) are isomorphic as \(K\)-algebras.
\end{mdthm}

\begin{proof}
    Suppose \(f:X \to Y\) is an isomorphism and \(g\) is its inverse. We prove that \(f^*:K[Y] \to K[X]\) and \(g^*:K[X]\to K[Y]\) are isomorphisms i.e.\ inverse to each other. 

    Indeed, \(fg\) is identical on \(Y\) so \((fg)^*=g^*f^*\) is the identity homomorphism from \(K[Y]\to K[Y]\) (by the definition of pullback). The same holds for \(f^*g^*\). 

    We now suppose that \(\varphi:K[Y]\to K[X]\) is an isomorphism the by the lemma above \(\varphi=f^*\) for some regular map \(f:X \to Y\) and \(\varphi\inv=g^*\) for some regular map \(g:Y \to X\). Note that \(gf\) induces the identity map \(f^*g^*=\varphi\varphi\inv : K[X] \to K[X]\). We also conclude that \(gf\) is identical and so \(f\) and \(g\) are inverse.
\end{proof}

\begin{mdexample}
    Consider \(\CC^1\) and the parabola \(y=x^2\). These two spaces are isomorphic. Clearly, the maps \((x,y) \mapsto x\) and \(x\mapsto(x,y)\) establish this. 
\end{mdexample}

\section{Hilbert's functions and Hilbert polynomials}

\subsection{Graded rings and modules and the Hilbert function}

\begin{definition}
    A \textbf{graded ring} is a ring with a direct sum decomposition
    \[S=S_0 \oplus S_1 \oplus \cdots \]
    satisfying the following properties:
    \begin{enumerate}
        \item Each \(S_i\) is closed under addition;
        \item \(S_iS_j \subset S_{i+j}\) for \(i,j\geq 0\);
        \item An element of \(S\) is a finite sum of elements from different \(S_i\).
    \end{enumerate}
\end{definition}

\begin{definition}
    An element \(s\in S\) is \textbf{homogeneous} if \(s\in S_i\) for some \(i\). A \textbf{homogeneous ideal} of \(S\) is an ideal generated by homogeneous elements. 
\end{definition}

\begin{example}
    \(S= K[X_1,\ldots,X_n]\) graded by degree 
    \[S = S_0 \oplus S_1 \oplus \cdots\]
    where \(S_d\) is the vector space of homogeneous polynomials of degree \(d\).
\end{example}

\begin{mdremark}
    We will only consider rings such that 
    \begin{enumerate}
        \item \(S_0 = K\),
        \item \(S\) is generated by \(S_0 \oplus S_1\),
        \item \(S_1\) is a finite-dimensional vector space over \(K\).
    \end{enumerate}
    In such rings each \(S_i\) is a finite-dimensional vector space over \(K\).
\end{mdremark}

\begin{definition}
    Let \(S\) be a graded ring finitely generated over \(S_0 = K\). The \textbf{Hilbert function of \(S\)} is defined as \(h_S(d)=\dim(S_d)\).
\end{definition}

\begin{mdexample}
    Let \(S = K[X_0,\ldots,X_N]\) then \(h_S(d)  = \binom{n+d}{d}\).
\end{mdexample}

\begin{definition}
    Let \(X \subset \PP^n_K\) be a projective variety. The \textbf{homogeneous ideal} \(I(X)\) is the ideal in \(K[X_0,\ldots,X_n]\) generated by all \ul{homogeneous} polynomials vanishing on \(X\).
\end{definition}

\begin{definition}
    The \textbf{homogeneous coordinate ring} \(S(X)\) of \(X\) is the quotient ring \(K[X_0,\ldots,X_n]/I(X)\).
\end{definition}

\begin{mdprop}
    We have that:
    \begin{itemize}
        \item \(I(X)\) is a graded ring, namely \(I(X) = \bigoplus_d I(X)_d\) where \(I(X)_d\) is the space of degree \(d\) polynomials in \(K[X_0,\ldots,X_n]\) vanishing on \(X\).
        \item \(S(X)\) is graded, namely \(S(X)_d =S_d/I(X)_d\).
    \end{itemize}
\end{mdprop}

\begin{proposition}
    Let \(X_F \subset \PP^n\) be a hypersurface defined by a homogeneous polynomial \(F\). We have that \(I(X_F) \subset K[X_0,\ldots,X_n]\) is the vanishing ideal of the variety \(V((F))\).
\end{proposition}

\begin{mdnote}
    In other words, every polynomial vanishing on \(\left\{ F=0 \right\} \subset \AA^{n+1}\) is a sum of homogeneous polynomials vanishing on \(\left\{ F=0 \right\}\). 
\end{mdnote}

\begin{proof}
    We prove the double inclusion of \(I(X_F) = I(V((F)))\).
    \begin{itemize}
        \item \(\subset\). Let \(f \in I(X_F)\). This means that \(f\) vanishes on the hypersurface \(X_F\). Thus, \(f\) vanishes at every point of \(X_F\), which implies that \(f\) vanishes on the variety \(V((F))\) as well. Hence, \(f \in I(V((F)))\).
        \item \(\supset\). Let \(G \in K[X_0,\ldots,X_n]\) and let 
        \[G = G_1 +\cdots +G_k\] be its decomposition into homogeneous components. Suppose \(G\) vanishes on \(V((F))\). Then each \(G_i\) vanishes on \(V((F))\). So each \(G_i\) vanishes on the hypersurface \(X_F\). So \(G = \sum_i G_i \subset I(X_F)\) i.e.\ \(I(V((F))) \subset I(X_F)\). 
    \end{itemize}
\end{proof}

\begin{definition}
    The \textbf{Hilbert function} \(h_X\) of a projective variety \(X\) is the Hilbert function of its homogeneous coordinate ring i.e. \(h_X(m)=\dim(S(X)_m)\).
\end{definition}

\begin{mdexample}
    The Hilbert function of \(\PP^n\) is \(h_{\PP^n}(d) = \binom{n+d}{n}\). Indeed, by definition the homogeneous coordinate ring \(S(\PP^n)\) of \(\PP^n\) is \(K[X_0,\ldots,X_n]\).
\end{mdexample}

\begin{definition}
    By \textbf{Hilbert function of an ideal} \(h_I\) we denote the Hilbert function on the ring \(K[X_0,\ldots,X_n]/I\).
\end{definition}

\begin{mdnote}
    The Hilbert function \( h_I(t) \) of an ideal \( I \) in a polynomial ring 
    measures the vector space dimension consisting of all polynomials of a given 
    degree \( t \) modulo the ideal.
\end{mdnote}

\begin{mdexample}
    For the given ideal 
\( I = (x_1^5, x_1x_2^2, x_2^3) \subset K[x_1, x_2] \), the Hilbert function 
counts the number of independent monomials of degree \( t \) that are not in 
\( I \).

To determine \( h_I(t) \), we count such monomials for each degree \( t \):

\begin{itemize}
    \item For \( t = 0 \), the monomial is \( 1 \), which gives \( h_I(0) = 1 \).
    \item For \( t = 1 \), the monomials are \( x_1 \) and \( x_2 \), thus \( h_I(1) = 2 \).
    \item For \( t = 2 \), the monomials are \( x_1^2, x_1x_2, \) and \( x_2^2 \), yielding \( h_I(2) = 2 \).
    \item For \( t = 3 \), the monomials are \( x_1^3, x_1^2x_2, \) and \( x_1x_2^2 \) 
    (since \( x_1x_2^2 \) is in \( I \)), giving \( h_I(3) = 2 \) (not \( 1 \) as incorrectly stated).
    \item For \( t = 4 \), \( x_1^4, x_1^3x_2, \) and \( x_1^2x_2^2 \) are the monomials not in \( I \), 
    so \( h_I(4) = 3 \).
    \item For \( t \geq 5 \), all monomials of that degree are divisible by at least one generator of \( I \), 
    hence \( h_I(t) = 0 \).
\end{itemize}

Therefore, the corrected Hilbert function should be:

\[
\begin{aligned}
h_I(0) &= 1, \\
h_I(1) &= 2, \\
h_I(2) &= 2, \\
h_I(3) &= 2, \\
h_I(4) &= 3, \\
h_I(t) &= 0 \text{ for } t \geq 5.
\end{aligned}
\]
\end{mdexample}



\subsection{Hilbert polynomials}

\begin{mdthm}
    For any projective variety \(X\) (or a homogeneous ideal \(I \subset K[X_0,\ldots,X_n]\)) the Hilbert function \(h_X(d)\) (or \(h_I(d) = \dim(S_d/I_d)\)) is equal to a certain polynomial for large enough \(d>0\).
\end{mdthm}

\begin{definition}
    Let \(X \subset \PP^n\) be a projective variety. The unique polynomial \(p_X\) such that \(p_X(d) = h_X(d)\) for large \(d\) is called the \textbf{Hilbert polynomial} of \(X\). 
\end{definition}

\begin{definition}
    Suppose \(p_X(d)= a_k d^k +\cdots a_0\) with \(a_k \neq 0\). The \textbf{dimension} of \(X\) is defined to be \(\dim(X)=k\). The \textbf{degree} of \(X\) is defined to be \(\deg(X)=k! \cdot a_k\).
\end{definition}

\begin{mdremark}
    The geometric definition. Suppose \(X \subset \PP^n\) is a projective variety, then \(\dim(X) = n - (\text{maximal dimension of \(\PP^k\) such that \(\PP^k \cap X = \varnothing\)}) - 1\). Moreover, the degree of \(X\) is given by \(\deg(X)= \#\text{of points intersection of \(X\) with a generic plane \(\PP^{n-\dim(X)}\)}\).
\end{mdremark}

\subsubsection{The Hilbert polynomial of a hypersurface}

\begin{mdlemma}
    Let \(F \in K[X_0,\ldots,X_n] = R\) be an irreducible, homogeneous polynomial of degree \(d\). Let \(X_F\) be the hypersurface in \(\PP^n\) given by the equation \(F=0\). The Hilbert polynomial of \(X_F\) is
    \[p_{X_F}(m) = \binom{m+n}{n}-\binom{m+n-d}{n}.\]
\end{mdlemma}

\section{Higher dimensional Bézout's theorem}

\subsection{Exact sequences and Hilbert functions}

\begin{lemma}[Exact sequences]
    Let \(f:U\to V\) and \(g: V \to W\) be linear maps of \(K\)-vector spaces. Assume that \(f\) is injective, \(g\) is surjective, and that \(\text{Im}(f)=\ker(g)\). That is,
    \[0 \to U \xrightarrow{f} V \xrightarrow{g} W \to 0\]
    is an exact sequence. Then \(\dim(V)=\dim(U)+\dim(W)\).
\end{lemma}

\begin{proof}
    This follows from the rank-nullity theorem. We have 
    \[\begin{aligned}
        \dim(V)&=\dim(\ker(g))+\dim(\text{Im})(g) \\
        &=\dim(\text{Im}(f))+\dim(W) \\
        &= \dim(U)+\dim(W).
    \end{aligned}\]
\end{proof}

\begin{mdprop}
    Let \(I, J \subset K[X_0,\ldots,X_n]\) be two homogeneous ideals. Then \(h_{I \cap J}+h_{I+J}=h_I+h_J\).
\end{mdprop}

\begin{proof}
    Set \(R_i =K[X_0,\ldots,x_i] \). For each \(m\) we have the following exact sequence 
    \[0 \to R_m/(I_m \cap J_m) \to R_m/I_m \times R_m/J_m \to R_m/(I_m+J_m) \to 0.\]
    The first non-trivial map sends \([f] \to ([f],[f])\) and the second map \(([f],[g]) \to [f]-[g]\). Now apply the lemma above.
\end{proof}

\begin{lemma}
    Let \(I \subset K[X_0,\ldots,X_n]\) be a homogeneous ideal and let \(f\in K[X_0,\ldots,X_n]\) such that \(f\not\in I\) be a homogeneous polynomial of degree \(N\). Assume that \(K[X_0,\ldots,X_n]/I\) is an integral domain. Then 
    \[h_{I+(f)}(d)=h_I(d)-h_I(d-N) \quad \text{for } d>N.\]
\end{lemma}

\subsection{Higher dimensional Bézout's theorem}

\begin{mdthm}
    Let \(X \subset \PP^n_K\) be an irreducible projective variety of dimension at least \(1\) and let \(f\in K[X_0,\ldots,X_n]\) be a homogeneous polynomial that does not vanish on \(X\). Then \(\deg(I(X)+(f))=\deg(X)\deg(f)\).
\end{mdthm}

\section{Interesting results for the exam}

\begin{mdthm}
    Let \( F = \sum a_i z_0^i z_1^{d-i} \) be a homogeneous polynomial in two variables, \( a_i \in \mathbb{C} \). We can write \( F = \prod_{k=1}^d (b_k z_0 + c_k z_1) \)
\end{mdthm}

\end{document}
