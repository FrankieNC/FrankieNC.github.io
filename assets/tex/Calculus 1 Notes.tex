\documentclass[12pt, a4paper]{article}   	
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage{color}   
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{multicol}
\usepackage[thinc]{esdiff}
\usepackage{physics}
\usepackage{bm}
\usepackage{pdfpages}
\usepackage{pdflscape}
\usepackage{listings}
\usepackage{float}

\usepackage{hyperref}

\hypersetup{colorlinks=true, linktoc=all, linkcolor=black,}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\imply}{\Rightarrow}
\newcommand{\Cal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}

\DeclareMathOperator{\cosec}{cosec}
\DeclareMathOperator{\cosech}{cosech}
\DeclareMathOperator{\arccosec}{arccosec}
\DeclareMathOperator{\arccosh}{arccosh}
\DeclareMathOperator{\arcsinh}{arcsinh}
\DeclareMathOperator{\arctanh}{arctanh}
\DeclareMathOperator{\arcsech}{arcsech}
\DeclareMathOperator{\arccosech}{arccosech}
\DeclareMathOperator{\arccoth}{arccoth}

\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{example}{Example}
\newtheorem{proposition}{Proposition}

\theoremstyle{plain}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Calculus I Revision Notes}
\date{}
\author{Francesco Chotuck}
\begin{document} 
\maketitle 

\tableofcontents

\pagebreak

\section{Standard Notation}

$$\begin{aligned}
\bb{N}&=\{1,2,3\ldots\} &\text{the set of natural numbers,} \\
\bb{Z}&=\{\ldots,-2,-1,0,1,2,\ldots\} &\text{the set of integers,} \\
\bb{Q}&=\{\f{m}{n}:m\in\bb{Z}, n\in\bb{N}\} &\text{the set of rational numbers,} \\
\bb{R}&= (-\infty,\infty) &\text{the set of real numbers,} \\
\bb{R}^+&= (0,\infty)&\text{the set of positive numbers,} \\
\bb{R}_0^+ &= [0,\infty) &\text{the set of non-negative real numbers.}
\end{aligned}$$

\pagebreak

\section{Functions of one variable}

\subsection{Definition of a function}

\begin{definition}
A \textbf{function} $f:A\to B$ is a map from a set $A$ to a set $B$ that associates a unique element in $B$ to each element in $A.$
\end{definition} 

\textbf{Notation:} The symbol $\to$ indicates a map between sets, $f:A\to B,$ and the symbol $\mapsto$ indicated a map acting on a single element in a set, $x\mapsto y,(x\in A,y\in B).$

\begin{remark}
A function $f:A\to B$ acts on \textbf{all} elements in $A$ to give an element in $B$ but it is \textbf{not necessary} that every element in $B$ is associated with an element in $A$ by a function.
\end{remark}

\subsection{Well-defined functions}

\begin{definition}
A function is said to be \textbf{well-defined} if and only if the function is \textbf{not} a \textbf{many-to-one} function.
\end{definition}

The vertical line test can be used to deduce whether a `function' is well-defined: 

\begin{definition}
Given a function $f,$ every vertical line that may be drawn intersect the graph of $f$ no more than once. If any vertical line intersects a set of points more than once, the set of points does not represent a function. 
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.6\textwidth]{./Resources/Vertical line test.png}
\caption{Illustration of vertical line test.}
\label{fig:Vertical line test}
\end{figure}

\subsection{Domain and Range}

\begin{definition}
For a function $f : A \to B,$ the set $A$ is called the \textbf{domain} of $f$ and the set $B$ is called the \textbf{range} of $f.$
\end{definition}

\textbf{Terminology:} The set of all elements $f(x)$ for all $x \in A$ is called the \textbf{image} of $f.$

\subsection{Injections, Surjections and Bijections}

\begin{definition}
A function $f : A \to B$ is \textbf{injective} (or \textbf{one-to-one}) if every element $f(x) \in B$ is mapped to by at most one element in the domain $A.$ An injective function is called an \textbf{injection}.

More formally expressed as: $$f: A \rightarrow B, \ \forall x_1,x_2 \in A, f(x_1) = f(x_2) \imply x_1 = x_2.$$ 
\end{definition}

\begin{example}
Let $f : \bb{Z} \to \bb{Z}$ be defined by $f(x) = 3x + 7.$ Prove that $f$ is injective. \\
\textbf{Solution:} Suppose $f(x)=f(y)$ for some $x,y \in \bb{Z}.$ So, substituting into $f, 3x+7+3y+7.$ Therefore, $3x=3y \imply x=y.$ Hence, $f$ is injective. 
\end{example}

\begin{theorem}
A function $f$ is injective if and only if every horizontal line intersects the graph of $f$ no more than once.
\end{theorem}

\begin{remark}
Variations of the horizontal line test can be used as such:
\begin{itemize}

	\item The function $f$ is surjective if and only if its graph intersects any horizontal line at \textbf{least} once.

	\item $f$ is bijective if and only if any horizontal line will intersect the graph \textbf{exactly} once.

\end{itemize}
\end{remark}

\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{./Resources/Horizontal line test.png}
\caption{Illustration of the horizontal line test}
\label{fig:horizontal line test}
\end{figure}

\begin{definition}
A function $f : A \to B$ is \textbf{surjective} (or \textbf{onto}) if every element in $B$ is mapped to by at least one element of $A.$ A surjective function is called a \textbf{surjection}. 

In mathematical notation the definition is as follows: $$f:A\to B, \forall y \in B, \exists x \in A \text{ such that } y=f(x).$$
\end{definition}

\begin{example}
Define the function g from the integers to the integers by the formula $g(x) = x - 8.$ Prove that $g$ is surjective. \\
\textbf{Solution:} Need to show that for every integer $y,$ there is an integer $x$ such that $f(x)=y.$ so, choose $x=y+8.$ The chosen value of $x$ is an integer since it is the sum of two integers. Therefore, $g(x)=g(y+8)=(y+8)-8=y.$ The function $g$ is surjective.
\end{example}

\begin{definition}
A function $f : A \to B$ is \textbf{bijective} if it is both surjective and injective. A bijective function is called a \textbf{bijection}.
\end{definition}

\subsection{Inverse functions}

\begin{definition}
The \textbf{inverse function}, denoted $f^{-1},$ of a bijective function $f : A \to B$ is a function defined by $$f^{-1}:B\to A,\quad f^{-1}(f(a))=a \quad \forall a\in A.$$
\end{definition}

\begin{remark}
For a function to have an inverse it must be bijective.
\end{remark}

\begin{remark}
The graph of the inverse function is a reflection of the function on the line $y=x,$ i.e. the graph is given by the original function but with the $x$-axis and $y$-axis interchanged.
\end{remark}

\subsection{Composition of functions}

\begin{definition}
The \textbf{composition} $f\circ g$ of functions $g:A\to B$ and $f:B\to C$ is defined as $$(f\circ g)(x):=f(g(x)), \quad \forall x \in A.$$
\end{definition}

\begin{remark}
Most of the times $f\circ g \neq g\circ f$ 
\end{remark}

\subsubsection{Equal functions}

\begin{definition}
The functions $f$ and $g$ are said to be \textbf{equal} if and only if \begin{itemize}
	\item $f$ and $g$ have the same domain;
	\item $f(x)=g(x)$ for every $x$ in that domain.
\end{itemize}
\end{definition}

\begin{example}
Is $f(x)=\f{x^2}{x}$ the same function as $g(x)=x?$\\
\textbf{Solution:} If the domain is $\bb{R}$ then no, because $f(0)\neq g(0).$ The domain can be changed so that $f=g.$
\end{example}

\subsection{Standard functions}

\subsubsection{Even and Odd functions}

\begin{definition}
\textbf{Even functions} are symmetric about the $y$-axis : $$f(x)=f(-x)\; \text{ for all } x.$$
\end{definition}

\begin{definition}
\textbf{Odd functions} are symmetric about the origin: $$-f(x)=f(-x) \; \text{ for all } x.$$
\end{definition}

\begin{remark}
Common even functions: cosine, hyperbolic cosine and absolute value. \\
Common odd functions: sine, hyperbolic sine, identity function.
\end{remark}

\subsubsection{Logarithm}

\begin{definition}
The \textbf{logarithmic function}, or logarithm, to the base $a \in \bb{R}^+ \backslash \{1\},$ written as $\log(x),$ is the inverse of the function $f :\bb{R}\to \bb{R}^+, f(x)=a^x.$
\end{definition}

\begin{theorem}
\textbf{Properties of the Logarithm:}
\begin{enumerate}
	
	\item[(i)] Change of basis, $\log_a(x)=\f{\log_b(x)}{\log_b(a)};$
	\item[(ii)] Additive property, $\log_a(xy)=\log_a(x)+\log_a(y);$
	\item[(iii)] Subtractive property, $\log_a(\f{x}{y})=\log_a(x)-\log_a(y);$
	\item[(iv)] $\log_a(x^y)=y\log_a(x).$

\end{enumerate}
\end{theorem}

\subsection{Trigonometric functions}

\subsubsection{Radians}

\begin{definition}
One \textbf{radian} is the angle subtended at the centre of a circle by an arc that is equal in length to the radius of the circle.
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=0.3\textwidth]{./Resources/Radian.png}
\caption{The radian measure of an angle $\theta.$}
\label{fig:radian}
\end{figure}

\subsubsection{Definition of trigonometric functions}

\begin{definition}
The functions \textbf{cosine} and $\textbf{sine}$ are defined by the point $(x,y)$ is at an angle $\theta$ to the $x$-axis are denoted as follows: $(x,y)=(\cos(\theta),\sin(\theta)).$
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/Graphs of trig functions.png}
\caption{Graphs of trigonometric functions}
\label{fig:trig functions}
\end{figure}

\begin{remark}
The graph of the functions $\secant(x)$ and $\cosecant(x)$ sit on the peaks of $\cos(x)$ and $\sin(x)$ respectively.
\end{remark}

\subsubsection{Trigonometric Identities}

\subsubsection*{Even and Odd}

\begin{enumerate}
	
	\item Cosine is an even function: $$\cos(\theta)=\cos(-\theta);$$

	\item Sine is an odd function: $$\sin(-\theta)=-\sin(-\theta).$$

\end{enumerate}

\subsubsection*{Reciprocal identities}

\begin{enumerate}
	
	\item $$\tan(\theta)=\f{\sin(\theta)}{\cos(\theta)};$$

	\item $$\cot(\theta)=\f{\cos{(\theta)}}{\sin(\theta)};$$

	\item $$\cosecant(\theta)=\f{1}{\sin(\theta)};$$

	\item $$\secant(\theta) =\f{1}{\cos(\theta)}.$$

\end{enumerate}

\subsubsection*{Pythagorean identities}

\begin{enumerate}
	
	\item $$\sin^2(\theta)+\cos^2(\theta)=1;$$

	\item $$1+\tan^2(\theta)=\secant^2(\theta);$$

	\item $$1+\cotangent^2(\theta)=\cosecant^2(\theta).$$

\end{enumerate}

\subsubsection*{Addition and subtraction}

\begin{enumerate}
	
	\item $$\sin(\alpha\pm \beta)=\sin(\alpha)\cos(\beta)\pm\cos(\alpha)\sin(\beta);$$

	\item $$\cos(\alpha\pm \beta)=\cos(\alpha)\cos(\beta)\mp \sin(\alpha)\sin(\beta).$$

	\item $$\tan(\alpha\pm \beta)=\f{\tan(\alpha)\pm\tan(\beta)}{1\mp\tan(\alpha)\tan(y)}.$$

\end{enumerate}

\subsubsection*{Double angle}

\begin{enumerate}
	
	\item $$\sin(2\theta)=2\sin(\theta)\cos(\theta);$$

	\item $$\begin{aligned}
	\cos(2\theta)&=\cos^2(\theta)-\sin^2(\theta) \\
			&= 2\cos^2(\theta)-1 \\
			&= 1-2\sin^2(\theta).
	\end{aligned}$$

\end{enumerate}

\subsubsection{Inverse trigonometric functions}

\begin{definition}
The trigonometric functions are not bijective so, they are not invertible on their full domains. By restricting the domain the inverse trigonometric functions are defined as follows: \begin{itemize}
	\item $$\arcsin(x):[-1,1]\to\left[-\f{\pi}{2},\f{\pi}{2}\right];$$
	\item $$\arccos(x):[-1,1]\to\left[0,\pi\right];$$
	\item $$\arctan(x):\bb{R} \to \left(-\f{\pi}{2},\f{\pi}{2}\right).$$
\end{itemize}
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/Inverse trig graphs.png}
\caption{The graph of each of the inverse trigonometric functions}
\label{fig:inverse trig graphs}
\end{figure}

\subsubsection{Trigonometric functions with Complex variables}

\begin{definition}
\textbf{Euler's formula} is $$e^{i\theta}=\cos(\theta)+i\sin(\theta)$$ where $\theta\in\bb{R}.$
\end{definition}

From Euler's formula it is possible to derive the following identities: \begin{itemize}

	\item $$\sin(x)=\f{1}{2i}\left(e^{ix}-e^{-ix}\right);$$
	\item $$\cos(x)=\f{1}{2}\left(e^{ix}+e^{-ix}\right).$$

\end{itemize}	

\subsection{Hyperbolic functions}

\begin{definition}
The hyperbolic sine and cosine are defined as follows: $$\begin{aligned}
\sinh(x):\bb{R}\to\bb{R} \quad \sinh(x)&=\f{e^{x}-e^{-x}}{2}\\
\cosh(x):\bb{R}\to [1,\infty) \quad \cosh(x)&=\f{e^{x}+e^{-x}}{2}.
\end{aligned}$$
\end{definition}

\begin{definition}
The hyperbolic tangent is defined as: $$\tanh(x)=\f{\sinh(x)}{\cosh(x)}=\f{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$ where $\tanh: \bb{R}\to(-1,1).$
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/Graphs of hyp functions.png}
\caption{Graphs of hyperbolic functions}
\label{fig:hyperbolic graphs}
\end{figure}

\subsubsection{Inverse hyperbolic functions}

\begin{definition}
The inverse hyperbolic sine, cosine and tangent are defined as follows:
$$\begin{aligned}
\arcsinh(x)&=\ln(x+\sqrt{1+x^2}) \\
\arccosh(x)&=\ln(x+\sqrt{x^2-1}) \\
\arctanh(x)&=\f{1}{2}\ln(\f{1+x}{1-x}).
\end{aligned}$$
\end{definition}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/Inverse hyp graphs.png}
\caption{Graphs of the inverse hyperbolic functions}
\label{fig:inverse hyperbolic}
\end{figure}

\begin{example}
Find the inverse of the hyperbolic sine function. \\
\textbf{Solution:} Begin by writing $y=\f{1}{2}(e^{x}-e^{-x})$ and rearrange the expression to find $x$ as a function of $y.$ $$\begin{aligned}
e^x-e^{-x}-2y&=0 \\
e^{2x}-1-2ye^{x}&=0 \\
(e^x-y)^2-y^2-1&=0 \quad \text{(by completing the square)} \\
\imply e^x-y&=\sqrt{1+y^2} \\
x&=\ln(y+\sqrt{1+y^2}).
\end{aligned}$$
\end{example}

\subsubsection{Hyperbolic identities}

\begin{enumerate}
	
	\item $$\cosh^2(x)-\sinh^2(x)=1$$

	\item $$1-\tanh^2(x)=\sech^2(x)$$

	\item $$\coth^2(x)-1=\cosech^2(x)$$

\end{enumerate}

\subsubsection*{Addition and subtraction}

\begin{enumerate}
	
	\item $$\sinh(x\pm y)=\sinh(x)\cosh(y)\pm\cosh(x)\sinh(y)$$

	\item $$\cosh(x\pm y)=\cosh(x)\cosh(y)\pm\sinh(x)\sinh(y)$$

	\item $$\tanh(x\pm y)=\f{\tanh(x)\pm\tanh(y)}{1\pm\tanh(x)\tanh(y)}$$

\end{enumerate}

\subsubsection*{Double `angle'}

\begin{enumerate}
	
	\item $$\sinh(2x)=2\sinh(x)\cosh(x)$$

	\item $$\begin{aligned}
	\cosh(2x)&=\cosh^2(x)+\sinh^2(x) \\
			&=2\cosh^2(x)-1 \\
			&=2\cosh^2(x)-1
	\end{aligned}$$

\end{enumerate}

\subsubsection{Hyperbolic functions with Complex variables}

Using Euler's formula the following identities can be derived: 

\begin{enumerate}

	\item $$\sin{x} = -i\sinh(ix)$$

	\item $$\cos{x} = \cosh(ix)$$

	\item $$\tan{x} = -i\tanh(ix)$$

	\item $$\sinh{x} = -i\sin(ix)$$

	\item $$\cosh{x} = \cos(ix)$$

	\item$$\tanh{x} = -i\tan(ix)$$

\end{enumerate}

\section{Limits}

\begin{definition}
The \textbf{right limit}: $$\lim_{x \to a^+} = L \iff \forall \varepsilon>0 \ \exists \delta>0 \text{ such that } 0<x-a<\delta \Rightarrow|f(x)-L|<\varepsilon.$$
\end{definition}

\begin{definition}
The \textbf{left limit}: $$\lim_{x \to a^-} = L \iff \forall \varepsilon>0 \ \exists \delta>0 \text{ such that } 0<a-x<\delta \Rightarrow|f(x)-L|<\varepsilon.$$
\end{definition}

\begin{definition}
The \textbf{limit}: $$\lim_{x \to a} f(x) = L \iff  \forall \varepsilon>0 \ \exists \delta>0 \text{ such that } 0<|x-a|<\delta \Rightarrow |f(x)-L|<\varepsilon.$$ 
\end{definition}

\begin{remark}
The limit of a function exists if and only if both the left and right limit exist and they are equal.
\end{remark}

\begin{example}
Prove $\lim_{x\to 1} (3x-1)=2$ with the definition of the limit. \\
\textbf{Solution:} Proceed by choosing a suitable $\delta$ for which the definition of the limit for $f(x)=3x-1$ is satisfied. \\
\textit{Rough work:} $$\begin{aligned}
|f(x)-L|<\varepsilon &\imply |(3x-1)-2| < \varepsilon \\
					&\imply |3x-3|<\varepsilon \\
					&\imply 3|x-1|<\varepsilon \\
					&\imply |x-1|< \f{\varepsilon}{3}.
\end{aligned}$$ In this case, choose $\delta=\f{\varepsilon}{3}$ then, $$\begin{aligned}
|x-1|<\delta 	&\imply |x-1|< \f{\varepsilon}{3} \\
				&\imply 3|x-1|<\varepsilon \\
				&\imply |3x-3|<\varepsilon \\
				&\imply |(3x-1)-2|<\varepsilon.
\end{aligned}$$ The proof is complete.
\end{example}

\subsection{Limits involving infinity}

\begin{definition}
$$\lim_{x \to \infty} f(x) = L \iff \forall \varepsilon>0, \ \forall x>X, \ \exists X>0 \text{ such that } |f(x)-L|<\varepsilon.$$
\end{definition}

\begin{definition}
$$\lim_{x \to -\infty} f(x) = L \iff \forall \varepsilon>0, \ \forall x<X, \ \exists X<0 \text{ such that } |f(x)-L|<\varepsilon.$$
\end{definition}

\subsection{Algebra of limits}

\begin{theorem}
For any real number $a$ and any constant $c,$ \begin{enumerate}
	
	\item $$\lim_{x\to a}x=a;$$
	\item $$\lim_{x\to a}c=c.$$

\end{enumerate}
\end{theorem}

\begin{theorem}
Let $f(x)$ and $g(x)$ be defined for all $x \neq a$ over some open interval containing $a.$ Assume that $L$ and $M$ are real numbers such that $lim_{x\to a} f(x) = L$ and $lim_{x\to a} g(x) = M.$ Let $c$ be a constant. Then, each of the following statements holds: \begin{enumerate}
	
	\item Sum and Difference law: $$\lim_{x\to a}(f(x)\pm g(x))=\lim_{x\to a} f(x)\pm \lim_{x\to a}g(x)=L \pm M;$$

	\item Constant multiple law: $$\lim_{x\to a} cf(x)=c \cdot \lim_{x\to a} f(x)=cL;$$

	\item Product law: $$\lim_{x\to a} (f(x)g(x))=\left(\lim_{x\to a} f(x)\right)\left(\lim_{x\to a} g(x)\right)=LM;$$

	\item Quotient law: $$\lim_{x\to a}\left(\f{f(x)}{g(x)}\right)\f{\lim_{x\to a}f(x)}{\lim_{x\to a}g(x)}=\f{L}{M}$$ for $M\neq 0;$

	\item Power law: $$\lim_{x\to a} (f(x))^n=\left(\lim_{x\to a}f(x)\right)^n=L^n$$ for every positive integer $n;$

	\item Root law: $$\lim_{x\to a}\sqrt[n]{f(x)}=\sqrt[n]{\lim_{x\to a} f(x)}=\sqrt[n]{L}$$ for all $L$ if $n$ is odd and for $L\geq 0$ if $n$ is even.

\end{enumerate}
\end{theorem}

\subsubsection{Limits of composite functions}

\begin{theorem}
If $f(x)$ is continuous at $L$ and $\lim_{x\to a} g(x) = L,$ then $$\lim_{x\to a}f(g(x))=f\left(\lim_{x\to a} g(x)\right)=f(L).$$
\end{theorem}

\subsection{Multiple limits}

\begin{theorem}
For functions of multiple variables the limits are defined as: $$\lim_{x\to a}\lim_{y\to b}f(x,y):=\lim_{x\to a}\left(\lim_{y\to b}f(x,y)\right).$$
\end{theorem}

\begin{remark}
In general $$\lim_{x\to a}\lim_{y\to b}f(x,y) \neq \lim_{y\to b}\lim_{x\to a}f(x,y).$$
\end{remark}

\begin{example}
Show that changing the order the limits are taken in the following double limit changes its value: $$\lim_{x\to \infty}\lim_{y\to-\infty}(1+\tanh(x+y)).$$ \\
\textbf{Solution:} $$\begin{aligned}
\lim_{x\to \infty}\left(\lim_{y\to-\infty}(1+\tanh(x+y))\right) &= \lim_{x\to \infty} (1-1)=0 \quad \text{whereas,} \\
\lim_{y\to -\infty}\left(\lim_{x\to\infty}(1+\tanh(x+y))\right) &= \lim_{y\to -\infty} (1+1)=2.
\end{aligned}$$
\end{example}

\subsection{Continuous functions}

\begin{definition}
A function $f(x)$ is \textbf{continuous at the point} $\mathit{x=a}$ if $\lim_{x \to a} f(x)$ exists and is equal to $f(a).$
\end{definition}

\begin{definition}
A function $f(x)$ is \textbf{continuous} if it is continuous at all points in its domain. 
\end{definition}

\subsection{Intermediate Value Theorem}

\begin{theorem}
Let $f(x)$ be a continuous function on $[a,b]$ and suppose $f(a) < y < f(b)$ then there exists an $x = c$ with $c \in (a,b)$ such that $f(c) = y.$
\end{theorem}

\begin{example}
Show that there is a root of the equation $4x^3-6x^2+3x-2=0$ in the interval $[1,2].$ \\
\textbf{Solution:} Consider the function $f(x)=4x^3-6x^2+3x-2$over the closed interval $[1,2].$ The function $f$ is a polynomial, therefore it is continuous over $[1,2].$ It follows that $f(1) =-1$ and $f(2)=12.$ Since, $f(1)<0<f(2)$ by the Intermediate Value Theorem there exists a value $c$ in the interval $(1,2)$ such that $f(c)=0,$ i.e. there is a solution for the equation $f(x)=0$ in the interval $(1,2).$
\end{example}

\subsection{Sandwich Theorem}

\begin{theorem}
Suppose $$f(x)\leq g(x) \leq h(x)$$ for all $x$ such that $0<|x-a|<\delta$ with some $\delta>0,$ and $$\lim_{x\to a} f(x)=\lim_{x\to a} h(x)=L,$$ where $L\in \bb{R}.$ Then $$\lim_{x\to a} g(x)=L.$$
\end{theorem}

\begin{example}
Find $$\lim_{x\to 0} x^2\cos\left(\f{1}{x^2}\right).$$ 
\textbf{Solution:} When trying to find functions to use to `sandwich', the functions must be similar enough to $g(x)$ and easier to evaluate their limit as $x\to a.$ To do this, start with the most complicated part of $g(x),$ to find constants or simpler functions that it stays between. \\
In this case the cosine function is bounded by $-1$ and $1,$ so $$\begin{aligned}
-1 \leq &\cos\left(\f{1}{x^2}\right) \leq 1 \\
-x^2\leq x^2&\cos\left(\f{1}{x^2}\right) \leq x^2.
\end{aligned}$$ Since $$\lim_{x\to 0} -x^2=\lim_{x\to 0} x^2=0$$ by the Sandwich theorem, $$\lim_{x\to0}x^2\cos\left(\f{1}{x^2}\right)$$
\end{example}
	
\subsection{Limits to learn}

\subsubsection{A common trick}

A common trick to evaluate limits: $$\lim_{x\to a}f(x)^{g(x)}=\lim_{x\to a} e^{\ln{f(x)^{g(x)}}}\lim_{x\to a}e^{g(x)\ln{f(x)}}=e^{\lim_{x\to a}\left(g(x)\ln{f(x)}\right)}.$$

\subsubsection{Standard limits}

\begin{enumerate}

	\item $$\lim_{x \to 0} \frac{\sin(x)}{x} = 1$$

	\item $$\lim_{x \to 0} \frac{e^x-1}{x} = 1$$

	\item $$\lim_{x \to 0} \frac{\cos(x)-1}{x} = 0$$

\end{enumerate}

\subsubsection{Common Limits}

\begin{enumerate}

	\item $$\lim_{x \to 0} \frac{\tan(x)}{x} = 1$$

	\item $$\lim_{x \to 0} \frac{\ln(1+x)}{x} = 0$$

	\item $$\lim_{x \to \infty} \left(1 + \frac{1}{x} \right)^x = e$$

\end{enumerate}

\section{Differentiation}

\subsection{Differentiation from First Principles}

\begin{definition}
The \textbf{derivative} of a function $f(x)$ with respect to $x$ is $$\diff{f}{x} \equiv \lim_{h\to0} \f{f(x+h)-f(x)}{h}.$$
\end{definition}

\subsection{Differentiable functions}

\begin{definition}
A function $f(x)$ is \textbf{differentiable} at the point $x = a$ if the derivative $\diff{f}{x}$ exists at $x = a.$ A function for which the derivative exists for all points in its domain is called a \textbf{differentiable function}.
\end{definition}

\begin{theorem}
If a function $f(a,b)\to \bb{R}$ is differentiable on the interval $(a,b),$ then it is continuous on $(a,b).$
\end{theorem}

\subsection{Properties of derivatives}

\begin{theorem}
Let $f$ and $g$ be differentiable functions. \begin{itemize}
	\item The product rule: $(fg)'=gf'+fg';$
	\item the quotient rule: $\left(\f{f}{g}\right)=\left(\f{gf'-fg'}{g^2}\right)$ if $g\neq 0.$
\end{itemize}
\end{theorem}

\subsection{Derivatives inverse functions}

\begin{theorem}
Let $f(x)$ be a function that is both invertible and differentiable. Let $y=f^{-1}(x)$ be the inverse of $f(x).$ For all $x$ satisfying $f'\left(f^{-1}(x)\right)\neq 0,$ $$\diff{y}{x}=\diff{}{x}\left(f^{-1}(x)\right)=\f{1}{f'\left(f^{-1}(x)\right)}.$$ Alternatively, if $y=g'(x)$ is the inverse of $f(x),$ then $$g(x)=\f{1}{f'(g(x))}.$$
\end{theorem}

\begin{example}
Find the derivative of $g(x)=\arcsin(x).$ \\
\textbf{Solution:} Since for $x\in\left[-\f{\pi}{2},\f{\pi}{2}\right], f(x)=\sin(x)$ is the inverse of $g(x)=\arcsin(x),$ begin by finding $f'(x).$ $$\begin{aligned}
f(x)&=\sin(x) \\
f'(x)&=\cos(x) \\
f'(g(x))&=\cos(\arcsin(x))=\sqrt{1-x^2}.
\end{aligned}$$ Therefore, $$g'(x)=\diff{}{x}(\arcsin(x))=\f{1}{f'(g(x))}=\f{1}{\sqrt{1-x^2}}.$$
\end{example}

\subsection{Mean Value Theorem}

\begin{theorem}
Let $f$ be a continuous function on $[a,b]$ and a differentiable function on $(a, b),$ then there exists a point $c \in (a, b)$ where $$f'(c)=\f{f(b)-f(a)}{b-a}.$$
\end{theorem}

\begin{corollary}
Let $f$ be differentiable over an interval $I.$ If $f'(x)=0$ for all $x\in I,$ then $f(x)$ is constant for all $x\in I.$
\end{corollary}

\subsubsection{Rolle's Theorem}

\begin{theorem}
Let $f$ be a continuous function over the closed interval $[a, b]$ and differentiable over the open interval $(a, b)$ such that $f(a) = f(b).$There then exists at least one $c \in (a, b)$ such that $f'(c) = 0.$
\end{theorem}

\begin{remark}
This is a special case of the mean value theorem.
\end{remark}

\begin{example}
Show that $f(x)=x^3-7x^2+25x+8$ has exactly one real root. \\
\textbf{Solution:} Note that $f(0)=8$ and $f(-1)=-25$ so, $f(-1)<0<f(0).$ Therefore, by the Intermediate Value Theorem there exists a $c\in(-1,0)$ such that $f(c)=0.$ This proves that the function has at least one real root. Assume there is more than one real root hence, there exists $a$ and $b,$ such that $$f(a)=f(b)=0.$$ Since, $f(x)$ s a polynomial it is continuous and differentiable everywhere by Rolle's theorem there exists a $c$ such that $f'(c)=0.$ If such $c$ exists then $$\begin{aligned}
f'(x)&=3x^2-14x+25 \\
\imply f'(c)&=3c^2-14c+25=0 \\
\imply c&=\f{7\pm \sqrt{26}i}{3}.
\end{aligned}$$ No such $c \in \bb{R}$ exists so, $f(x)$ has exactly one real root.
\end{example}
	
\subsection{Standard derivatives}

\subsubsection{Trigonometric derivatives}

\begin{enumerate}
	
	\item $$\diff{(\arcsin{x})}{x} = \frac{1}{\sqrt{1-x^2}}$$

	\item $$\diff{(\arccos{x})}{x} = -\frac{1}{\sqrt{1-x^2}}$$

	\item $$\diff{(\arctan{x})}{x} = \frac{1}{\sqrt{1+x^2}}$$

	\item $$\diff{(\sec{x})}{x} = \sec{x}\tan{x}$$

	\item $$\diff{(\cot{x})}{x} = -\cosec^2{x}$$

	\item $$\diff{(\cosec{x})}{x} = -\cosec{x}\cot{x}$$

\end{enumerate}

\subsubsection{Hyperbolic derivatives}

\begin{enumerate}

	\item $$\diff{(\tanh(x))}{x}=\sech^2(x)$$

	\item $$\diff{(\coth(x))}{x}=-\cosech^2(x)$$

	\item $$\diff{(\sech(x))}{x}=-\sech(x)\tanh(x)$$

	\item $$\diff{(\cosech(x))}{x}=-\cosech(x)\coth(x)$$
	
	\item $$\diff{(\arcsinh{x})}{x} = \frac{1}{\sqrt{1+x^2}}$$

	\item $$\diff{(\arccosh{x})}{x} = \frac{1}{\sqrt{x^2-1}}$$

	\item $$\diff{(\arctanh{x})}{x} = \frac{1}{1-x^2}$$

\end{enumerate}

\section{Integration}

\begin{definition}
The \textbf{integral} $\int_a^b f(x)\,dx$ is the total \textbf{signed} area between the graph $y=f(x)$ and the $x$-axis in the $(x,y)$ plane between $x=a$ and $x=b,$ counted positively for $f(x)>0$ and negatively for $f(x)<0.$
\end{definition}

\begin{theorem}
If $f:[a,b] \to \bb{R}$ is continuous, then $\int_a^b f(x) \, dx$ exists.
\end{theorem}

\subsection{Riemann Integral}

\subsection{Fundamental theorem of Calculus}

\begin{theorem}
\textbf{FTC I} Let $f:[a,b]\rightarrow \mathbb{R}$ be a continuous function and let $F:[a,b] \rightarrow \mathbb{R}$ be defined by $$F(x) = \int_{a}^{x} f(t) \, dt.$$ Then, F is a continuous function on $[a,b],$ differentiable on the interval $(a,b)$ and $$\frac{dF}{dx} = f(x).$$
\end{theorem}

\begin{remark}
The function $F(x)$ is called the \textbf{antiderivative} or the \textbf{primitive} of $f(x).$
\end{remark}

\begin{example}
Let $$F(x)=\int_0^{x^3} \sin(e^t)\, dt.$$ Find $F'(x).$ \\
\textbf{Solution:} Let $$H(x) = \int_0^x \sin(e^t) \, dt$$ then, by FTC I $$H'(x)=\sin(e^x).$$ Note that $$\begin{aligned}
F(x)&=H(x^3) \\
\imply F'(x)&=\diff{}{x}(H(x^3)) \\
 		&= 3x^2 H'(x^3) \quad \text{(by the chain rule)}\\
 		&= 3x^2 \sin(e^{x^3}).
\end{aligned}$$ 

\end{example}

\begin{example}
Let $$F(x)=\int_0^{\cosh(x)} e^{\sqrt{t}+x^2} \, dt.$$ Write $F'(x)$ in terms of $F(x).$ \\
\textbf{Solution:} Notice that $$\begin{aligned}
F(x)&= \int_0^{\cosh(x)} e^{\sqrt{t}+x^2} \, dt \\
	&= e^{x^2}\int_0^{\cosh(x)} e^{\sqrt{t}} \, dt.
\end{aligned}$$ Let $$H(x)=\int_0^x e^{\sqrt{t} \, dt.}$$ Now $$\begin{aligned}
F(x)&=e^{x^2}H(\cosh(x)) \\
F'(x)&=2xe^{x^2}H(\cosh(x))+e^{x^2}H'(\cosh(x))\sinh(x) \\
	&= 2xF(x)+e^{x^2}e^{\sqrt{x}}\sinh(x).
\end{aligned}$$ Therefore, $$F'(x)=2xF(x)+e^{x^2+\sqrt{x}}\sinh(x).$$
\end{example}

\begin{theorem}
Let $f:[a,b]\rightarrow \mathbb{R}$ be a continuous function and let $F:[a,b] \rightarrow \mathbb{R}$ be defined by $$\frac{dF}{dx}=f(x) \quad \forall x \in [a,b].$$ Then, if $f(x)$ is integrable on $[a,b],$ $$\int_{a}^{b} f(x) \, dx= F(b)-F(a).$$
\end{theorem}

\subsection{Integration techniques}

\subsubsection{t-substitution}

For complicated integrals involving trigonometric functions we can use the following substitution. \\ 
Let $t=\tan{\frac{x}{2}},$ then:
$$\begin{aligned}
\sin{x} = \frac{2t}{1+t^2}; \quad \cos{x} = \frac{1-t^2}{1+t^2}; \quad \frac{dx}{dt} = \frac{2}{1+t^2}
\end{aligned}$$

\subsubsection{Integration by parts}

\begin{theorem}
Let $u = f (x)$ and $v = g(x)$ be functions with continuous derivatives. Then, the integration-by-parts formula for the integral involving these two functions is: $$\int u\diff{v}{x}\,dx = uv -\int v\diff{u}{x}dx$$
\end{theorem}

\begin{remark}
The acronym \textbf{LIATE} can be used to choose which function to differentiate. (The I stands inverse trig/hyp functions).
\end{remark}

\subsubsection{Partial fractions}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/Partial fractions table.png}
\caption{Table of partial fraction decomposition.}
\label{fig:Partial fractions}
\end{figure}

\subsection{Volume of revolution}

\begin{figure}[H]
\centering
\includegraphics[width=0.67\textwidth]{./Resources/Revolution graph.png}
\caption{Illustration of a graph's revolution}
\label{fig:revolution}
\end{figure}

\begin{theorem}
Let $f(x)$ be continuous and non negative. Define $R$ as the region bounded above by the graph of $f(x),$ below by the $x$-axis, on the left by the line $x = a,$ and on the right by the line $x = b.$ Then, the volume of the solid of revolution formed by revolving $R$ around the $x$-axis is given by $$V=\int_a^b \pi \left[f(x)\right]^2.$$
\end{theorem}

\begin{remark}
For the volume of revolution around the $y$-axis, $V$ is evaluated by $$V=\int_c^d \pi\left[g(y)\right]^2 \, dy.$$
\end{remark}

\subsection{Length of a curve}

\subsubsection{Cartesian coordinates}

\begin{theorem}
Let $f(x)$ be a smooth function over the interval $[a, b].$ Then the \textbf{arc length} of the portion of the graph of $f(x)$ from the point $(a, f (a))$ to the point $(b, f (b))$ is given by $$s=\int_a^b \sqrt{1+\left(\diff{y}{x}\right)^2} \, dx.$$
\end{theorem}

\subsubsection{Parametric coordinates}

\begin{theorem}
$$s=\int_a^b \sqrt{\left(\diff{x}{t}\right)^2+\left(\diff{y}{t}\right)^2} \, dt.$$
\end{theorem}

\subsubsection{Polar coordinates}

\begin{theorem}
$$s=\int_a^b \sqrt{r^2+\left(\diff{r}{\theta}\right)^2} \, d\theta.$$
\end{theorem}

\section{Power Series}

\subsection{Infinite sums}

\begin{definition}
An \textbf{infinite series} is an expression of the form $\sum_{n=n_0}^{\infty} a_n$ where $n, n_0 \in \bb{Z}.$ For a real series $a_n \in \bb{R}.$
\end{definition}

\begin{definition}
A \textbf{partial sum} of an infinite series $S \equiv \sum_{n=n_0}^{\infty} a_n$ has the form $S_N \equiv \sum_{n=n_0}^{N} a_n.$
\end{definition}

\subsection{Convergence}

\begin{definition}
A series $S \equiv \sum_{n=n_0}^{\infty} a_n$ is called \textbf{convergent} if the limit $S\equiv \lim_{N\to \infty} (S_N)$ exists, where$ S_N \equiv \sum_{n=n_0}^{N} a_n.$ If a series is not convergent it is called \textbf{divergent}.
\end{definition}

\begin{lemma}
If an infinite series $S\equiv \sum_{n=n_0}^{\infty} a_n$ is convergent then, $$\lim_{n\to \infty}(a_n)=0.$$

\begin{proof}
Consider, $$\lim_{n\to \infty} (S_n-S_{n-1})=\lim_{n\to \infty}(S_n)-\lim_{n\to \infty}(S_{n-1})=S-S=0.$$
\end{proof}
\end{lemma}

\begin{definition}
A series $\sum_{n=n_0}^{\infty} a_n$ is \textbf{absolutely convergent} if the series $\sum_{n=n_0}^{\infty} |a_n|$ converges.
\end{definition}

\begin{theorem}
Every absolutely convergent series is convergent, i.e. if $\sum_{n=n_0}^{\infty} |a_n|$ converges then $\sum_{n=n_0}^{\infty} a_n$ is also convergent.
\end{theorem}

\begin{theorem}
Let $\sum_{n=n_0}^{\infty} b_n$ be a convergent series of non-negative numbers. Suppose there exists $M>)$ and $N\in \bb{Z}$ such that $$|a_n|\leq Mb_n, \quad \forall n\geq N.$$ Then $\sum_{n=n_0}^{\infty} a_n$ is absolutely convergent. \textit{(Theorem 13.4 of Sequences and series lecture notes).}
\end{theorem}

\begin{theorem}
Suppose $b_n\geq 0$ for all $n\geq n_0$ and there exists $C>0$ such that $$\sum_{n=n_0}^{N} b_n \leq C \quad \forall N\geq n_0.$$ Then the series $\sum_{n=n_0}^{N} b_n$ converges and its sum is $\leq C.$
\end{theorem}

\subsection{Series convergence criteria}

\subsubsection{Limit comparison test}

\begin{theorem}
Let $\sum_{n=1}^{\infty} a_n$ and $\sum_{n=1}^{\infty} b_n$ be positive-term series $(a_n>0 \text{ and } b_n>0)$ such that $$\lim_{n\to \infty} \frac{a_n}{b_n} = L \neq 0.$$ Then $\sum_{n=1}^{\infty} a_n$ converges if and only if $\sum_{n=1}^{\infty} b_n$ converges.
\end{theorem}

\subsubsection{The n'th Root Test or Cauchy’s Criterion}

\begin{theorem}
Let $\sum_{n=1}^{\infty} a_n$ be a positive-term series. Then

$$\begin{aligned}
\lim_{n\to \infty} |a_n|^{\frac{1}{n}}
\begin{cases} 
      <1 \sum_{n=1}^{\infty} a_n \;&\text{is convergent,} \\
      =1 						\;&\text{test is inconclusive,}  \\
      >1 \sum_{n=1}^{\infty} a_n \;&\text{is divergent.}
\end{cases}
\end{aligned}$$

\end{theorem}

\subsubsection{The Ratio Test or D'Alembert's Criterion}

\begin{theorem}
Let $\sum_{n=1}^{\infty} a_n$ be a series of positive-terms and for each $n \in \mathbb{N}$ let $\alpha_n=\frac{a_{n+1}}{a_n}.$
$$\begin{aligned}
\lim_{n\to \infty} |\alpha_n|
\begin{cases} 
      <1 \sum_{n=1}^{\infty} a_n &\text{is convergent,} \\
      =1 							&\text{test is inconclusive,}  \\
      >1 \sum_{n=1}^{\infty} a_n &\text{is divergent.}
\end{cases}
\end{aligned}$$
\end{theorem}


\subsection{Series as a function of \texorpdfstring{$x$}{TEXT}}

\begin{definition}
A function which can be locally written as a convergent power series is called an \textbf{analytic function}.
\end{definition}

\begin{definition}
A \textbf{power series} is an expression of the form $S(x)\equiv \sum_{n=n_0}^{\infty} b_n x^n.$
\end{definition}

\begin{theorem}
For every power series $\sum_{n=n_0}^{\infty} b_nx_n$ there exists $R \geq 0$ or $R = +\infty$ such that the series is absolutely convergent for all $x$ with $|x| < R$ and is divergent for all $x$ with $|x| > R.$
\end{theorem}

\begin{definition}
The $R$ from the previous theorem is known as \textbf{radius of convergence} of a power series $\sum_{n} b_nx^n$ where $$R=\lim_{n \to \infty} |b_n|^{-\frac{1}{n}}$$ and, where it exists, $$R=\lim_{n \to \infty} \left|\frac{b_n}{b_{n+1}}\right|.$$
\end{definition}

\begin{remark}
What happens when $$R = \begin{aligned}
\begin{cases} 
      |x| \quad&\text{ inconclusive} \\
      0 \quad&\text{ the series diverges for all } x \neq 0  \\
      \infty \quad&\text{ the series converges for all } x \in \mathbb{R}.
\end{cases}
\end{aligned}$$
\end{remark}

\begin{example}
Find the radius of convergence for $$S(x)=\sum_{n=0}^{\infty} \f{x^n}{n}.$$ \\
\textbf{Solution:} Here $b_n=\f{1}{n}$ so, $$\begin{aligned}
R 	&=\lim_{n\to \infty} \left|\f{b_n}{b_{n+1}}\right| \\
	&=\lim_{n\to \infty} \left|\f{\f{1}{n}}{\f{1}{n+1}}\right| \\
	&= \lim_{n\to \infty}\f{n+1}{n} \\
	&= \lim_{n\to \infty}\left(1+\f{1}{n}\right) \\
	&=1.
\end{aligned}$$ Hence, the series converges absolutely for $|x|<1$ and diverges for $|x|>1.$ At $x=1$ the series $S(x)=\sum_{n=0}^{\infty} \f{1}{n},$ so the series is divergent.
\end{example}

\subsection{Taylor's theorem}

\begin{definition}
If $f$ has derivatives up to order $N$ at $x=a,$ then the \textbf{Taylor series} to order $N$ for the function $f$ at $a$ is $$f(x)=\sum_{n=0}^N \f{f^{(n)}(a)}{n!}(x-a)^n+R_{N+1}(x) \quad \text{where } R_{N+1}=\f{1}{N!}\int_a^{x}f^{N+1}(y)(x-y)^N \, dy.$$ 
\end{definition}

\subsubsection{Maclaurin series}

\begin{definition}
The \textbf{Maclaurin series} for a function $f(x)$ is a Taylor series expansion about $x=0$ and to order $N$ is written $$f(x)=\sum_{n=0}^N \f{f^{(n)}(0)}{n!}x^n+R_{N+1}(x) \quad \text{where } R_{N+1}=\f{1}{N!}\int_a^{x}f^{N+1}(y)(x-y)^N \, dy.$$
\end{definition}

\subsubsection{Common Maclaurin's series}

\begin{enumerate}

	\item 
	$$\begin{aligned}
	e^x= \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\ldots & \quad x \in \mathbb{R}
	\end{aligned}$$

	\item
	$$\begin{aligned}
	\sin(x)= \sum_{n=0}^{\infty} (-1)^n\frac{x^{2n+1}}{(2n+1!)} = x-\frac{x^3}{3!}+\frac{x^5}{5!}-\frac{x^7}{7!}\ldots & \quad x \in \mathbb{R}
	\end{aligned}$$

	\item
	$$\begin{aligned}
	\cos(x)= \sum_{n=0}^{\infty} (-1)^n\frac{x^{2n}}{(2n!)} = 1-\frac{x^2}{2!}+\frac{x^4}{4!}-\frac{x^6}{6!}+\ldots & \quad x \in \mathbb{R}
	\end{aligned}$$

	\item 
	$$\begin{aligned}
	\ln(1+x)= \sum_{n=1}^{\infty} (-1)^{n+1}\frac{x^n}{n!} = x-\frac{x^2}{2!}+\frac{x^3}{3!}-\frac{x^4}{4!}\ldots & \quad x \in (-1,1]	\end{aligned}$$

	\item $$\begin{aligned}
	\frac{1}{1-x}= \sum_{n=0}^{\infty} x^n = 1+x+x^2+x^3+\ldots & \quad x \in (-1,1)
	\end{aligned}$$

\end{enumerate}

\section{L'Hôpital's Rule}

\begin{theorem}
Suppose $f$ and $g$ are differentiable functions over an open interval containing $a,$ except possibly at $a.$ If $\lim_{x\to a} f(x)=0$ and $l\lim_{x\to a} g(x)=0,$ then $$\lim_{x\to a}\left(\f{f(x)}{g(x)}\right)=\lim_{x\to a} \left(\f{f'(x)}{g'(x)}\right),$$ assuming the limit on the right exists or is $\pm \infty.$
\end{theorem}

\begin{theorem}
Suppose $f$ and $g$ are differentiable functions over an open interval containing $a,$ except possibly at $a.$ If $\lim_{x\to a} f(x)=\pm \infty$ and $l\lim_{x\to a} g(x)=\pm \infty,$ then $$\lim_{x\to a}\left(\f{f(x)}{g(x)}\right)=\lim_{x\to a} \left(\f{f'(x)}{g'(x)}\right),$$ assuming the limit on the right exists or is $\pm \infty.$
\end{theorem}

\end{document}		