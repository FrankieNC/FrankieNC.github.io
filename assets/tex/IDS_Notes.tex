\documentclass[12pt, a4paper]{article}   	
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage{color}   
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{multicol}
\usepackage[thinc]{esdiff}
\usepackage{physics}
\usepackage{bm}
\usepackage{pdfpages}
\usepackage{pdflscape}
\usepackage{listings}
\usepackage{float}

\usepackage{hyperref}

\hypersetup{colorlinks=true, linktoc=all, linkcolor=black,}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\imply}{\Rightarrow}
\newcommand{\Cal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}

\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{example}{Example}
\newtheorem{proposition}{Proposition}

\theoremstyle{plain}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Introduction to Dynamical Systems Notes}
\date{}
\author{Francesco Chotuck}
\begin{document} 
\maketitle 

\tableofcontents

\pagebreak

\section{Differential equations}

\subsection{First order Differential Equations}

\begin{definition}
The order of an ODE is the order of the highest derivative appearing in the ODE.
\end{definition}

\begin{definition}
If we plot the points $\{(t,g(t)); t \in \bb{R}\}$ in the Cartesian plane we generate a \textbf{trajectory} or \textbf{solution curve} of which passes through the point $(t_0,x_0)$ determined by the initial condition $x(t_0) = x_0.$
\end{definition}

\begin{definition}
As we change the initial condition (i.e. the value of $x_0,$ the prescribed value of $x$ at $t = t_0,$ and possibly $t_0$) we generate a \textit{family} of solution curves of also called the \textbf{flow}.
\end{definition}

\begin{definition}
We say that the family of functions $\{h(\cdot, C)\}$ is a \textit{general solution} of an ODE, if $$\diff{}{t}h(t, C) \equiv f(t, h(t,C)),$$ for any C.
\end{definition}

\begin{remark}
The notation is to be understood that $x=h(\cdot, C)$ i.e. a function with parameter $C.$ So, a first order ODE has $1$ free parameter.
\end{remark}

\subsubsection{First order, explicit}

Explicit equations which can be solved by direct integration for example: $$\diff{x}{t} = f(t),$$ i.e. $$ x = \int f(t) \, dt = F(t)+C.$$ Therefore, the general solution is $$x= x(t) = F(t)+C.$$

\subsubsection{First order, variables separable}

Equations which appear in the form of $$\diff{x}{t}=f(t)g(x).$$ We can separate the variable in order tot evaluate the integral i.e. $$\begin{aligned}
\int \f{1}{g(x)} \, dx &= \int f(t) \, dt \\
G(x)+C_1&=F(t)+C_2 \\
G(x) &= F(t) +C \quad (\text{where } C=C_2-C_1).
\end{aligned}.$$ Hence, the general solution of a first order variable separable ODE is as follows $$x=x(t)=G^{-1}(F(t)+C).$$ It is not always possible to express the solution an explicit expression for $G^{-1}.$ Therefore, another possible form for a solution is $$G(x)=F(t)+C.$$

\subsubsection{First order, homogeneous}

\begin{definition}
A multivariable function is said to be \textit{homogenous} of degree $d$ if rescaling all the variables simultaneously by the same factor $\lambda$ is the same as rescaling the function by $\lambda^d$ :$$F(\lambda x_1,\lambda x_2,\ldots,\lambda x_n) = \lambda^d F(x_1,x_2,\ldots,x_n).$$
\end{definition}

\begin{definition}
An ODE is called homogenous if it is of the form $$\diff{x}{t}=f(t,x)$$ where $f(t,x)$ is homogenous of degree $0,$ that is if $$f(\lambda t,\lambda x) = f(t,x).$$
\end{definition}

\begin{remark}
Homogenous ODEs are invariant to rescaling.
\end{remark}

\begin{note}
An homogenous ODE can be written in the form $$\diff{x}{t}= G\left(\f{x}{t}\right),$$ by setting $\lambda = \f{1}{t}.$
\end{note}

To solve a first order homogeneous ODE we set $v(t)=\frac{x}{t}$ or $v(t)=xt$ where $v$ is a function of $t.$ Note, we can think of $v$ as velocity, $x$ as displacement and $t$ as time. We then have $$
\begin{aligned} 
\diff{x}{t}=\diff{}{t}(vt)&=t\diff{v}{t}+v =G(v) \\
\diff{v}{t}&=\frac{G(v)-v}{t}.
\end{aligned}$$ which is separable with the solution $$\int \frac{1}{G(v)-v} \, dv = \ln|t|+C.$$ After carrying out the $v$ integration we then have a relation between $x(t)$ and $t$ which, with luck, we can simplify to find $x(t)$ -- but this is often not possible.

\subsubsection{First order, linear}

First order, linear ODEs are of the form $$\diff{x}{t} = f(t)x +g(t).$$ They are called \textit{linear} because, when written as $$\diff{x}{t}-xf(t)=g(t),$$ the left-hand side is a linear function of $x$ and its derivative. Let $L$ be this linear operator we have that $$L(x)=g, \quad \text{where } L=\left(\diff{}{t}-f(t)\right), \quad L(x)=\left(\diff{}{t}-f(t)\right)x=\left(\diff{x}{t}-xf(t)\right).$$ First order linear ODEs can be solved with the method of \textbf{integrating factors.} 
\begin{align} \diff{x}{t}-xf(t)=g(t) \end{align} Multiply through by $\mu =\mu(t)$ where $\mu$ is the integrating factor for $(1).$ We have \begin{align} \mu\diff{x}{t}-\mu xf(t)&=\mu g(t). \end{align} Notice that \begin{align} \diff{}{t}(\mu x) = x\diff{\mu}{t}+\mu\diff{x}{t}, \end{align} so substituting $(3)$ into $(2)$ we have $$\diff{}{t}(\mu x) + \left(-\mu f(t)-\diff{\mu}{t}\right)x=\mu g(t).$$ By choosing $\mu$ such that $$\begin{aligned} -\mu f(t)&-\diff{\mu}{t} =0  \\ \diff{\mu}{t} &= -\mu f(t), \end{aligned}$$ one finds that $(2)$ simplifies to $$\diff{}{t}(\mu x)=\mu g(t),$$ an explicit first order ODE, which is easier to solve. It is obvious that the choice of $$\mu=\mu(t)= e^{-\int f(t) \, dt}.$$

\begin{remark}
For a first order linear ODE of the form $$\diff{x}{t}+xf(t)=g(t),$$ the integrating factor is $$\mu(t)=e^{\int f(t) \, dt}.
$$
\end{remark}

\section*{Aside on first order linear ODE}

Recall that a linear ODE can be written as \begin{align} L(x)=g(t). \end{align} We know from \textit{Linear Algebra} that the linear operator $L$ that there is a function, known as the \textbf{complementary function}, $x_{cf}(t)$ satisfies the property \begin{align} L(x_{cf})=0. \end{align} So, $x_{cf}(t)$ is an element of the kernel of $L$; the kernel of a linear operator is a vector space so the solution to $L(x_{cf})=0$ form a vector space -- for a first order ODE this will be a one-dimensional vector space. 

The ODE $(4)$ is known as an \textbf{inhomogeneous} linear differential equation, the term $g(t)$ on the right-hand side is called the \textbf{inhomogeneous term} and any solution $x_{pi}(t)$ of $(4)$ is called a particular integral. \\
An equation such as $(5)$ is known as a \textbf{homogeneous} linear differential equation, where the solution of it is $x_{cf}(t).$ 

Hence, by the \textit{principle of linearity} the general solution for $(4)$ is $$x(t)=x_{pi}(t)+x_{cf}(t), \quad L(x_{pi})=g(t) \text{ and } L(x_{cf})=0.$$

\begin{remark}
This means that the difference of $x_{pi}$ and $x_{cf}$ can also form the general solution.
\end{remark}

\subsection*{Example exam questions}

\begin{example}
To which type of ODE is the following a solution to: $$y^2=x^3+Cx^2.$$ \textbf{Solution:} There are two methods to work it out \begin{itemize}

	\item Work out the ODE explicitly:

	Rearrange to get $$\begin{aligned} 
	C &= \frac{y^2}{x^2}-x \\
	\diff{}{x}(C) &= \diff{}{x} \left(\frac{y^2}{x^2}-x\right) \\
	0 &= \frac{2yy'}{x^2}-\frac{2y^2}{x^3}-1 \\
	y'&=\frac{y}{x}+\frac{x^2}{2y}.
	\end{aligned}$$ This ODE is not explicit, separable, homogeneous or linear.

	\item Remember the form of solutions:

	We have $$y^2=x^3+Cx^2 \iff y = \pm \sqrt{x^3+Cx^2}.$$ This is not in the form of the solution of the known types: 
	\begin{itemize}
	
		\item $y=F(x)+C$ (Explicit);

		\item $F(y)=G(x)+C$ (Separable);

		\item $y=F(x)+CG(x)$ (Linear);

		\item $y=xF(Cx)$ (Homogeneous).
	
	\end{itemize}

\end{itemize}
\end{example}

\subsection{Initial value problems}

\begin{proposition}
The initital value problem $$\diff{x}{t}=\frac{x}{t} \quad x(0)=x_0$$ has \textbf{no solutions} if $x_0\neq 0$ and has an infinite number of solutions if $x_0=0.$
\end{proposition}

\subsection{Existence and uniqueness}

\begin{theorem}
(a version of Peano’s theorem) Consider the initial value problem $$\diff{x}{t}= f(t,x), \quad x(t_0)=x_0.$$ If we can find a value $\delta$ such that the function $f : A \to \bb{R}$ is continuous on the set $A$ defined $$\{(t,x): |t-t_0 \leq \delta, |x-x_0|\leq \delta|\}$$ (a square centre the initial point $(t_0,x_0)$ and side of length $2\delta$), then there is an interval $[t_0-\delta_1, t_0 + \delta_1]$ around the point value $t_0$ (with $\delta_1 \leq \delta$) in which there exists a solution $x(t).$ 
\end{theorem}

\begin{remark}
The i.v.p. could have a solution for all $t,$ or just for a very small range, the theorem does not tell you precisely where $x(t)$ is defined, but it does say that close to $t_0$ it exists.
\end{remark}

\begin{theorem}
(a version of Picard’s theorem / the Picard-Lindelöf theorem) Consider the initial value problem $$\diff{x}{t}=f(t,x), \quad x(t_0)=x_0.$$ If we can find a value $\delta$ such that the function $f : A \to \bb{R}$ and the partial derivative $f_x=\pdv{f}{x} : A \to \bb{R}$ are both continuous on the set $A$ defined as $$\{(t,x): |t-t_0 \leq \delta, |x-x_0|\leq \delta|\}$$ (a square centre the initial point $(t_0,x_0)$ and side of length $2\delta$), then there is an interval $[t_0-\delta_1, t_0 + \delta_1]$ around the point value $t_0$ (with $\delta_1 \leq \delta$) in which there exists a solution $x(t)$ and in which the solution is \textbf{unique}. 
\end{theorem}

\begin{note}
In a question it suffices to check whether $f$ and $f_x$ are continuous at a point $(t_0,x_0).$
\end{note}

\subsubsection{Picard's and Peano's Theorems (Short)}

\textbf{Peano:} If $f(t,x)$ is continuous in a box around $(t_0,x_0)$ then there is a solution to the i.v.p. for \underline{some} range $t \in [t_0-\delta,t_0+\delta].$ \\ \textbf{Picard:} If $f(t,x)$ and $\pdv{f}{x}(t,x)$ are continuous in a box around $(t_0,x_0)$ then there is a \underline{unique} solution to the i.v.p. for \underline{some} range $t \in [t_0-\delta,t_0+\delta].$

\subsubsection{Types of solutions}

\includepdf[pages=-]{./Resources/forms of solutions.pdf}

\subsection{Second order Differential Equations}

\begin{definition}
We say that the family of functions $x(t) = h(t, C1, C2),$ involving two free parameters, $C_1, C_2$
is a general solution of a second order ODE if it is a solution for all $C_1,C_2.$
\end{definition}

\begin{remark}
A second order ODE has $2$ free parameters
\end{remark}

\begin{definition}
Two coupled first order ODES for functions $x_1(t), x_2(t)$ are equations of the form $$\begin{aligned}
\diff{x_1}{t}&=f_1(t,x_1,x_2), \\
\diff{x_2}{t}&=f_2(t,x_1,x_2).
\end{aligned}$$
\end{definition}

\begin{proposition}
Every second order ODE is equivalent to a pair of coupled first order ODEs.
\end{proposition}

\subsubsection{Second order linear, with constant coefficients}

These are ODEs of the form $$\diff[2]{x}{t}+a\diff{x}{t}+bx=\phi(t),$$ where $a$ and $b$ are constants and $\phi$ is a given function of time. Just as with first order linear equations, these can be written in terms of a linear differential operator $L:$ $$L(x)=\phi, \quad L=\left(\diff[2]{}{t}+a\diff{}{t}+b\right).$$

\begin{definition}
The function $\phi(t)$ in the equation above is called the \textbf{inhomogeneous term}.
\end{definition}

\begin{definition}
If the function $\phi(t)$ in the equation above is zero, the equation is said to be a \textbf{homogeneous linear} ODE. (The reason is that $L(x)$ is homogeneous of degree 1, $L(\lambda x) = \lambda L(x)).$
\end{definition}

\begin{definition}
A \textbf{particular solution} of an inhomogeneous second order linear ODE is any one solution of the ODE.
\end{definition}

\begin{definition}
A \textbf{complementary function} for an inhomogeneous second order linear ODE is a solution of the corresponding homogeneous linear ODE.
\end{definition}

\subsection{Solving a second order linear ODE with constant coefficients. Method I: general theory}

Here we give a construction of the general solution to a second order linear ODE with constant coefficients. Note: it is \textbf{NOT} recommended as a practical method. We shall call this method, \textbf{Method 1.} \\ 
One way to explain why the fact that the coefficients are constants means the equation can be solved is that it lets one factorise the operator L into the product of two first order linear differential operators: $$L=\left(\diff[2]{}{t}+a\diff{}{t}+b\right)=\left(\diff{}{t}-\lambda_1\right)\left(\diff{}{t}-\lambda_2\right) = \diff[2]{}{t}-(\lambda_1+\lambda_2)\diff{}{t}+\lambda_1\lambda_2,$$ where $\lambda_1$ and $\lambda_2$ satisfy $$\lambda_1+\lambda_1 = -a, \quad \text{and} \quad \lambda_1\lambda_2=b.$$ This means that $\lambda_1$ and $\lambda_2$ each satisfy the equation $$(\lambda-\lambda_1)(-\lambda-\lambda_2)=\lambda^2-(\lambda_1+\lambda_2)\lambda +\lambda_1\lambda_2= \lambda^2+a\lambda+b=0.$$ This is call the \textbf{auxiliary} or \textbf{characteristic} equation. We have now turned our second order equation into two first order linear equations which we can solve by using the method of integrating factors (twice). \\
As the first step, we write $L_2(x) = u$ and so the original equation becomes $$L(x)=L_1(L_2(x))=L_1(u)=\left(\diff{}{t}-\lambda_1\right)u = \phi,$$ using the integrating factor we have a solution for $u.$ Following the integrating factor method once again to solve for $L_2(x) = u$ we obtain the solution for $x,$ i.e. solving $$L_2(x)=u=\left(\diff{}{t}-\lambda_2\right)x.$$ Provided $\lambda_1 \neq \lambda_2$ the general (homogeneous)solution is $$C_1e^{\lambda_1t}+C_2e^{\lambda_2t}.$$ Whereas, if $\lambda_1=\lambda_2$ i.e. $\lambda$ is a repeated root we get $$C_1te^{\lambda_1t}+C_2e^{\lambda_1t}.$$

\begin{example}
Find the general solution of the differential equation $$\diff[2]{x}{t}-9x=t,$$ we rewrite the ODE as a linear operator $L:$ so we have $$L(x)=\diff[2]{x}{t}-9x =\underbrace{\left(\diff{}{t}-3\right)}_{L_1}\underbrace{\left(\diff{}{t}+3\right)}_{L_2}x=t.$$ Setting $u = \diff{x}{t}+3x,$ so we have to solve the first order linear ODE $L_1(u)=t:$ $$\begin{aligned}
\diff{u}{t}-3u&=t\\
\imply\diff{}{t}\left(ue^{-3t}\right) &=te^{-3t}\\
\imply u&=e^{3t}\int te^{-3t} \,dt \\
&= -\f{t}{3}-\f{1}{9}+C_1e^{3t}.
\end{aligned}$$ Now that we know $u$ we can solve $L_2(x)=u$ so we have: 
$$\begin{aligned}
\diff{x}{t}+3x &= u \\
\diff{x}{t}+3x &= -\f{t}{3}-\f{1}{9}+C_1e^{3t}.
\end{aligned}$$ By using the integrating factor of the LHS (i.e. $e^{3t}$) and multiply both sides:
$$\begin{aligned}
\diff{}{t}\left(xe^{3t}\right)&=\left(\f{t}{3}+\f{1}{9}\right)e^{3t}+C_1e^{6t}\\
\imply x&= e^{-3t}\int \left[\left(\f{t}{3}+\f{1}{9}\right)e^{3t}+C_1e^{6t}\right] \, dt \\
&= -\f{t}{9}+C_2e^{-3t}+\f{C_1}{t}e^{3t}.
\end{aligned}$$ We can re-scale $C_1$ by 6 to get the ``nicer'' expression $$x=\f{t}{9}+C_1e^{3t}+C_2e^{-3t}.$$
\end{example}

\subsubsection{Method II: guessing a particular integral}

This is a simpler method than Method 1 which involves `guessing' the particular integral -- this is the standard A-level/Pre-U method of solving second order linear ODE.

\subsubsection*{Solving the homogeneous equation: simpler method}

The homogeneous equation is solved with the help of solutions of the auxiliary equation. This method is based on the observation that the equation $\ddot{x}+a\dot{x}+bx=0$ is solved by functions of the form $$x(t)=x=e^{\lambda t}$$ \textbf{provided} $\lambda$ is properly chosen. The choice of $\lambda$ is obtained by the roots of the substituting the solution of the ODE, using $\dot{x}=\lambda e^{\lambda t}$ and $\ddot{x}=\lambda^2 e^{\lambda t},$ so we get $$\ddot{x}+a\dot{x}+bx=(\lambda^2+a\lambda+b)e^{\lambda t}=0.$$ Since $e^{\lambda t} \neq 0,$ we see that $e^{\lambda t}$ is a solution of the ODE provided that $$p(\lambda)=\lambda^2+a\lambda+b=0.$$ Since the auxiliary equation is a quadratic in $\lambda$ we have three possibilities \begin{enumerate}
	
	\item[(i)] $a^2-4b \geq 0,$ the auxiliary equation has two distinct real solutions $\lambda_1,\lambda_2.$ So, it follows that $x_1=e^{\lambda_1t}$ and $x_2=e^{\lambda_2t}$ therefore, the general solution is a linear combination of these two solutions ($x_1$ and $x_2$ are complementary functions so $L(x_1)=L(x_2)=0$) i.e. $$x=x(t)=C_1e^{\lambda_1t}+C_2e^{\lambda_2t}.$$

	\item[(ii)] $a^2-4b=0,$ the auxiliary equation has two repeated roots $\lambda_{1,2} =\lambda;$ in this case we that the general solution is $$x=C_1te^{\lambda t}+C_2e^{\lambda t}.$$

	\item[(iii)] $a^2-4b=0,$ the auxiliary equation has two complex conjugate roots $\lambda_{\pm}=\mu \pm i\nu.$ This means that $x_{\pm}(t)=e^{(\mu \pm i\nu)t}$ are the solutions solutions and the general solution using case (i) is $$C_1e^{(\mu + i\nu)t}+C_2e^{(\mu - i\nu)t},$$ where $C_1$ and $C_2$ are free parameters. This solution is fine but we often want the general solution in terms of a \textit{real} function so we have $$x=x(t)=C_1e^{\mu t}\cos{(\nu t)}+C_2e^{\mu t}\sin{(\nu t)},$$ where in this case $C_1$ and $C_2$ are free \textit{real} parameters. 

\end{enumerate}

\subsubsection*{Finding a particular integral}

A guess is often called a “trial function” or an \textit{ansatz} (plural \textit{ansätze}) to make it sound better. This is often the fastest way. The reason this can be done is that the linear operator L in the equation $$L(x)=\phi,$$ maps polynomials to polynomials, trigonometric functions to trigonometric functions, exponential functions to exponential functions and so on.

\begin{center}
\begin{tabular}{|c|c|} 
 \hline
 $\mathbf{f(x)}$ & \textbf{Substitution} \\ \hline
 $c$ & $\gamma$ \\ \hline
 $bx+c$ & $\beta x+ \gamma$ \\ \hline
 $ax^2+bx+c$ & $\alpha x^2+\beta x +\gamma$ \\ \hline
 $Qe^{px}$ & $Qe^{px}$ (or $Qxe^{px}$ or $Qx^2e^{px}$) \\ \hline
 $r\cos{(\omega x)}+s\sin{(\omega x)}$ & $\rho \cos{(\omega x)}+\sigma \sin{(\omega x)}$ \\ \hline
\end{tabular}
\end{center}

\begin{tcolorbox}
IMPORTANT: If there is a multiple of $f(x)$ in the complementary function then the usual choice of substitution will not work. So, multiply the substitution by $x$ until you find a substitution that works. (This is referring to the substitution of $Qe^{px}$)
\end{tcolorbox}

\begin{remark}
If the inhomogeneous term is a polynomial in this course it is usual to `guess' a solution of the ODE to be a quadratic polynomial or higher, so that the second derivative has a non-zero term.
\end{remark}

\subsubsection{Method III: varying constants}

If the inhomogeneous term is complicated so that a particular integral is not easy to guess, this method is suitable to find it. This method is best illustrated by an example.

\begin{example}
Find the general solution of the differential equation $$\diff[2]{x}{t}-9x=t.$$ \\ \textbf{Solution:} The characteristic equation is $\lambda^2 - 9 = 0$ so that $\lambda= \pm3$ and the solution to the homogeneous equation is $$x=Ae^{3t}+Be^{-3t}.$$ We can take as a starting point for the method of varying constants the function $$x=F(t)e^{3t},$$ so that $$\dot{x}=\left(\dot{F}+3F\right)e^{3t}, \quad \ddot{x}=\left(\ddot{F}+6\dot{F}+9F\right)e^{3t},$$ and the equation for $F$ and for $\psi=\dot{F}$ become 
\begin{align}
\ddot{x}-9x&=t \nonumber\\
\imply \left(\ddot{F}+6\dot{F}\right)e^{3t}&=t \nonumber\\
\imply \left(\ddot{F}+6\dot{F}\right)&=te^{-3t} \nonumber\\
\imply \dot{\psi}+6\psi&=te^{-3t}.
\end{align} Using the integrating factor (i.e. $e^{6t}$) of $(6)$, it becomes $$\diff{}{t}\left(\psi e^{6t}\right)=te^{3t},$$ which can be integrated directly to give \begin{align}
\psi e^{6t} &= \int te^{3t} \, dt \nonumber \\
			&= \left(\f{t}{3}-\f{1}{9}\right)e^{3t}+C \nonumber \\
\imply 	\psi&= \left(\f{t}{3}-\f{1}{9}\right)e^{-3t}+Ce^{-6t},
\end{align} $(7)$ can be integrated again to give $$\begin{aligned}
F 	&=\int \psi \, dt \\
	&= -\f{t}{9}e^{-3t}+C'e^{-6t}+D \\
\imply x=Fe^{3t}&=-\f{t}{9}+C'e^{-3t}+De^{3t}.
\end{aligned}$$
\end{example}

\subsection{Two coupled first order ODEs}

The system we are considering are equations of the form $$\begin{pmatrix} \dot{x} \\ \dot{y} \end{pmatrix} = \begin{pmatrix} a&b \\ c&d \end{pmatrix} \begin{pmatrix} x\\y \end{pmatrix}$$ which we can also write as $$\dot{\bm{r}}=M\bm{r}, \text{ where } \bm{r}=\begin{pmatrix} x\\y \end{pmatrix}, M=\begin{pmatrix} a&b \\ c&d \end{pmatrix}.$$ As mentioned in the previous section we can write two coupled first order linear ODEs as second order linear ODE; we do this by eliminating $y$ and writing the ODE in terms of $x$ and we get $$\ddot{x}-\underbrace{(a+d)}_{t=\Trace(M)}\dot{x}+\underbrace{(ad-bc)}_{\Delta=\det(M)}=0.$$ We know from the previous section that we should for a solution of the $x=Ae^{\lambda t}$ where $\lambda$ satisfies $$\lambda-(a+d)\lambda+(ad-bc)=0.$$ Substituting $x=Ae^{\lambda t}$ (and after a bit of algebraic manipulation) we have $$M\bm{\xi}=\lambda\bm{\xi}, \text{ where } M= \begin{pmatrix} a&b \\ c&d \end{pmatrix}.$$ Therefore, to solve these types of ODEs we must find the eigenvectors and their eigenvalue obtained by the characteristic polynomial. Recall from LAG1, that for any real $2\times 2$ matrix $M,$ is similar to one of only three forms of Jordan form matrices so, exactly one of the following holds:

\begin{itemize}
	
	\item[(i)] $M$ has two real eigenvalues, $\lambda_1,\lambda_2$, and we can find two linearly independent eigenvectors ($\bm{\xi}_1$ and $\bm{\xi}_2$) that span $\bb{R}^2;$ so the general solution to the matrix differential equation is $$\begin{pmatrix} x(t) \\ y(t) \end{pmatrix}=\bm{r}(t)=C_1\bm{\xi}_1e^{\lambda_1 t}+C_2\bm{\xi}_2e^{\lambda_2 t}.$$

	\item[(ii)] $M$ has only one real eigenvalue and only a one--dimensional space of eigenvectors; then one can always fine non-zero vectors $\bm{\xi}$ and $\bm{\zeta}$ such that $$M\bm{\xi}=\lambda \bm{\xi}, \quad M\bm{\zeta}=\lambda \bm{\zeta}+\bm{\xi}.$$ That is, $\bm{\xi}$ is an eigenvector and $\bm{\zeta}$ is another vector which is not an eigenvector. In this case the general solution is $$\begin{pmatrix} x(t) \\ y(t) \end{pmatrix}=\bm{r}(t)=C_1\bm{\xi}_1e^{\lambda t}+C_2(\bm{\zeta}+t\bm{\xi})e^{\lambda t}.$$

	\item[(iii)] $M$ has two distinct complex conjugate eigenvalues ($\alpha\pm i\beta$) with complex conjugate eigenvectors. Then we can find a non-zero complex vector $\bm{\xi}$ such that $\bm{\xi}=\bm{\mu}+i\bm{\nu}$ and $\bm{\xi}^{*}=\bm{\mu}-i\bm{\nu}$ are corresponding eigenvectors (where $\alpha, \beta, \bm{\mu}$ and $\bm{\nu}$ are real) such that $$M\bm{\mu}=\alpha\bm{\mu}-\beta\bm{\nu}, \quad M\bm{\nu}=\beta\bm{\mu}+\alpha\bm{\nu}.$$ In this case the general solution is $$\begin{pmatrix} x(t) \\ y(t) \end{pmatrix}=\bm{r}(t)=C_1\left(\bm{\mu}\cos{(\beta t)}-\bm{\nu}\sin{(\beta t)}\right)e^{\alpha t}+C_2\left(\bm{\mu}\sin{(\beta t)}+\bm{\nu}\cos{(\beta t)}\right)e^{\alpha t}.$$ Another way to write this is $$\bm{r}(t)=\Re\left(D\bm{\xi}e^{\lambda t}\right),$$ where $D = C_1+iC_2$ is an arbitrary complex number.

\end{itemize}

Recall that in each case one can find a real matrix $P$ such that $P^{-1}MP$ is in one of the real Jordan canonical forms. The Jordan canonical form of each case are as follows:

\begin{itemize}

	\item[(i)] $$P^{-1}MP=\begin{pmatrix} \lambda_1&0\\0&\lambda_2 \end{pmatrix}, \quad P=(\bm{\xi}_1:\bm{\xi}_2).$$

	\item[(ii)] $$P^{-1}MP=\begin{pmatrix} \lambda&1\\0&\lambda \end{pmatrix}, \quad P=(\bm{\xi}_1:\bm{\zeta}).$$

	\item[(iii)] $$P^{-1}MP=\begin{pmatrix} \alpha&\beta\\-\beta&\alpha \end{pmatrix}, \quad P=(\bm{\mu}:\bm{\nu}).$$

\end{itemize}

\textbf{Example:} from lecture scans.
\includepdf[pages=-]{./Resources/Chapter3-2022-example.pdf}

\subsection{Existence and Uniqueness -- Picard's theorem}

\begin{theorem}
\textbf{Picard’s theorem} Let $A$ denote the cube $$\{(t,x_1,x_2):|t-t_0|\leq \delta, |x_1-a_1\leq \delta|, |x_2-a_2|\leq \delta, \text{ for some } \delta>0\}$$ and suppose that $f_1,f_2 : A \to \bb{R}$ are continuous. Suppose also that the partial derivatives w.r.t. the second and third arguments, $\pdv{f_i}{_1}$ and $\pdv{f_i}{x_2}$ for $i = 1,2$ are continuous on $A.$  Then the system of differential equations $$\diff{x_1}{t}=f_1(t,x_1,x_2), \quad \diff{x_2}{t}=f_2(t,x_1,x_2), \quad x_1(t_0)=a_1, \quad x_2(t_0)=a_2$$ has a \textbf{unique solution} $x_1=\phi(t),x_2=\phi(t), t\in[t_0-\delta_1,t_0+\delta+1],$ for some $\delta_1\leq \delta.$
\end{theorem}

\subsection{Simple Harmonic Motion}

\begin{definition}
We say the equation $$\diff[2]{x}{t}=-\omega^2x,$$ is the equation of \textbf{simple harmonic motion.} The constant $\omega$ is called the \textit{angular frequency.} The general solution is $$x=C_1\sin(\omega t)+C_2\cos(\omega t).$$
\end{definition}

If we define $$y=-\frac{1}{\omega}\diff{x}{t}$$ then $(x,y)$ satisfy the coupled first order ODEs $$\diff{}{t}\begin{pmatrix} x\\y \end{pmatrix}=\begin{pmatrix} 0&-\omega\\ \omega & 0 \end{pmatrix}\begin{pmatrix} x\\y \end{pmatrix}.$$ Then the general solution is $$x=C_1\sin(\omega t)+C_2\cos(\omega t), \quad y=-C_1\cos(\omega t)+C_2\sin(\omega t).$$ The motion is periodic with period $T$ and frequency $f$ given by $$T=\frac{2\pi}{\omega}, \quad f=\frac{\omega}{2\pi}.$$ A physical system with its motion dictated by the equation of simple harmonic motion can be said to be a \textit{simple harmonic oscillator.}

\section{Dynamical Systems}

First we give a definition of a dynamical system as we will consider it in this course.

\begin{definition}
A \textbf{dynamical system} (DS) is any system described by a set of variables $\{x_1,\ldots, x_n\}$ which depend on time $t$ and satisfy a set of ordinary differential equations (ODEs) of the form $$\diff{}{t}x_i=f_i(t,x_i,\ldots,x_n), \quad i = 1,\ldots,n.$$ The functions $f_i$ are called \textbf{velocity functions}. If we write the variables as a vector, then its velocity $\bm{f}$ is given by the velocity functions, $$\bm{x} = \begin{pmatrix} x_1 \\x_2\\\vdots\\x_n \end{pmatrix}, \diff{}{t}\bm{x} = \bm{f}(t,\bm{x})= \begin{pmatrix} \dot{x_1}\\\dot{x_2}\\\vdots\\\dot{x_n} \end{pmatrix} =\begin{pmatrix} f_1 \\f_2\\\vdots \\ f_n \end{pmatrix}.$$ If the variables $\bm{x}$ take a value $\bm{x}(t_1) = \bm{a}$ at time $t_1$ then we say that the system is in the state $\bm{a}$. 
\end{definition} 

\begin{definition}
The \textbf{phase space} $\Gamma$ is the set of admissible values for $\bm{x}$ or $\{x_i\}.$
\end{definition}

\begin{definition}
If the description involves $n$ variables, the dynamical system is said to be of \textbf{order} $n.$
\end{definition}

\begin{definition}
If the time t does not explicitly appear in the velocity functions fi, the system is said to be \textbf{autonomous}, otherwise it is \textbf{non-autonomous}. Thus for autonomous systems, we have $$\diff{}{t}\bm{x}=\bm{f}(\bm{x}).$$
\end{definition}

\begin{proposition}
 An $n^{th}$ order non-autonomous system is equivalent to an $(n+1)^{th}$ order autonomous system.
\end{proposition}

\begin{definition}
The set of points $\{(t,x(t)); t \in \bb{R}\}$ which solve a ODE, and for which $x(t_0) = x_0$ is called the \textbf{trajectory} or \textbf{solution curve} of the DS passing through $x_0.$
\end{definition}

\begin{definition}
The set of all trajectories obtained by varying $t_0$ and $x_0$ through all physically allowed values is called the \textbf{flow} of the DS.
\end{definition}

\begin{definition}
The set of points $\{x(t); t \in \bb{R}\}$ which solve the ODE, and for which $x(t_0) = x_0$ is called the \textbf{orbit} of the DS passing through $x_0.$ 
\end{definition}

\begin{definition}
The set of all orbits obtained by varying $t_0$ and $x_0$ through all physically allowed values is called the \textbf{phase-flow} of the DS. 
\end{definition}

\begin{definition}
A picture which gives an illustrative selection of the orbits and the direction of motion is called a \textbf{phase portrait}.
\end{definition}

\begin{remark}
For a first order autonomous DS an orbit is represented as a set, either and open or closed interval.\\
For a second order autonomous DS an orbit can be thought as looking down onto the $xy$-plane showing the direction of motion of the DS. In this course an orbit is only applicable to second order DS.
\end{remark}

\begin{definition}
An \textbf{invariant set} $S$ is a set such that if $x(t_0) \in S$ then $x(t) \in S$ for all $t,$ both $t > 0$ and $t < 0.$
\end{definition}

\begin{definition}
A point $\bm{a} = (a_1, \ldots a_n)$ is called a \textbf{fixed point} (or \textbf{equilibrium point}) of the (autonomous) DS if $$f_i(\bm{a})=0 \iff \diff{}{t}x_i=0 \quad \text{ at } \bm{x=a}, \quad i=1,\ldots,n.$$A system which is at a FP will stay there forever, unless perturbed.
\end{definition}

\begin{definition}
A FP $\bm{a}$ of a DS is called \textbf{stable}, if for any $\varepsilon > 0$ we can find a $\delta > 0$ such that $\|\bm{x}(0)-\bm{a}\| < \delta$
implies that $\|\bm{x}(t)-\bm{a}\|< \varepsilon$ for all time. i.e. the motion never moves away from the FP. \\ A FP a of a DS is called \textbf{strongly stable}, if for any $\delta > \varepsilon > 0$ we can find a time $T$ such that $\|\bm{x}(0)-\bm{a}\| < \delta$ implies that $\|\bm{x}(t)-\bm{a}\| < \varepsilon$ for all times $t > T,$ i.e. the motion must move in towards the FP.
\end{definition}

\subsection{First order Autonomous Systems}

\subsubsection{Solving a first order autonomous dynamical system}

We will usually want to know the motion from the position $x(t_0) = x_0$ at time $t0$ to the position $x(t)$ at a later time $t.$ We can derive it more correctly from 
$$\begin{aligned}
t=t_0+(t-t_0)&=t_0+\int_{t_0}^{t}\,dt\\
			&=t_0+\int_{x(t_0)}^{x(t)} \diff{t}{x}\,dx\\
			&=t_0+\int_{x_0}^{x} \frac{dx}{dx/dt}\\
			&=t_0+\int_{x_0}^{x} \f{1}{f(x)}\,dx, 
\end{aligned}$$ or just 

\begin{tcolorbox}
$$\tau=t-t_0=\int_{x_0}^{x} \f{1}{f(x)}\,dx$$
\end{tcolorbox}

This represents the time taken to go from $x_0 = x(t_0)$ to $x = x(t).$ 

\subsubsection*{Trajectories}

These are the curves which solve the DS (i.e. the ODE) and are commonly drawn on the $x$-$t$ plane ($t$ axis runs vertically) since the gradient represents $\f{1}{f(x)};$ whereas, on the $t$-$x$ the gradient is the velocity function i.e. $f(x).$ The flow is obtained by translating one of the trajectories by $t.$

\subsubsection*{Fixed points}

The fixed points of a first order autonomous system are values a of $x$ for which $f(a) = 0.$ These correspond to constant trajectories, $x(t) = a,$ (i.e. $x=a$) since this satisfies the ODE $$\diff{}{t}x(t)=\diff{}{t}a=0=f(a)=f(x(t)).$$
These trajectories are horizontal lines in the $t$-$x$ plane and vertical lines in the $x$-$t$ plane.

\subsubsection*{Phase portrait}

The phase portrait is a sketch of the phase space of the dynamical system (which will be the real line or some subset) together with arrows indicating the direction of the motion along the orbit. The direction is to the right if $x$ is increasing, i.e. if $f(x) > 0$ and to the left if $x$ is decreasing, i.e. $f(x) < 0.$

\includepdf[pages=-]{./Resources/1st order DS sketch.pdf}

\subsubsection{Termination of Motion}

\begin{definition}
The motion of a dynamical system is said to be \textbf{terminating}, if it reaches a fixed point or goes to $\pm\infty$ in finite time.
\end{definition}

\begin{remark}
The phrase ``goes to $\pm\infty$ in finite time'' means that $\lim_{t \to c} x(t) =\pm\infty$ for constant $c$ with a DS described by  DS $\dot{x}=f(x).$
\end{remark}

\subsubsection*{How to decide if a motion terminates}

There are four methods that we can use to decide if a motion terminates. 
\begin{enumerate}
	
	\item Solve the dynamical system exactly, calculate the time taken.
	\item Use Picard’s theorem, which can prove that a motion does not terminate at a fixed point [but this is all it can prove].
	\item Use Asymptotic analysis - find a much simpler system with the same behaviour which you can solve exactly.
	\item Estimate the time taken to reach the fixed point and find a finite upper bound [motion terminates] or an infinite lower bound [motion does not terminate].

\end{enumerate}

\begin{remark}
First of all determine the evolution of the motion by sketching the velocity function then, apply one of the four methods.
\end{remark}

\subsubsection{Corollary to Picard's Theorem}

Suppose that $x = a$ is a fixed point of the DS $$\dot{x}=f(x)$$ and that $f(x)$ and $f'(x)$ are both continuous at $x = a,$ then the only trajectory that terminates at $x = a$ in finite time is the constant trajectory $x(t) = a.$

\begin{example}
In this case $$f(x)=(x-1)(x-2)(x-3) \quad f'(x)=3x^2-12x+11,$$ are both continuous for all $x,$ and in particular around $x = 2$ and so any motion that approaches $x = 2$ cannot reach $x = 2$ in finite time. \textbf{The motion cannot terminate at $\mathbf{x = 2}$ by the corollary to Picard’s theorem.} 
\end{example}

\subsubsection{Asymptotic Analysis}

With this method we can find functions which are \textit{approximately} similar to the velocity function of a DS as $x\to a$ where is a fixed point.

\begin{definition}
A function $f$ is said to be \textbf{asymptotically equivalent} to another function $g$ as $x \to x^*$ -- in symbols $f(x)\sim g(x)$ as $x \to x^*$ if and only if $$f(x)=g(x)(1+\varepsilon(x)), \quad \text{with } \varepsilon(x)\to0, \quad \text{as } x\to x^*.$$ In this definition the case $x\to\pm\infty$ is allowed. This is the same as $$f(x) \sim g(x) \text{ as } x\to x^* \iff \lim_{x\to x^*} \frac{f(x)}{g(x)}=1.$$
\end{definition}

Since this is an equivalence relation, the following properties are true \begin{enumerate}
	
	\item Symmetric: $f \sim g \iff g \sim f;$
	\item Reflexive: $f\sim f;$
	\item Transitive: $f\sim g$ and $g\sim h \imply f\sim h.$

\end{enumerate}

\begin{remark}
If you are given a function $f(x)$ and asked to find a simpler function which is asymptotically equivalent to $f(x)$ as $x \to x^*,$ it is often usual to choose a function of the form $c(x-x^*)^{\alpha}$ [if this is indeed correct], and this is often called the \textbf{leading asymptotic behaviour}, but this is not a unique choice and other choices may be better for the problem in hand.
\end{remark}

\subsubsection{Asymptotic analysis for DS}

Suppose $\dot{x}=f(x), f(x^*)=0$ and $f(x)\sim g(x)$ as $x\to x^*.$ \begin{enumerate}
	
	\item[Result 1.] The motion with $\dot{x}=f(x)$ terminates at $x=x^*$ (in finite time) $\iff$ the motion with $\dot{x}=g(x)$ terminates at $x=x^*$ (in finite time).

	\item[Result 2.] Suppose $\tau_f$ is finite, if $f \sim g$ as $x\to x^* \imply \tau_f(x_0)\sim \tau_g(x_0)$ as $x_0\to x^*.$ 

\end{enumerate}

\subsubsection{Estimating and bounding time of motion}

\textbf{Estimates} and \textbf{Bounds} for the time $\tau$ for a dynamical system to move from $x = x_0$ to $x = x_1$ can be obtained without giving the full solution of the ODE describing the dynamical system in question, i.e. without actually doing the $x$-integral exactly. This is based on this idea: `if a dynamical system moves from $a$ to $b$ at a faster speed then a second, it will take less time; if it moves slower then it will take more time.' Given a velocity function $f(x)$ we choose two other velocity functions $h(x)$ and $g(x)$ such that $h(x)>f(x)>g(x).$ Ideally, $h(x)$ and $g(x)$ would be chosen in a manner that is simpler to solve than $f(x).$
This in turn becomes 
$$\begin{aligned}
h(x)&\geq f(x)\geq g(x) \\
\frac{1}{h(x)}&\leq \frac{1}{f(x)}\leq \frac{1}{g(x)}\\
\imply \int_{a}^{b} \frac{1}{h(x)} \, dx &\leq  \int_{a}^{b} \frac{1}{f(x)} \, dx \leq \int_{a}^{b} \frac{1}{g(x)} \, dx. \\
\tau_h &\leq \tau_f \leq \tau_g
\end{aligned}$$ If $\tau_g$ is finite then $\tau_f$ is finite so this proves that the motion terminates. Whereas if $\tau_h$ is infinite then $\tau_f$ is infinite which proves the motion does not terminate.

\subsection{Results to remember}

\begin{proposition}
This is only valid if $x(0)>0.$
If $\dot{x}\sim x^{\alpha}$ then $x\to \infty$ in finite time if and only if $\alpha>1.$ \\
If $\dot{x}\sim -x^{\alpha}$ then $x\to 0$ in finite time if and only if $\alpha<1.$ \\
\end{proposition}

\subsubsection{Stability of Fixed points}

The stability of a fixed point can be described as \textit{stable} or \textit{unstable} as illustrated on in the figure below.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/1st order FP stability.png}
\end{figure}

\begin{definition}
The stability analysis based on (and requiring only) Taylor series expansions to $1^{st}$ order is referred to as \textbf{linear stability analysis} of a fixed point.
\end{definition}

If $a$ is the fixed point, $f(a)=0$ then the Taylor expansion of $f(x)$ around $a$ is $$\begin{aligned}
f(x)&=\underbrace{f(a)}_{=0}+(x-a)f'(a)+\ldots \\
f(x)&=(x-a)f'(a)+\ldots\\
\imply f(x)&\sim (x-a)f'(a) \text{ as } x\to 0 \text{ [if $f'(a)\neq 0$]}
\end{aligned}$$ The sign of $f'(a)$ tells us whether a fixed point is stable or not.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/1st Linear Stability analysis FP.png}
\end{figure}

On the other hand, if $f'(a)=0$ we have to look at the first non-zero derivative.

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/1st Linear Stability analysis FP zero gradient.png}
\end{figure}

\subsubsection{Structural stability}

\begin{definition}
A system described by the differential equation $\dot{x}=f(x)$ is called \textbf{structurally unstable}, if the number of fixed points is changed by some arbitrarily small perturbation $f(x) \to f (x) + \varepsilon g(x),$ (i.e., for \textit{some} $g$ and $\varepsilon$ arbitrarily small), otherwise it is structurally stable (i.e., for \textit{all} $g$ there is a sufficiently small $\varepsilon$, such that the number of fixed points remains unchanged).
\end{definition} 

This can be encapsulated as: \textit{`If you change the velocity function in any small way, can you change the number of fixed points?'}

\textbf{Example from the lecture scans}

\includepdf[pages=-]{./Resources/Structural stability example.pdf}

\subsection{Second Order Autonomous Systems}

A second order autonomous system describes the time evolution of the state of a system in some two-dimensional phase-space $\Gamma$ through two differential equations. \\ In this chapter we often find it convenient to use conventional $x,y$ notation instead of $x_1,x_2$ and sometimes to write $f_1,f_2$ as $F,G,$ so the we could write these equations as 
$$\begin{aligned}
\begin{cases}
\diff{x}{t}&=f_1(x,y) \\
\diff{y}{r}&=f_2(x,y)
\end{cases} \quad \text{or} \quad 
\begin{cases}
\diff{x}{t}&=F(x,y) \\
\diff{y}{r}&=G(x,y)
\end{cases}
\end{aligned}$$


\subsubsection{Phase space and Phase portraits}

\begin{definition}
The \textbf{phase space} $\Gamma$ is defined as the set of admissible values for the coordinates $x$ and $y.$ The pair $(x, y)$ denotes a point in $\bb{R}^2,$ so generally $\Gamma \subset \bb{R}^2.$
\end{definition}

\begin{definition}
The set of points $\{(t,x(t),y(t));t \in \bb{R}\}$ with $(x,y)$ is called a \textbf{trajectory} or \textbf{solution curve} (through $(x_0, y_0)$).
\end{definition}

\begin{definition}
A point $(a, b) \in \bb{R}^2$ is a \textbf{fixed point} of the dynamical system if and only if $f_1(a, b) = 0$ and $f_2(a, b) = 0.$
\end{definition}

\begin{definition}
The set of points $\{(x(t),y(t));t \in \bb{R}\}$ is called an \textbf{orbit} or \textbf{phase curve} (through $(x_0, y_0)$).
\end{definition}

If we parametrise the phase curve as $y(x)$ then we have $$\diff{y}{x}=\f{\dot{y}}{\dot{x}}=\f{G(x,y)}{F(x,y)}.$$ It is sometimes possible to solve this differential equation to find the phase curves even if the $t$ dependence of $x(t)$ and $y(t)$ cannot be found. It is also sometimes useful to solve the equations for the phase curve first and then substitute $y = y(x)$ in the equation for $\dot{x}$ to solve for $x(t)$ explicitly.

\begin{definition}
The collection of all possible phase curves is called the \textbf{phase flow} of the dynamical system.
\end{definition}

\begin{definition}
A graphical representation of the the phase space $\Gamma,$ including a graph of the \textbf{phase flow} and/or the velocity $f(x,y)$ evaluated at representative points
\end{definition}

\begin{definition}
The set of points $(x, y) \in \bb{R}^2$ for which $f_1(x, y) = 0$ is called the \textbf{null-cline} of $f_1.$ Similarly, The set of points $(x, y) \in \bb{R}^2$ for which $f_2(x, y) = 0$ is called the \textbf{null-cline} of $f_2.$
\end{definition}

\textbf{Note:} On a null-cline of $f_1,$ the $x$-component of the velocity vanishes so that the flow is vertical; similarly, on a null-cline of $f_2,$ it is the $y$-component of the velocity which is zero so that the flow is horizontal. Fixed points are \textit{intersections} of the null-clines of $f_1$ and $f_2.$ \textbf{It is common to include the null-clines of the velocity functions in the phase portrait} since the null-clines divide the phase space into regions on which the velocity field points in different general directions which we can call by the points of the compass: NW, NE, SW, SE: 
\begin{itemize}

	\item [NE:] If $\{f_1 > 0, f_2 > 0\}$ then the flow is in the general direction $\nearrow$ or in the direction NE.

	\item[NW:] If $\{f_1 < 0, f_2 > 0\}$ then the flow is in the general direction $\nwarrow$ or in the direction NW.

	\item[SW:] If $\{f_1 < 0, f_2 < 0\}$ then the flow is in the general direction $\swarrow$ or in the direction SW.

	\item[SE:] If $\{f_1 > 0, f_2 < 0\}$ then the flow is in the general direction $\searrow$ or in the direction SE.

\end{itemize}

\subsubsection*{Examples}

\includepdf[pages=-]{./Resources/2nd Phase portrait example.pdf}

\subsubsection{Separable systems}

\begin{definition}
If we have the special situation $$\dot{x}=F(x), \quad \dot{y}=G(y),$$ then we say that it is a \textbf{separable dynamical system,} the differential equations for $x(t)$ and $y(t)$ are also separable and can be solved easily to find $$t-t_0=\int_{x_0}^{x(t)} \f{dx}{F(x)}=\int_{y_0}^{y(t)} \f{dy}{G(y)}.$$ If a system is not separable in the original coordinates $(x,y)$ it may be possible to find new coordinates $(u,v)$ in which it is.
\end{definition}

If a system is not separable in the original coordinates $(x,y)$ it may be possible to find new coordinates $(u,v)$ in which it is. Consider a change in variable in polar coordinates. 

To find $\dot{r}$ and $\dot{\theta}$ it can be simplest to differentiate the equations equations $$r^2=x^2+y^2, \quad \tan\theta=\f{y}{x},$$ leading to $$\begin{aligned}
2r\dot{r}&=2x\dot{x}+2y\dot{y} \imply \dot{r}=\f{x\dot{x}+y\dot{y}}{r}=\f{xF(x,y)+yG(x,y)}{r}, \\
\sec^2\theta \, \dot{\theta} &=(1+\tan^2\theta)\dot{\theta}=\f{x\dot{y}-y\dot{x}}{x^2} \imply \dot{\theta}=\f{x\dot{y}-y\dot{x}}{x^2+y^2}=\f{xG(x,y)-yF(x,y)}{x^2+y^2}.
\end{aligned}$$

\subsubsection{Limit cycles}

A limit cycle of a second order dynamical system is a periodic trajectory corresponding to a closed orbit that does not pass through any fixed points. The orbits that pass close to the limit cycle can either all stay close to, or be attracted in towards, the limit cycle, in which case it is a stable limit cycle, they can all be repelled (it is an \textit{unstable limit cycle}) or some can be attracted and some repelled; formally this is also unstable but can be called “half-stable”.

To decide whether a limit cycle is stable or unstable analyse the velocity function $\dot{r}=f(r).$

\includepdf[pages=-]{./Resources/Limit cycles.pdf}

\subsubsection{Linear stability analysis}

We will investigate the stability of the fixed point by performing a \textit{linear stability analysis,} based on a two-variable Taylor expansion (to first order) of the velocity functions $F$ and $G$ in the vicinity of the fixed point. This procedure involves three steps.

\subsubsection*{Step 1 — Taylor Expansion of Velocity Functions}

Let’s consider a second order autonomous system described by $$\diff{x}{t}=F(x,y), \quad \diff{y}{t}=G(x,y).$$ Let’s assume that the point $(a,b)$ is a fixed point, i.e. $F(a,b) = G(a,b) = 0$ and $(a,b)$ is an intersection point of the null-clines of $F$ and $G.$ The constant trajectory $\{x = a, y = b\}$ is a solution of the dynamical system. We can substitute $F(x,y)$ and $G(x,y)$ by their first order approximations and obtain the linearised system: 
$$\begin{aligned}
\dot{x}&=F(x,y)=\underbrace{F(x,y)}_{=0}+(x-a)\pdv{F}{x}+(y-b)\pdv{F}{y} \\
\dot{y}&=G(x,y)=\underbrace{G(x,y)}_{=0}+(x-a)\pdv{G}{x}+(y-b)\pdv{G}{y}.
\end{aligned}$$ Writing this in vector notation we have: $$\begin{pmatrix} \dot{x} \\\dot{y} \end{pmatrix}= \begin{pmatrix} (x-a)\pdv{F}{x}+(y-b)\pdv{F}{y} \\ (x-a)\pdv{G}{x}+(y-b)\pdv{G}{y} \end{pmatrix} = \underbrace{\begin{pmatrix} \pdv{F}{x} & \pdv{F}{y} \\ \pdv{G}{x} & \pdv{G}{y} \end{pmatrix}}_{\text{The Jacobian $J$}}\begin{pmatrix} x-a \\ y-b \end{pmatrix}.$$ The matrix $J$ denotes the \textbf{Jacobian matrix} given by $$J=\begin{pmatrix} \pdv{F}{x} & \pdv{F}{y} \\ \pdv{G}{x} & \pdv{G}{y} \end{pmatrix},$$ evaluated at the fixed point $(a,b).$ Writing $\tilde{x}=x-a, \tilde{y}=y-b$ the linearised system above becomes $$ \diff{}{t}\begin{pmatrix} \tilde{x} \\\tilde{y} \end{pmatrix}=J \begin{pmatrix} \tilde{x}\\\tilde{y} \end{pmatrix}.$$

\subsubsection*{Step 2 — Finding the Jordan canonical form of the Jacobian}

We recall that the real Jordan canonical forms of a real $2\times 2$ matrix are $$\begin{pmatrix} \lambda &0 \\ 0&\mu \end{pmatrix}, \; \begin{pmatrix} \lambda &1 \\ \lambda&0 \end{pmatrix}, \; \begin{pmatrix} \alpha &\beta \\ -\beta&\alpha \end{pmatrix},$$ where $\lambda,\mu,\alpha$ and $\beta$ are all real. The eigenvalues of the matrix $$M=\begin{pmatrix} a&b\\c&d \end{pmatrix},$$ satisfy the equation $$\det(M -\lambda I)=p(\lambda)=\lambda^2-t\lambda+\Delta=0,$$ where $t$ is the \textit{trace} and $\Delta$ is the \textit{determinant} of the matrix $M:$ $$t=(a+d)=\Trace(M), \; \Delta=ad-bc=\det(M).$$ Given the characteristic equation, we can find the eigenvalues as $$\lambda_\pm=\f{1}{2}t\pm\f{1}{2}\sqrt{D}\; D=t^2-4\Delta.$$ $D$ is called the \textit{discriminant} of the matrix $M.$ Looking at the form of the eigenvalues, we see that it is the sign of the discriminant $D$ which determines if they are real or complex. We distinguish three cases: \begin{enumerate}
	
	\item[\textbf{(1)}] $D > 0, \lambda_\pm$ are both real and the Jordan canonical form $C$ is diagonal. It is possible to find a real matrix $P$ such that the canonical form $C$ is diagonal, $$C=P^{-1}MP=\begin{pmatrix} \lambda_{+} &0 \\ 0&\lambda_{-} \end{pmatrix}.$$ If $\bm{\xi}_\pm$ are non-zero eigenvectors corresponding to the eigenvalues $\lambda_\pm$, then one possible choice for $P$ is $P = (\bm{\xi}_{+}:\bm{\xi}_{-})$ where $(\bm{u} : \bm{v})$ is the matrix with columns given by the two vectors $\bm{u}$ and $\bm{v}.$

	\item[\textbf{(2)}]  $D < 0, \lambda_\pm$ are complex conjugate complex numbers, $\lambda_\pm = \alpha \pm i\beta.$ It is possible to find a real matrix $P$ such that the canonical form is $$C=P^{-1}MP=\begin{pmatrix} \alpha &\beta \\ -\beta&\alpha \end{pmatrix}$$ and the Jordan canonical form is not diagonal (it has a complex diagonal canonical form but this is the real canonical form). If $\bm{\xi} = \bm{\mu} + i\bm{\nu}$ is a non-zero eigenvector with eigenvalue $\lambda$ and $\bm{\mu}$ and $\bm{\nu}$ are real vectors then one possible choice for $P$ is $P= (\bm{\mu} : \bm{\nu}).$

	\item[\textbf{(3)}] $D = 0,$ there is only one solution for $\lambda$ but two possibilities for the Jordan canonical form. \begin{enumerate}
		
		\item[\textbf{(3i)}] $M$ could be diagonal, i.e. it is $$C=P^{-1}MP=\begin{pmatrix} \lambda &0 \\ 0&\lambda \end{pmatrix}.$$ This is already in normal form, there is nothing to be done. Note that in this case $P^{-1}MP = M$ for any invertible matrix $P;$

		\item[\textbf{(3ii)}] If $M$ is not diagonal, then it is possible to find a real matrix $P$ such that $$C=P^{-1}MP=\begin{pmatrix} \lambda &1 \\ 0&\lambda \end{pmatrix}.$$ It is always possible in this case to find real non-zero vectors $\bm{\xi}$ and $\bm{\zeta}$ such that $M\bm{\xi}= \lambda\bm{\xi}$ and $M\bm{\zeta} = \lambda\bm{\zeta}  + \bm{\xi}$ and one possible choice for $P$ is $P = (\bm{\xi} : \bm{\zeta}).$

	
	\end{enumerate}

\end{enumerate}

\subsubsection*{Step 3 — Exploring the Consequences for Dynamics}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/2nd FP nature.png}
\end{figure}

\section{Classical Mechanics}

\begin{definition}
\textbf{Kinematics} is the study of the mathematical description of motion and all the properties that follow just from the language of mathematics, without any knowledge of how the motion is governed by physical laws.
\end{definition}

\begin{definition}
\textbf{Dynamics} is the study of the laws that govern motion and their consequences
\end{definition}

\subsection{Motion of a particle}

We assume that space is Euclidean and that it can be described in terms of a system of three dimensional Euclidean position vectors measured relative to some conveniently chosen origin. In order to describe a particle’s motion an observer $O$ may choose a reference frame consisting of a right handed system of Cartesian axes $Oxyz,$ usually fixed relative to the observer; the position of a particle $P$ at a particular instant is then made precise by assigning its position vector which we can write as $\bm{r = \overline{OP}}.$

\begin{definition}
The \textbf{velocity} $\bm{v}(t)$ of a particle at time $t$ is defined as the the derivative of the position with respect to time $$\bm{v}(t)=\dot{\bm{r}}=\diff{\bm{r}}{t}.$$ We say this is the velocity \textit{relative} to $O$ or \textit{as measured by the observer $O$.} The velocity is the rate of change of the position vector and is a \textit{vector}.
\end{definition}

\begin{definition}
The \textbf{speed} $v$ of the particle (as measured by $O$) at time $t$ is the magnitude of its velocity, $v = |\bm{v}| = |\dot{\bm{r}}|.$
\end{definition}

\begin{definition}
The acceleration $\bm{a}(t)$ (as measured by $O$) of the particle at time $t$ is defined by $$\bm{a}(t)=\dot{\bm{v}}=\diff{\bm{r}}{t}=\ddot{\bm{r}}=\diff[2]{\bm{r}}{t}.$$ The acceleration is the rate of change of the velocity vector with respect to time and is also a \textit{vector}.
\end{definition}

\begin{definition}
The distance travelled by the particle is given by the integral $$l=s=\int_{t_0}^{t_1}|\dot{\bm{r}}|\,dt=\int_{t_0}^{t_1}|\dot{\bm{v}}|\,dt=\int_{t_0}^{t_1}v\,dt$$
\end{definition}

\subsubsection{Kinematical results}

\begin{proposition}\label{prop:Change of speed}
The rate of change of the speed of a particle is given by $$\dot{v}=\f{1}{v}\bm{v\cdot a}.$$
\end{proposition}

\begin{proof}
The speed $v$ is the magnitude of the velocity $\bm{v},$ $v = |\bm{v}|$ and so $v^2 = \bm{v \cdot v}.$ Differentiating with respect to $t$ we get $$\begin{aligned}
v^2&=\bm{v\cdot v} \\
\imply \diff{}{t}v^2&=\diff{}{t}(\bm{v\cdot v}) \\
\imply 2v\dot{v}&=2\bm{v\cdot \dot{v}} =2\bm{v\cdot a} \\
\imply \dot{v}v&=\bm{v\cdot a}.
\end{aligned}$$
\end{proof}

\begin{proposition}
If a particle moves at constant speed then its acceleration is \textbf{orthogonal to its velocity.}
\end{proposition}

\begin{proposition}
The rate of change $\dot{r}$ of the distance of a particle from the origin is given by $$\dot{r}=\f{1}{r}\bm{r\cdot v}.$$
\end{proposition}

\begin{proof}
Similar to the proof of Proposition \ref{prop:Change of speed}
\end{proof}

\begin{proposition}
If a particle is moving on a sphere of radius $R,$ centre the origin, then the velocity is orthogonal to the position vector.
\end{proposition}

\begin{proof}
On a sphere the position from the centre is constant thus, $\dot{r}=0.$
\end{proof}

\begin{definition}
A particle undergoes \textbf{uniform motion} if it is moving in a straight line at constant speed. This is equivalent to the particle moving with zero acceleration.
\end{definition}

\subsection{Newton's Law of motion}


Newton assumed the existence of a preferred class of observers and associated reference frames, called \textit{inertial frames of reference}, with respect to which Newton’s laws of motion were supposed to hold. \\ First, every particle $P$ possesses an \textit{inertial mass} $m > 0$ which is normally assumed to be constant, whatever the state of motion of the particle. If $\bm{r}(t)$ is the position vector of $P$ relative to an observer $O$ its \textbf{momentum} $\bm{p}$ is defined by the equation $$\bm{p}=m\dot{\bm{r}}=m\bm{v},$$ where $\bm{v}$ is the particle's velocity.

\begin{definition}
An \textbf{inertial frame of reference} is a frame of reference that is not undergoing acceleration -- or, equivalently, it is a frame of reference in which Newton's first law of motion holds. (From Wikipedia)
\end{definition}

\subsubsection{Newton's First Law (N1)}

Newton’s First Law states that with respect to an inertial frame of reference a particle moves with constant velocity in a straight line unless constrained to depart from this state by a \textit{force} acting on it.

\subsubsection{Newton's Second Law (N2)}

Newton’s Second Law states that with respect to an inertial frame of reference the rate of change of momentum of a particle is equal to the total force acting on the particle. If $\bm{F}_1, \bm{F}_2, \ldots, \bm{F}_n$ are the forces acting on a particle $P,$ whose position vector is r(t) with respect to the origin $O$ of an inertial frame of reference, we have $$\diff{}{t}(m\dot{\bm{r}})=\bm{F},$$ where the total force $\bm{F}$ is given by $$\bm{F}=\sum_{i=1}^{n} \bm{F}_i.$$ Assuming that $m$ is constant we may write $$m\ddot{\bm{r}}=\bm{F}=m\bm{a}.$$
\subsubsection{Newton's Third Law (N3)}

Newton’s Third Law is often stated in the form “action and reaction are equal (in magnitude) but opposite (in direction)”. To see more clearly the implication of this law suppose that the force exerted on a particle $i$ by a particle $j$ is $\bm{F}_{ij};$ correspondingly the force exerted by particle i on particle $j$ is $\bm{F}_{ji}$ and $$\bm{F}_{ij}+\bm{F}_{ji}=\bm{0},$$ or $$\bm{F}_{ij}=-\bm{F}_{ji}.$$

The forces in question could be gravitational or, in the case of two particles connected by a taut string, the tension force provided by the string.

\subsubsection{Centre of Mass}

A consequence of $N3$ leads to the idea of \textbf{centre of mass}: [See lecture scans]

\includepdf[pages=-]{./Resources/Centre of mass.pdf}

\subsection{Newton's Law of Gravitation}

Consider two particles of gravitational masses $m_1,m_2$ respectively and suppose that the position vector of $m_2$ with respect to $m_1$ is $\bm{r}.$ Newton’s law of gravitation states that the gravitational force $\bm{F}$ exerted on $m_2$ by $m_1$ is given by $$\bm{F}=-\frac{G_Nm_1m_2}{r^3}\bm{r}, \quad r=|\bm{r}|,$$ where $G_N$ is an universal constant known as Newton's constant of gravitation.

\subsubsection{Motion in the Earth’s gravitational field}

If we then take the Earth to be point particle of mass $M$ at $\bm{r}_E$ (relative to the origin/observer $O$), and the body moving in the Earth’s field to be a point particle of mass $m$ at $\bm{r}_{body},$ and we denote the position of the body relative to the centre of the Earth by $\bm{r} = \bm{r}_{body} - \bm{r}_E,$ then Newton’s laws give $$M\ddot{\bm{r}}_E=\frac{GMm}{r^3}\bm{r}, \quad m\ddot{\bm{r}}_{body}=-\frac{GMm}{r^3}\bm{r},$$ or $$\mu\ddot{\bm{r}}=-\frac{GMm}{r^3}\bm{r}, \quad \mu =\frac{Mm}{M+m}.$$ If $M\gg m$ then $\mu \approx m$ and we get $$m\ddot{\bm{r}}=-\frac{GMm}{r^3}\bm{r}.$$ We can factor $m$ off both sides to find that the acceleration of the (light) particle now only depends on its position, \begin{tcolorbox}
$$\ddot{\bm{r}}=-\frac{GM}{r^3}\bm{r}.$$
\end{tcolorbox} and its value is the \textbf{gravitational field} of the Earth at the point $r$ relative to the centre of the Earth, $$\bm{g}=-\frac{GM}{r^3}\bm{r}.$$

\subsection{Motion in a straight line}

Let $x(t)$ denote the coordinate at time $t$ of a particle of mass $m$ moving in a straight line along the $x$-axis under the influence of a force $F\bm{e}_1$. The equation of motion is (written three ways) $$m\ddot{x}\bm{e}_1=F\bm{e}_1, \quad m \begin{pmatrix} \dot{x} \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} F \\0 \\0  \end{pmatrix}, m\dot{x} =F.$$

\begin{example}
Consider a particle moving vertically close to the Earth’s surface where the gravitational field can be approximated by $$\bm{g}=\begin{pmatrix} 0 \\ 0 \\ -g \end{pmatrix},$$ where $g$ is a positive constant.

In this case $$\bm{F}=m\ddot{\bm{r}} = m\bm{g} = \begin{pmatrix}0 \\ 0 \\ -mg  \end{pmatrix}.$$ Motion is vertical so, $\bm{r}=\begin{pmatrix} x_0 \\ y_0 \\ z(t)\end{pmatrix}$ with $x_0,y_0$ constant therefore, $$\dot{\bm{r}}=\begin{pmatrix} 0 \\ 0\\ \dot{z} \end{pmatrix}, \quad \ddot{\bm{r}} =\begin{pmatrix} 0\\0\\\dot{z} \end{pmatrix}.$$ Then Newton's equations are $$m\ddot{z}=-mg.$$
\end{example}

\subsection{The energy equation}

\textbf{(This case is for motion in a straight line)} We now make the special assumption that $F$ depends only on $x,$ the position coordinate of the particle so that $F = F (x)$ and the equation of motion becomes $$\begin{aligned}
m\ddot{x}&=F(x)\\
m\ddot{x}\dot{x}&=F(x)\dot{x} \\
\diff{}{t}\left(\f{1}{2}m\dot{x}^2\right)&=F(x)\dot{x}\\
\f{1}{2}m\dot{x}^2&=\int F(x)\dot{x} \, dt +C \\
				&= \int F(x)\diff{x}{t}\, dt +C \\
				&= \int F(x)\, dx +C \\
\underbrace{\f{1}{2}m\dot{x}^2}_{\text{Kinetic energy, } T}&+\underbrace{\left(-\int F(x)\, dx\right)}_{\text{Potential energy, } V} =\underbrace{C}_{\text{Total energy, } E}. \\
\end{aligned}$$
\begin{tcolorbox}
$$T+V=E=\text{constant}$$
\end{tcolorbox}

We note that from the energy equation we have that $$F(x)=-\diff{V}{x}.$$

\subsubsection*{Interpretation of the potential force law}

\begin{figure}[H]
\centering
\includegraphics[width=1\textwidth]{./Resources/Potential force law.png}
\end{figure}

\subsection{Use of the Energy equation}

The relation $$T+V=E,$$ can be re-written as $$T=\f{1}{2}m\dot{x}^2=E-V(x).$$ Since the LHS is non-negative the position of $x$ is restricted to regions in which $$E-V(x)\geq 0.$$

\subsection{Equilibrium and Stability}

Consider a particle of mass $m,$ moving along the $x$-axis under a potential $V(x),$ so that the force acting on $m$ is given by $$F=-\diff{V}{x}=-V'(x).$$ By $\bm{N2}$ the equation of motion is $$m\ddot{x}=-V'(x), \qquad \ddot{x} = -\f{V''(x)}{m}.$$

\begin{definition}
A point $x=a$ for which $V'(a)=0$ is known as an \textbf{equilibrium position} for the particle, i.e. the possible equilibrium positions are the stationary points of $V.$
\end{definition}

Using Taylor's theorem it is possible to linearise $V(x)$ for $x=a$ leading to the equation of motion which approximates to $$\ddot{x}=-\f{V''(a)}{m}(x-a).$$ Writing $z=x-a$ we obtain $$\ddot{z}+\f{V''(a)}{m}z=0.$$ Depending on the sign of $V''(a)$ there are two cases.

\textbf{Case 1:} Suppose that $V''(a)>0,$ so that $x=a$ is a \textbf{minimum} of $V$. Setting $$\omega^2=\f{V''(a)}{m}>0$$ we obtain $$\ddot{z}+\omega^2z=0.$$ This is clearly an equation of motion for \textit{simple harmonic motion} so, this equation has general solution $$z=A\sin(\omega t+\varepsilon),$$ where $A$ and $\varepsilon$ are constants. Expressing $z$ in terms of $x$ we now have $$x(t)=a+A\sin(\omega t+\varepsilon).$$ The initial conditions $x(0)=a, \dot{x}(0)=u$ require that $A\sin(\varepsilon)=0, A\omega\cos(\varepsilon)=u$ from which it follows that $\varepsilon=0, A=\f{u}{\omega}.$ Therefore $$x(t)=a+\f{u}{q}\sin(\omega t).$$ The particle executes \textit{simple harmonic motion} about $x=a$ with amplitude $\f{u}{\omega}$ and period $\f{2\pi}{\omega}.$ Hence, the position $x=a$ is a position of \textbf{stable equilibrium.} 

\textbf{Case 2:} Now suppose that $V''(a) < 0,$ so that $x = a$ is a \textbf{maximum} of $V.$ Setting $$\Omega^2=-\f{V''(a)}{m}>0$$ we obtain $$\ddot{z}-\Omega^2z=0.$$ This equation has general solution $$\begin{aligned}
z(t)=\alpha e^{\Omega t}+\beta e^{-\Omega t},
\imply x(t)=a+\alpha e^{\Omega t}+\beta e^{-\Omega t},
\end{aligned}$$ where $\alpha, \beta$ are constants. Imposing the initial conditions $x(0)=a, \dot{x}(0)=u$ we find that $$x(t)=a+\f{u}{\Omega}\sinh(\Omega t).$$ Since $\sinh(\Omega t) \to \infty$ as $t\to \infty$ we see that $x$ does not remain ``near'' $a,$ so, $x=a$ is a position of \textbf{unstable equilibrium.}

In summary: 

\begin{definition}
Let $x=a$ is a stationary/equilibrium point then, 
\begin{itemize}

	\item if $V''(a)>0,$ so that $x=a$ is a minimum, the position $x=a$ is a position of \textbf{stable equilibrium}, and performs simple harmonic motion ``near'' $x=a;$ 

	\item if $V''(a)<0$ so that $x=a$ is a maximum, the position $x=a$ is a position of \textbf{unstable equilibrium};

\end{itemize}
\end{definition}

\subsection{Periodic motion}

Periodic motion can only happen when the motion is happening in the potential well i.e. when $E<V_\text{max}.$

\section{Hamiltonian systems}

\begin{definition}
If $H(x,p,t)$ is a function of time $t$ and the position $x$ and momentum $p$ of a particle moving in one dimension, then \textbf{Hamilton’s equations} of motion for $x$ and $p$ are $$\dot{x}=\pdv{H}{p}, \quad \dot{p}=-\pdv{H}{q}.$$
\end{definition}

\begin{remark}
Sometimes $q$ is used to denote position such that $p$ is the \textbf{conjugate momentum} of $q.$
\end{remark}

\subsection{The variation of \texorpdfstring{$H$}{TEXT} with \texorpdfstring{$t$}{TEXT}}

In the most general situation, $H$ can depend on $t$ both implicitly through $q(t), p(t)$ but also
explicitly, so that $$H=H(q(t),p(t),t),$$ this can happen for motion in a potential if the potential changes with time.

Using the chain rule we can find how $H$ changes with time: $$\diff{H}{t}=\pdv{H}{q}\diff{q}{t}+\pdv{H}{p}\diff{p}{t}+\pdv{H}{t},$$ and then using Hamilton's equation $$\begin{aligned}
&=\pdv{H}{q}\pdv{H}{p}-\pdv{H}{p}\pdv{H}{q}+\pdv{H}{t}
&=\pdv{H}{t}.
\end{aligned}$$ If $H$ is only a function of $q$ and $p$ and does not depend on $t$ explicitly, i.e. $H \equiv H(q,p),$ then $\delta H/\delta t = 0 $ and so $\text{d}H/\text{d}t= 0,$ in other words, $H$ is constant in time.

In this course only autonomous Hamiltonian systems shall be considered i.e. systems with $H=H(q,p).$

\subsection{Testing if a second order system is Hamiltonian}

\begin{definition}
A second order autonomous system $$\begin{pmatrix} \dot{q} \\ \dot{p} \end{pmatrix} =\bm{v} =\begin{pmatrix} F(q,p) \\ G(q,p) \end{pmatrix}$$ is \textbf{Hamiltonian} if there exists a function $H(q,p)$ such that $$F=\pdv{H}{p} \quad G=-\pdv{H}{q}.$$
\end{definition}

\begin{corollary}\label{cor:Hamiltonian test}
If a second order autonomous dynamical system is Hamiltonian then $$\pdv{F}{q}+\pdv{G}{p}=\pdv{}{q}\pdv{H}{p}-\pdv{}{p}\pdv{H}{q}=0.$$
\end{corollary}

\begin{remark}
If $\bm{v}=(F,G)$ then the equation in Corollary \ref{cor:Hamiltonian test} can be written as $\bm{\nabla \cdot v}=0.$
\end{remark}

\subsubsection{How to find the Hamiltonian function}

Consider the system $$\begin{pmatrix} \dot{q} \\ \dot{p} \end{pmatrix} =\begin{pmatrix} F(q,p) \\ G(q,p) \end{pmatrix}$$

If the system is indeed Hamiltonian to find the function $H(q,p)$ we need to evaluate the line integral $$H(Q,P)=\int_{(0,0)}^{(Q,P)} F \, dp -G \, dq.$$ It is simpler to split the integral in two to find the Hamiltonian \begin{tcolorbox}
$$H(Q,P)=\int_0^Q -G(q,0) \, dq +\int_0^P F(Q,p) \, dp.$$
\end{tcolorbox}

\begin{example}
Show that the following system is Hamiltonian and find a suitable Hamiltonian function $H.$ Consider the system $$\begin{pmatrix} \dot{q} \\ \dot{p} \end{pmatrix} =\begin{pmatrix} p-q^2 \\ 2qp \end{pmatrix}.$$ We have $$\pdv{(p-q^2)}{q}+\pdv{(2qp)}{p}=0.$$ So, the system is indeed Hamiltonian. We have that $$\begin{aligned}
H(Q,P)	&=\int_0^Q -G(q,0) \, dq +\int_0^P F(Q,p) \, dp \\
		&=\int_0^Q 2q(0) \, dq + \int_0^P p-Q^2 \, dp \\
		&=\int_0^P p-Q^2 \, dp \\
		&=\left[\f{1}{2}p^2 -Q^2p\right]_0^P \\
		&=\f{1}{2}P^2 -Q^2P.
\end{aligned}$$ So our Hamiltonian is $$H(q,p)=\f{1}{2}p^2-q^2p.$$
\end{example}

\subsection{Solving Hamilton's equation}

If the Hamiltonian is given as $H(q,p)=\f{p^2}{2m}+V(q)$ then $\dot{q}=p$ and $\dot{p}=V'(q).$ Recall that $H(q,p)=E$ so we can rearrange to find $p,$ which is $\pm\sqrt{2(E-V(q))}.$ So the equation of motion for the phase curves is $$\dot{q}=p=\pm\sqrt{2(E-V(q))}.$$ To find the equation of motion of the \textbf{separatrix} we need to substitute the coordinates of the hyperbolic fixed point to find its correspondent energy.

\subsection{Stability}

A Hamiltonian system is a second order autonomous dynamical system so, skills from the previous sections van be used to analyse the system.

Recall that fixed points are defined by $\dot{q}=\dot{p}=0.$ Using the Taylor expansion on the system on a Hamiltonian system we have: $$\begin{aligned}
\dot{q} &=\pdv{H}{p}+q\pdv[2]{H}{q}{p}+p\pdv[2]{H}{p}+\ldots \\
\dot{p} &=-\pdv{H}{q}-q\pdv[2]{H}{q}-p\pdv[2]{p}{q} + \ldots
\end{aligned}$$ evaluate at fixed point $(q,p)$. So, we have the Jacobian $$J=\begin{pmatrix} \pdv[2]{H}{q}{p} & \pdv[2]{H}{p} \\ -\pdv[2]{H}{q} & -\pdv[2]{H}{p}{q} \end{pmatrix} = \begin{pmatrix} H_{qp} &H_{pp} \\ -H_{qq} & -H_{pq} \end{pmatrix},$$ where $H_{pq} =\pdv[2]{H}{p}{q}$ etcetera. The nature of the fixed points is determined by the eigenvalues of the Jacobian.

The three possibilities: 

\begin{enumerate}
	
	\item[(i)] $t=\Delta=0.$ The eigenvalues of $J$ are zero and we have learned nothing from the linear analysis.

	\item[(ii)] $t=0,\Delta<0.$ The eigenvalues are real and of opposite sign. The fixed point is hyperbolic and is structurally stable.

	\item[(iii)] $t=0, \Delta>0.$ The eigenvalues are imaginary. The fixed point is elliptic. (This is the case because we have more information about the system.)


\end{enumerate}

\begin{remark}
The notation for partial derivatives: $f_{xy}= \pdv{}{y}\left(\pdv{f}{x}\right).$
\end{remark}

\subsection{Motion in a potential}

The Hamiltonian and Hamilton's equations are simply $$H=\f{p^2}{2m}+V(q), \quad \dot{q}=\f{p}{m}, \quad \dot{p}=-V'.$$ A fixed point corresponds to $\dot{q}=\dot{p}=0,$ i.e. $p=0$ and $V'=0,$that is a stationary point of the potential. In this case the matrix $J$ is $$J=\begin{pmatrix} 0 & \f{1}{m} \\ -V'' & 0 \end{pmatrix},$$ and the eigenvalues of $J$ are $\pm\sqrt{\f{-V''}{m}}.$

Once again, there are three cases:
\begin{enumerate}
	
	\item[(i)] $V''=0,$ this tells us nothing.

	\item[(ii)] $V''<0,$  the eigenvalues are real and of opposite sign, this is a hyperbolic fixed point.

	\item[(iii)] $V''>0,$ the eigenvalues are imaginary, this is an elliptic fixed point.

\end{enumerate}

\subsubsection{Phase portraits for motion in a potential}

TO DO !!

\subsubsection{Motion along the separatrix}

An orbit that passes through a hyperbolic fixed point is called a \textbf{separatrix}. We can analyse motion in a spearatrix given that at the hyperbolic fixed point $x^*$ we have, $H(x^*,0)=E.$ Rearranging the equation we can obtain an equation of motion for $\dot{q}.$

\subsubsection{Motion along other orbits}

\subsection{Summary: how to analyse motion in a potential}

\includepdf[pages=-]{./Resources/Summary of analysis in a potential.pdf}

\section{Appendix}

\subsection{Solutions to memorise for ODEs}

(From Skills 9) This is an exhaustive list of ODEs (and their solutions) which you are expected to be able to write without working out the solution explicitly.
\begin{itemize}

	\item If $\dot{x} = \lambda x \imply x=Ce^{\lambda t};$

	\item if $\ddot{x}=0 \imply x=x_0+v_0t;$

	\item if $\ddot{x}+\omega^2x=0 \imply x=C_1\cos{\omega t}+C_2\sin{\omega t}.$

\end{itemize}

\subsection{Notation for fixed points}

In a first order DS fixed points are expressed as $x=a;$ similarly points of equilibrium in a potential are expressed as $x=a$ if $V'(a)=0.$ For second order DS fixed points are expressed as a coordinate pair $(x,y)=(a,b).$

\end{document}		