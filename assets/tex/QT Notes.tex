\documentclass[12pt, a4paper]{article}
\usepackage{francesco}
\usepackage{physics}
\usepackage[colorlinks=true,
            urlcolor=RubineRed,
            linktoc=all,
            linkcolor=black,
            pdfauthor={Francesco N. Chotuck},
            pdftitle={Introductory Quantum Theory Notes}
            ]{hyperref}
%\usepackage[none]{hyphenat}

\pagestyle{fancy}
\lhead{Francesco Chotuck}
\rhead{5CCM234A Introductory Quantum Theory}
\setlength{\headheight}{15pt}

\title{Introductory Quantum Theory Notes}
\date{}
\author{Francesco Chotuck}
\begin{document}
\maketitle

\begin{abstract}
    This is KCL undergraduate module 5CCM234A, instructed by Dr Petr Kravchuk. The formal name for this class is ``Introductory Quantum Theory''.
\end{abstract}

\tableofcontents

\pagebreak

\section{The wave function}

\subsection{The Schrödinger equation}

\begin{definition}
    The `position' of a particle is determined by the complex-valued function \(\Psi(x,t)\), called the \textbf{wave function}.
\end{definition}

\begin{mdnote}
    We can think of \(\Psi(x,t)\) as the analogous to the position of a particle in classical dynamics given by \(x(t)\).
\end{mdnote}

\begin{theorem}
    Since \(\Psi \in \CC\) we can write \(\abs{\Psi}^2 = \Psi^*\Psi\) where \(\Psi^*\) is the complex conjugate of \(\Psi\).
\end{theorem}

\begin{definition}
    We define a fundamental quantity 
    \[\hbar = \frac{h}{2 \pi}\]
    where \(h\) is the Planck's constant. It has units \(\unit{\joule\second}\)
\end{definition}

\begin{mdthm}
    We obtain the wave function from solving the (time-dependent) \textbf{Schrödinger equation}:
    \[i \hbar \diffp{}{t} \Psi(x,t) = - \frac{\hbar^2}{2m} \diffp[2]{}{x}\Psi(x,t) + U(x)\Psi(x,t),\]
    where \(U(x)\) is the potential energy function of the particle.
\end{mdthm}

\begin{mdremark}
    The equation above is for a particle moving in one direction (the \(x\)-axis) but this holds for a particle moving in \(\RR^3\) by changing \(x \mapsto \bm{x}\) (or sometimes written \(\vec{x}\)).
\end{mdremark}

\begin{mdnote}
    Recall the total energy of a particle in motion in a dynamical system is given by 
    \[E = \frac{p^2}{2m} +U(x)\]
    Therefore, we can think of the Schrödinger equation as describing the total energy of the system: 
    \[\underbrace{i \hbar \diffp{}{t} \Psi(x,t)}_{E\Psi} = \underbrace{- \frac{\hbar^2}{2m} \diffp[2]{}{x}\Psi(x,t)}_{\frac{p^2}{2m} \Psi} + U(x)\Psi(x,t),\]
\end{mdnote}

\begin{mdremark}
    If the particle in motion does not have any potential energy the Schrödinger equation can be adjusted by removing the `potential energy' term i.e. the \(U(x)\) term.
\end{mdremark}

\subsection{Statistical interpretation of the wave function}

\begin{mdthm}
    The \textbf{probability density} of a wave function is given by
    \[\abs{\Psi(x,t)}^2 \, dx = \{\text{probability of finding the particle between } x \text{ and } (x+dx), \text{ at time } t\}\]
\end{mdthm}

\begin{figure}[H]
     \begin{center}
         \includegraphics[width=\textwidth]{./Resources/Wave function probability.png}
     \end{center}
\end{figure}

Suppose the graph a wave function is as the one above. The particle would be relatively likely to be found near \(A\), and unlikely to be found near \(B\). The shaded area represents the probability of finding the particle in the range \(dx\).

\subsubsection{Normalisation}

\begin{mdthm}
    \[\int_{-\infty}^{\infty} \abs{\Psi(x,t)}^2 \, = 1.\]
\end{mdthm}

\begin{mdremark}
    This is a required condition otherwise all the statistical interpretation of the wave function would be nonsense.
\end{mdremark}

\begin{corollary}
    The value 
    \[\int_a^b \abs{\Psi(x,t)}^2 \, dx = \{\text{probability of finding the particle between } a \text{ and } b, \text{ at time } t\}\]
\end{corollary}

\begin{definition}
    The process of altering the wave function such that it satisfies the theorem above is called \textbf{normalisation} of the wave function.
\end{definition}

\begin{mdremark}
    If \(\Psi\) is a solution to the Schrödinger equation then so is \(A\Psi\) where \(A \in \CC\) is a scalar. Thus, to normalise a wave function we multiply by a complex scalar which makes the integral equal to \(1\).
\end{mdremark}

\begin{definition}
    Solution to the Schrödinger equation with integral of \(\abs{\Psi}^2\)  is infinite or when \(\Psi = 0\) are said to be \textbf{non-normalisable} solutions.
\end{definition}

\begin{mdnote}
    Physically these solutions cannot exist so, they must be rejected.
\end{mdnote}

\begin{mdthm}
    If the integral 
    \[\int_{-\infty}^{\infty} \abs{\Psi(x,t)}^2 \, dx\]
    is finite then the wave function \(\Psi\) can be normalised.
\end{mdthm}

\subsection{Operators and expected values}

\begin{definition}
    A linear operator \(\wh{\mathcal{O}}\) is a rule that takes a wave function \(\Psi\) and produces a new wave function that we denote \(\wh{\mathcal{O}}\Psi\). Linearity means that it satisfies the property 
    \[\wh{\mathcal{O}}\left( \alpha \Psi_1 +\beta \Psi_2 \right) = \alpha \wh{\mathcal{O}}\Psi_1 + \beta\wh{\mathcal{O}}\Psi_2.\]
\end{definition}

\begin{mdremark}
    Linear operators in this module are defined \(\wh{\mathcal{O}}: V \to V\) where \(V\) is the vector space of complex-valued functions.
\end{mdremark}

\begin{theorem}
    The notation \(\wh{\mathcal{O}}_1 \wh{\mathcal{O}}_2\) means composition of linear operators i.e. \((\wh{\mathcal{O}}_1 \wh{\mathcal{O}}_2)(x) = \wh{\mathcal{O}}_1(\wh{\mathcal{O}}_2(x))\).
\end{theorem}

\begin{definition}
    The linear \textbf{operator of position}, \(\wh{x}\), for a fixed time \(t\) is defined as 
    \[(\wh{x} \, \Psi)(x) = x\Psi(x).\]
\end{definition}

\begin{mdremark}
    This is terrible notation. The \underline{operator}, \(\wh{x}\), is acting on the function \(\Psi\) and yields a \underline{new function} \((\wh{x} \, \Psi)\). We can think of \((\wh{x} \, \Psi)\) as a function \(f\). Therefore, \(f\) acting on \(x\), written \(f(x)\), in this notation is written as \((\wh{x} \, \Psi)(x)\).
\end{mdremark}

\begin{mdnote}
    Think of it as \(\wh{\mathcal{O}}(\text{function}) = \text{function}\).
\end{mdnote}

\begin{definition}
    The linear \textbf{operator of momentum}, \(\wh{p}\), for a fixed time \(t\) is defined as 
    \[(\wh{p} \, \Psi)(x) = - i\hbar \diffp{}{x} \Psi(x).\]
\end{definition}

\begin{mdthm}
    If \(\Psi\) is normalised then the expectation value of a general linear operator \(\wh{\mathcal{O}}\), at a fixed time \(t\) is given by 
    \[\langle \wh{\mathcal{O}} \rangle = \int_{-\infty}^{\infty} \Psi^*(x) (\wh{\mathcal{O}}\Psi)(x) \, dx.\]
\end{mdthm}

\begin{definition}
    For a particle in a normalised state \(\Psi\), the \textbf{expectation value of the position}, \(x\), for a fixed time \(t\) is 
    \[\begin{aligned}
        \langle \wh{x} \rangle &= \int_{-\infty}^{\infty} x \abs{\Psi(x)}^2 \, dx \\
        &= \int_{-\infty}^{\infty} \Psi^*(x) \,  x \,\Psi(x) \, dx.
    \end{aligned}\]
\end{definition}

\begin{definition}
    For a particle in a normalised state \(\Psi\), the \textbf{expectation value the momentum}, \(p\), for a fixed time \(t\) is 
    \[\begin{aligned}
        \langle \wh{p} \rangle &= \int_{-\infty}^{\infty} \Psi^*(x)\left( - i \hbar \diffp{}{x}\Psi(x) \right) \, dx \\
       &= -i \hbar \int_{-\infty}^{\infty} \Psi^*(x) \left( \diffp{}{x}\Psi(x) \right) \, dx.
    \end{aligned}\]
\end{definition}

\begin{mdthm}
    For a general operator \(\wh{\mathcal{O}}\) acting on \(\wh{x}\) we have
    \[(\wh{\mathcal{O}}(\wh{x}) \Psi)(y) = \wh{\mathcal{O}}(y)\Psi(y).\]
\end{mdthm}

\subsection{The Hamiltonian operator}

\begin{definition}
    The \textbf{Hamiltonian operator} is defined as 
    \[\wh{H} = \frac{\wh{p}^2}{2m} + U(\wh{x}),\]
    where \(U(\wh{x})\) is defined as 
    \[(U(\wh{x})\Psi) (y) = U(y)\Psi(y).\]
\end{definition}

\begin{mdthm}
    The Schrödinger equation with the Hamiltonian operator can be written as 
    \[i \hbar \diffp{}{t}\Psi(x,t) = \wh{H} \Psi(x,t).\]
\end{mdthm}

\begin{definition}
    An operator \(\wh{\mathcal{O}}\) for which 
    \[\int_{-\infty}^{\infty} \Psi^*(x)(\wh{\mathcal{O}} \Phi)(x) \, dx= \int_{-\infty}^{\infty} \left[ (\wh{\mathcal{O}} \Psi) (x) \right]^* \Phi(x) \, dx,\]
    for any \(\Phi\) and \(\Psi\) is said to be \textbf{Hermitian}.
\end{definition}

\begin{mdthm}
    The Hamiltonian operator is Hermitian.
\end{mdthm}

\begin{proof}
    We will be using the following facts for this proof:
    \begin{enumerate}
        \item The sum of Hermitian operators is Hermitian.
        \item The product of Hermitian operators is Hermitian.
        \item A Hermitian operator multiplied by a scalar \(\alpha \in \RR\) is Hermitian.
    \end{enumerate}
    Using these facts, we have to prove that \(\wh{p}\) and \(U(\wh{x})\) are Hermitian operators.
    \begin{itemize}
        \item Proof of \(U(\wh{x})\) being Hermitian. \\
        Suppose \(U(\wh{x})\) is Hermitian then 
        \[\int_{-\infty}^{\infty} \Psi^*(x) [U(x)\Phi(x)] \, dx = \int_{-\infty}^{\infty} [U(x)\Psi(x)]^* \Phi(x) \, dx.\]
        This can only happen if \(U(x) = U^*(x)\) that is, \(U(x) \in \RR\). In this course the potential is always real.
        \item Proof of \(\wh{p}\) being Hermitian. \\
        Suppose \(\wh{p}\) is Hermitian, we check that the condition holds.
        \[\begin{aligned}
            \int_{-\infty}^{\infty} [\wh{p} \, \Psi(x)]^* \Phi(x) &= \int_{-\infty}^{\infty} \left( -i \hbar \diffp{}{x} \Psi(x) \right)^* \Phi(x) \\
            &= i \hbar \int_{-\infty}^{\infty} \left( \diffp{}{x} \Psi^*(x) \right) \Phi(x) \\
            &= - i \hbar \int_{-\infty}^{\infty} \Psi^*(x) \diffp{}{x} \Phi(x) \quad \text{(integration by parts)}\\
            &= \int_{-\infty}^{\infty} \Psi^*(x) \,\wh{p} \, \Phi(x).
        \end{aligned}\]
    \end{itemize}
\end{proof}

\subsection{Conservation of probability}

\begin{mdthm}
    The Schrödinger equation preserves the normalisation of the wave function.
\end{mdthm}

\begin{mdnote}
    Suppose we have normalised a function at \(t=0\), as time goes on and \(\Psi\) evolves we need to ensure that \(\Psi\) remains normalised; luckily the Schrödinger equation has the property illustrated above
\end{mdnote}

\begin{proof}
    If the integral \(\int_{-\infty}^{\infty} \abs{\Psi(x,t)}^2 =1\) at one moment in time then it is \(1\) for all other \(t\). Thus, we want to prove that 
    \[\diff{}{t}\int_{-\infty}^{\infty} \abs{\Psi(x,t)}^2 =0.\]
    We have that 
    \[\begin{aligned}
        \diff{}{t}\int_{-\infty}^{\infty} \abs{\Psi(x,t)}^2 \, dx &= \int_{-\infty}^{\infty} \diffp{}{t} \Psi(x,t)\Psi^*(x,t) \, dx\\
        &= \int_{-\infty}^{\infty} \Psi^*(x,t)\diffp{}{t}\Psi(x,t) \, dx + \int_{-\infty}^{\infty} \Psi(x,t)\diffp{}{t}\Psi^*(x,t) \, dx \\
        &= \int_{-\infty}^{\infty} \Psi^*(x,t) \frac{1}{i \hbar} (\wh{H}\Psi)(x,t) \, dx - \int_{-\infty}^{\infty} \frac{1}{i \hbar} [(\wh{H}\Psi)(x,t)]^* \Psi(x,t) \, dx \\
        &= \frac{1}{i \hbar} \left[ \int_{-\infty}^{\infty} \Psi^*(x,t)(\wh{H}\Psi)(x,t) \, dx -  \int_{-\infty}^{\infty} [(\wh{H}\Psi)(x,t)]^* \Psi(x,t) \, dx  \right] \\
        &= 0.
    \end{aligned}\]
    Since \(\wh{H}\) is Hermitian the last two integrals are equal and hence, cancel each other out.
\end{proof}

\subsection{Evolution in time of expectation values}

\begin{definition}
    A \textbf{commutator} is defined as 
    \[[\wh{A}, \wh{B}] = \wh{A}\wh{B} - \wh{B}\wh{A}.\]
\end{definition}

\begin{theorem}
    Properties of commutator. For any \(\alpha, \beta \in \CC\) and any operator we have:
    \begin{itemize}
        \item \(\left[ \alpha \wh{A}+\beta\wh{B} , \wh{C}\right] = \alpha \left[ \wh{A},\wh{C} \right]+\beta \left[ \wh{B} , \wh{C}\right]\).
        \item \(\left[ \wh{A},\alpha\wh{B}+\beta\wh{C} \right] = \alpha\left[ \wh{A},\wh{B} \right]+\beta\left[\wh{A},\wh{C} \right]\).
        \item \(\left[ \wh{A},\wh{B} \right] = -\left[ \wh{B},\wh{A} \right]\).
    \end{itemize}
\end{theorem}

\begin{mdnote}
    The first two properties imply that the commutator is bilinear.
\end{mdnote}

\begin{mdthm}
    For any observable \(\wh{\mathcal{O}}\), the time derivative of its expectation value is given by 
    \[\diffp{}{t} \langle \wh{\mathcal{O}} \rangle = \frac{i}{\hbar} \left\langle \left[ \wh{H}, \wh{\mathcal{O}} \right] \right\rangle,\]
    where \(\wh{H}\) is the Hamiltonian operator.
\end{mdthm}

\begin{mdremark}
    An equivalent restatement is 
    \[\diffp{}{t} \langle \wh{\mathcal{O}} \rangle = \frac{1}{i \hbar} \left\langle \left[\wh{\mathcal{O}} ,\wh{H} \right] \right\rangle.\]
\end{mdremark}

\begin{proof}
    Recall, 
    \[\langle \wh{\mathcal{O}} \rangle = \int_{-\infty}^{\infty} \Psi^*(x) (\wh{\mathcal{O}}\Psi)(x).\]
    Taking its time derivative we have,
    \[\begin{aligned}
        \diffp{}{t} \langle \wh{\mathcal{O}} \rangle &= \int_{-\infty}^{\infty} \diffp{}{t} \left[ \Psi^*(x) (\wh{\mathcal{O}}\Psi)(x) \right] \, dx \\
        &= \int_{-\infty}^{\infty} \left[ \diffp{}{t} \Psi(x) \right]^* (\wh{\mathcal{O}} \Psi)(x) \, dx + \int_{-\infty}^{\infty} \Psi^*(x) \left( \wh{\mathcal{O}} \diffp{}{t}\Psi \right)(x) \, dx \\
        &= \frac{1}{\hbar} \left( \int_{-\infty}^{\infty} \left[ \wh{H} \Psi(x) \right]^* (\wh{\mathcal{O}} \Psi)(x)\, dx - \int_{-\infty}^{\infty} \Psi^*(x) (\wh{\mathcal{O}}\wh{H} \Psi)(x)\right) \, dx.
    \end{aligned}\]
    Since \(\wh{H}\) is Hermitian we can write (with \(\Psi = \Psi\) and \(\Phi=\wh{\mathcal{O}}\Psi\)),
    \[\int_{-\infty}^{\infty} \left[ \wh{H}\Psi(x) \right]^* \wh{\mathcal{O}} \Psi(x) \, dx = \int_{-\infty}^{\infty} \Psi^*(x) (\wh{H}\wh{\mathcal{O}}\Psi)(x) \, dx.\]
    Plugging this into the above we obtain 
    \[\begin{aligned}
        \diffp{}{t} \langle \wh{\mathcal{O}} \rangle &= \frac{i}{\hbar} \left( \int_{-\infty}^{\infty} \Psi^*(x) \left(\wh{H}\wh{\mathcal{O}} \right) \Psi(x) \, dx - \int_{-\infty}^{\infty} \Psi^*(x) \left(\wh{\mathcal{O}}\wh{H}\right)\Psi(x) \right) \\
        &= \frac{i}{\hbar} \left( \int_{-\infty}^{\infty} \Psi^*(x)(\wh{H}\wh{\mathcal{O}}-\wh{\mathcal{O}}\wh{H})\Psi(x) \right) \, dx \\
        &= \frac{i}{\hbar} \left\langle \wh{H}\wh{\mathcal{O}}-\wh{\mathcal{O}}\wh{H} \right\rangle  \\
        &= \frac{i}{\hbar} \left[ \wh{H},\wh{\mathcal{O}} \right].
    \end{aligned}\]
\end{proof}

\begin{mdthm}[Ehrenfest theorem]
    We have the following relation
    \[\begin{aligned}
        \diffp{}{t} \left\langle \wh{x} \right\rangle &=  \frac{\left\langle \wh{p} \right\rangle}{m}, \\
        \diffp{}{t} \left\langle \wh{p} \right\rangle &= - \left\langle \diffp{}{x}U(\wh{x}) \right\rangle.
    \end{aligned}\]
\end{mdthm}

\begin{mdremark}
    Consider the Classical Dynamics form of the statement above:
    \[\begin{aligned}
        \diffp{}{t} x(t) &= \frac{p(t)}{m} \\
        \diffp{}{t} p(t) &= -\diffp{}{t} U(x(t)).
    \end{aligned}\]
    The Quantum and Classical statements are equivalent if \(p(t) = \langle \wh{p} \rangle\) and \(x(t) = \langle \wh{x} \rangle\). This is clearly not the case: in Quantum we have 
    \[\diffp{}{t} \langle \wh{p} \rangle = \langle F(\wh{x}) \rangle ,\]
    whereas, in Classical we have
    \[\diffp{}{t} \langle \wh{p} \rangle =  F(\langle \wh{x}\rangle).\]
    Therefore, \(\langle F(\wh{x}) \rangle \neq F(\langle \wh{x} \rangle)\) always; in only certain cases the equality is achieved.
\end{mdremark}

\section{Simple quantum mechanical systems}

\subsection{Separation of variables}

\begin{definition}
    Solutions to the Schrödinger equation which are of the form 
    \[\Psi(x,t) = \psi(x)\Phi(t)\]
    are called \textbf{separable solutions}.
\end{definition}

Recall the Schrödinger equation 
\[i \hbar \diffp{}{t}\Psi(x,t) = \wh{H} \Psi(x,t).\]
Assuming \(\Psi(x,t)\) is separable we substitute to obtain 
\[\begin{aligned}
    i\hbar \psi(x) \diffp{\Phi(t)}{t} &= \Phi(t) (\wh{H}\psi)(x) \\
    \underbrace{\frac{i \hbar}{\Phi(t)} \diffp{\Phi(t)}{t}}_{t \text{ dependence}} &= \underbrace{\frac{(\wh{H}\psi)(x)}{\psi(x)}}_{x \text{ dependence}}.
\end{aligned}\]

Since, the LHS is only dependent on \(t\) and the RHS is only dependent on \(x\) we must have that both sides are equal to a constant -- otherwise by varying \(t\), we could change the LHS without touching the RHS and the two would no longer be equal. So, we set 
\[\frac{i \hbar}{\Phi(t)} \diffp{\Phi(t)}{t} = \frac{(\wh{H}\psi)(x)}{\psi(x)} = E \in \RR,\]
i.e. the energy.

Analysing the LHS:
\[\begin{aligned}
    i \hbar \diffp{\Phi(t)}{t} &= E \Phi(t) \\
    \then \diffp{\Phi(t)}{t} &= -\frac{iE}{\hbar} \Phi(t) \\
    \then  \Phi(t) &= A \exp\left( -\frac{iE}{\hbar} t\right) \\
    &= Ae^{-\frac{iE}{\hbar} t}.
\end{aligned}\]

\begin{mdremark}
    In some texts the partial derivatives become total derivatives as the wave function \(\Psi(x,t)\) is no longer multivariable. This allows us to treat the PDEs as ODEs and solve them with techniques we learnt previously.
\end{mdremark}

Analysing the RHS we have the following theorem.

\begin{mdthm}
    The \textbf{time-independent} Schrödinger equation is given by 
    \[- \frac{\hbar^2}{2m} \diffp[2]{}{x}\psi(x)+U(x)\psi(x)=\psi(x)E.\]
    Equivalently, it can be written as 
    \[(\wh{H}\psi)(x)=E\psi(x).\]
\end{mdthm}

\begin{mdnote}
    Due to separation of variables the partial derivative above can be written as a total derivative and use standard techniques to solve a second order ODE.
\end{mdnote}

The time-dependent Schrödinger equation can be solved for a given potential thus, leading to the following.

\begin{mdcor}
    The general solution of the Schrödinger equation is given by 
    \[\Psi(x,t) = \psi(x) e^{ -\frac{iE}{\hbar} t}.\]
\end{mdcor}

\begin{mdremark}
    The constant \(A\) from \(\Phi(t)\) is incorporated into the normalisation constant of \(\Psi(x,t)\).
\end{mdremark}

\subsection{Particle in a box}

\begin{definition}
    Suppose 
    \[U(x) = \begin{cases}
        0 &\text{if } 0\leq x \leq L \\
        \infty &\text{otherwise}.
    \end{cases}\]
    We call this potential the \textbf{infinite square well} or a \textbf{particle moving a box}.
\end{definition}

\begin{figure}[H]
    \centering
    \resizebox{12cm}{!}{%
        \begin{tikzpicture}

            \filldraw[fill=gray!35, draw=white] (-1,0) rectangle (1,4);
            \filldraw[fill=gray!35, draw=white] (4,0) rectangle (6,4);
            
            \node at (0,2) {$\begin{aligned}
            U(x) &= \infty \\ \Psi(x) &=0
            \end{aligned}$};
            
            \node at (5,2) {$\begin{aligned}
            U(x) &= \infty \\ \Psi(x) &=0
            \end{aligned}$};
            
            \node at (2.5,2) {$U(x) =0$};
            
            \draw[-Stealth,thick] (-1,0) -- (6,0) node[right]{$x$};
            
            \draw[dashed,red,thick] (1,0) -- (1,4);
            
            \node at (1,-0.25){$0$};
            
            \draw[dashed,red,thick] (4,0) -- (4,4);
            
            \node at (4,-0.25){$L$};
            
        \end{tikzpicture}}
        \caption{Figure of the infinite square well}
\end{figure}

Since \(U(x)=0\) inside the `box' the Schrödinger equation becomes
\[-\frac{\hbar^2}{2m} \diffp[2]{\Psi(x)}{x} = E\Psi(x),\]
this takes the form of \textbf{simple harmonic motion} thus, the solution is given by 
\[\Psi(x) = A \cos(kx)+B\sin(kx) \quad \text{where } k \equiv \frac{\sqrt{2mE}}{\hbar},\]
where \(A\) and \(B\) are arbitrary constants. We assume \(E >0\) (since \(E<0\) does not work if tried) which forces \(k>0\). Continuity of the potential function requires that 
\[\Psi(0) = \Psi(L) = 0,\]
which allows us to evaluate the arbitrary constants.
\[\begin{aligned}
    \Psi(0) &= 0 \quad \then A =0 \\
    \Psi(L) &= 0 \quad \then B\sin(kL)=0.
\end{aligned}\]
The second equation implies \(B = 0\) or \(\sin(kL)=0\); if \(B=0\) then \(\Psi(x)=0\) which is not normalisable hence cannot be used. Therefore, \(\sin(kL) = 0\) which means that 
\[kL = n \pi \quad \text{for } n \in \ZZ.\]
However, \(k=0\) would again imply \(\Psi(x)=0\) and the negative solutions give nothing new, since \(\sin(-\theta)=-\sin(\theta)\) and we can absorb the minus sign into \(B\). Hence, \(n \in \ZZ_{\geq 0}\) and the distinct solutions are 
\[k_n = \frac{\pi n}{L} \quad \text{for } n \in \ZZ_{\geq 0}.\]
Since, \(k_n\) depends on \(E\) we can work out the possible values of \(E\):
\[\boxed{E_n = \frac{\pi^2 n^2 \hbar^2}{2mL^2}.}\]

\begin{mdnote}
    The existence of \(E_n\) implies that `quantum' particles have discrete energy levels.
\end{mdnote}

Now, we need to find \(B\), we do this by normalising the wave function \(\Psi = B \sin(kx)\):
\[\begin{aligned}
    1= \int_{-\infty}^{\infty} \abs{\Psi(x)}^2 \, dx &= \int_{-\infty}^{\infty} \abs{B}^2 \sin^2(kx) \, dx \\
    &= \int_{-\infty}^{0} 0 \, dx + \int_{0}^{L} \abs{B}^2 \sin^2(kx) \, dx + \int_{L}^{\infty} 0 \, dx \\
    &= \int_{0}^{L} \abs{B}^2 \sin^2(kx) \, dx \\
    &= \abs{B}^2 \frac{L}{2}.
\end{aligned}\]
We conclude \(B = \sqrt{\frac{2}{L}}\). The wave function for a particle under the influence of this potential is given by 
\[\begin{aligned}
    \Aboxed{\Psi_n(x,t) &= \sqrt{\frac{2}{L}} \sin\left( \frac{\pi n}{L}x \right) \exp\left( \frac{-i E_n t}{\hbar} \right)} \\
    &= \sqrt{\frac{2}{L}} \sin\left( \frac{\pi n}{L}x \right) e^{-i\frac{E_n}{\hbar}t}.
\end{aligned}\]

\begin{mdremark}
    Since \(B \in \CC\) we can be \(B = \sqrt{\frac{2}{L}} e^{i\theta}\), which we call the phase of the wave function, but we set this to \(1\).
\end{mdremark}

\subsubsection{Superposition of states}

\begin{definition}
    \textbf{Superposition} is the term physicists use for a linear combination or the continuous analogue thereof, namely an integral.
\end{definition}

\begin{mdremark}
    This is due to the properties of the Schrödinger equation being a differential equation: any linear combination of some solutions is also a solution.
\end{mdremark}

\subsection{Potential well}

\begin{definition}
    Suppose 
    \[U(x) = \begin{cases}
        -U_0 &\text{if } -\frac{L}{2} \leq x \leq \frac{L}{2} \\
        0 &\text{if } \abs{x} > \frac{L}{2},
    \end{cases}\]
    where \(U_0>0\). We call this potential the \textbf{finite square well} or the \textbf{potential well}.
\end{definition}

\begin{figure}[H]
     \begin{center}
        \resizebox{12cm}{!}{%
        \begin{tikzpicture}

            \draw[-Stealth, thick] (-3,0) -- (3,0) node[right]{$x$};
            
            \draw[-Stealth, thick] (0,-3) -- (0,2) node[right]{$U(x)$};
            
            \draw[thick,red] (-1,0) -- (-1,-2);
            
            \draw[thick,red] (1,0) -- (1,-2);
            
            \draw[thick,red] (-1,-2) -- (1,-2);
            
            \draw[thick,red] (1,0) -- (2.8,0);
            
            \draw[thick,red] (-1,0) -- (-2.8,0);
            
            \node at (0.4,-2.3){$-U_0$};
            
            \node at (-1.1,0.3){$-\frac{L}{2}$};
            
            \node at (1,0.3){$\frac{L}{2}$};
            
            \node[gray] at (-2,-1){Region $1$};
            
            \node[gray] at (-0,-3.3){Region $2$};
            
            \node[gray] at (2,-1){Region $3$};
            
        \end{tikzpicture}}
     \end{center}
     \caption{A diagram of the potential well}
\end{figure}

We split the potential well into three regions as shown by the Figure above. The Schrödinger equation for Region \(1\) and Region \(3\) is the time-independent equation with no potential term i.e.
\[-\frac{\hbar^2}{2m} \diffp[2]{\Psi(x)}{x} = E \Psi(x),\]
where \(E<0\). Therefore, 
\[\begin{aligned}
    \text{Region } 1: \quad \Psi(x) &= A_1e^{kx}+B_1e^{-kx} \\
    \text{Region } 3: \quad \Psi(x) &= A_3e^{kx}+B_3e^{-kx}
\end{aligned}\]
for 
\[k \equiv \frac{\sqrt{-2mE}}{\hbar}>0.\]

\begin{mdremark}
    We use Euler's formula to write the trigonometric functions in terms of exponentials.
\end{mdremark}

Since \(\Psi(x)\) must be normalisable to be an appropriate solution we have that the terms \(A_3 =0\) and \(B_1 =0\) otherwise, when taking the integral over from \(-\infty\) to \(\infty\) these terms would diverge. Region \(2\) is the only region with a potential thus, the Schrödinger equation becomes:
\[\begin{aligned}
    -\frac{\hbar}{2m} \diffp[2]{\Psi(x)}{x} -U_0\Psi(x) &= E\Psi(x) \\
    -\frac{\hbar}{2m} \diffp[2]{\Psi(x)}{x} &= (E+U_0)\Psi(x).
\end{aligned}\]

Although, \(E<0\) we must have \(E\geq U_0\) which implies, \(E+U_0\geq 0\). The general solution is 
\[\Psi(x) = A_2\cos(k_2 x)+B_2\sin(k_2 x)\]
where 
\[k_2 \equiv \frac{\sqrt{2m(E+U_0)}}{\hbar}.\]

The next step is to impose boundary conditions: \(\Psi\) and \(\diffp{\Psi(x)}{x}\) must be continuous at \(-\frac{L}{2}\) and \(\frac{L}{2}\). Note that the potential function is an even function (\(U(x)=U(-x)\)) i.e. symmetric about the line \(y=x\). This implies: if \(\Psi(x)\) is a solution of the Schrödinger equation, then \(\Psi(-x)\) is a solution. Furthermore, the Schrödinger equation is linear and homogeneous in \(\Psi\) i.e. we can add and subtract solutions thus, we define 
\[\begin{aligned}
    \Psi_+(x) &:= \Psi(x)+\Psi(-x) \\
    \Psi_-(x) &:= \Psi(x) -\Psi(-x),
\end{aligned}\]
which are also solutions. These new solutions have the property that the original solution can be recovered from them,
\[\Psi(x) = \frac{\Psi_+(x)+\Psi_-(x)}{2}\]
and they are even and odd respectively.
\[\Psi_{\pm}(x) = \pm \Psi(x).\]

\subsubsection*{The even case}

Suppose that the solution we are after, \(\Psi(x)\), is even. Using the solutions to Region \(1\) and Region \(2\) and applying continuity:

\[\begin{aligned}
    A_1 e^{-k \frac{L}{2}} &= A_2 \cos \left(\frac{k_2 L}{2} \right) +B_2 \sin \left( \frac{-k_2 L}{2} \right) \\
    A_1 k e^{-k \frac{L}{2}} &= A_2 k_2 \sin \left( \frac{k_2 L}{2} \right) +B_2 k_2 \cos \left( \frac{k_2 L_2}{2} \right).
\end{aligned}\]

Imposing that \(\Psi(x)\) is even we must have \(B_2 = 0\) so, the equations above become 
\[\begin{aligned}
    A_1 e^{-k \frac{L}{2}} &= A_2 \cos \left(\frac{k_2 L}{2} \right) \\
    A_1 k e^{-k \frac{L}{2}} &= A_2 k_2 \sin \left( \frac{k_2 L}{2} \right).
\end{aligned}\]

Taking the ration of these two equations, we find 

\[k=k_2 \tan\left( \frac{k_2 L}{2} \right).\]

Expressing \(k\) in terms of \(k_2\): recall that 
\[\begin{aligned}
    k = \sqrt{\frac{-2mE}{\hbar^2}} \quad &\text{and} \quad k_2 = \sqrt{\frac{2m(U_0+E)}{\hbar^2}} \\
    \then k &= \sqrt{\frac{2mU_0}{\hbar^2} -k_2^2}.
\end{aligned}\]

Since, \(E<0\) it implies that \(k \in \RR\) and that 
\[0 \leq k_2 \leq \sqrt{\frac{2mU_0}{\hbar^2}}.\]

Finally, we have an expression in \(k_2\) only,

\[\sqrt{\frac{2mU_0}{\hbar^2} - k_2^2} = k_2 \tan \left( \frac{k_2 L}{2} \right).\]

This equation allows us to determine \(k_2\) (and thus E) in terms of the parameters of the problem.

\subsubsection*{The odd case}

Suppose now that the solution, \(\Psi(x)\) is odd. Doing a similar process with the even case  we have the ratio

\[\sqrt{\frac{2mU_0}{\hbar^2} - k_2^2} = k_2 \tan \left( \frac{k_2 L + \pi}{2} \right).\]

\subsubsection*{Graphical representation}

\begin{figure}[H]
     \begin{center}
         \includegraphics{./Resources/Even and odd solutions.png}
         \caption{A graphical solution of the even and odd case. The LHS is plotted in solid black whereas, the RHS, if even, is plotted in solid red and if odd in dashed blue. The red and blue circle marks represent the solution to the even and odd case; the number of solutions depends on the parameters of the problem.}
     \end{center}
\end{figure}

\subsection{Quantum tunnelling and reflection}

We now study the solutions to the problem in the previous section with \(E>0\). We define \(V_0 =- U_0\) and imagine \(V_0\) being positive (thus, \(U_0\) being negative). 

\begin{definition}
    Suppose
    \[\begin{aligned}
        U(x) = \begin{cases}
            V_0 &\text{if }  -\frac{L}{2} \leq x \leq \frac{L}{2} \\
            0 &\text{if } \abs{x} > \frac{L}{2} 
        \end{cases}
    \end{aligned}\]
    where \(V_0 >0\). We call this potential the \textbf{potential barrier}.
\end{definition}

\begin{figure}[H]
     \begin{center}
        \resizebox{12cm}{!}{
            \begin{tikzpicture}

                \draw[-Stealth, thick] (-3,0) -- (3,0) node[right]{$x$};
                
                \draw[-Stealth, thick] (0,-1) -- (0,3) node[right]{$U(x)$};
                
                \draw[thick,red] (-1,0) -- (-1,2);
                
                \draw[thick,red] (1,0) -- (1,2);
                
                \draw[thick,red] (-1,2) -- (1,2);
                
                \draw[thick,red] (1,0) -- (2.8,0);
                
                \draw[thick,red] (-1,0) -- (-2.8,0);
                
                \node at (0.4,2.3){$V_0$};
                
                \node at (-1.1,-0.3){$-\frac{L}{2}$};
                
                \node at (1,-0.3){$\frac{L}{2}$};
                
                \node[gray] at (-2,1){Region $1$};
                
                \node[gray] at (0,3.5){Region $2$};
                
                \node[gray] at (2,1){Region $3$};
                
    \end{tikzpicture}
        }
     \end{center}
\end{figure}

We solve the time-independent Schrödinger equation for each region, and we have 

\[\begin{aligned}
    \Psi(x) = \begin{cases}
        A_{+} e^{\frac{ipx}{\hbar}} + A_{-}e^{-\frac{ipx}{\hbar}} &\text{for } x < - \frac{L}{2} \\
        B_{+} e^{\frac{ip_2 x}{\hbar}} + B_{-}e^{-\frac{ip_2 x}{\hbar}} &\text{for } - \frac{L}{2} \leq x \leq \frac{L}{2}\\
        C_{+} e^{\frac{ipx}{\hbar}} + C_{-}e^{-\frac{ipx}{\hbar}} &\text{for } x > \frac{L}{2},
    \end{cases}
\end{aligned}\]

where \(p\) represents momentum and is defined as \(p = \sqrt{2mE}\) whereas, \(p_2\) is determined by \(p_2 = \sqrt{2m(E-V_0)}\). Imposing the condition that \(\Psi(x)\) must be continuous we can evaluate some constants, so we have 
\[\begin{aligned}
    A_{-}(p) &= \text{Stuff}(p) \cdot A_{+} \\
    C_{+}(p) &= \text{Stuff}(p) \cdot A_{-}.
\end{aligned}\]

However, with these constants the solution \(\Psi(x)\) is not normalisable thus, we cannot fix \(A_{+}\) with the normalisable condition; \ul{we choose \(A_{+}=1\),} and we have 

\[\begin{aligned}
    \Aboxed{ A_{-}(p) &= \frac{p^2-p_2^2}{(p+p_2)^2} \cdot \frac{e^{\frac{-ipL}{\hbar}} - e^{\frac{i(2p_2 -p)L}{\hbar}}}{ 1 - \left( \frac{p-p_2}{p+p_2} \right)^2 e^{\frac{i2p_2 L}{\hbar}}}} \\
    \Aboxed{ C_{+}(p) &= \frac{4pp_2}{(p+p_2)^2} \cdot \frac{e^\frac{i(p_2-p)L}{\hbar}}{1 - \left( \frac{p-p_2}{p+p_2} \right)^2 e^{\frac{i2p_2 L}{\hbar}}}}
\end{aligned}\]

We have written \(A_{-}(p)\) and \(C_{+}(p)\) to stress that these coefficients depend on the value of \(p\) which is connected \(E\) via \(E = \frac{p^2}{2m}\).

In this thought experiment we want the particle coming from the left and travelling to the right. Analysing the solution of the Schrödinger equation:
\begin{itemize}
    \item the term \(A_{+} e^{\frac{ipx}{\hbar}}\) describes a particle moving to the right in Region \(1\), in the direction of the positive \(x\) axis towards the potential barrier in Region \(2\).
    \item The term \(A_{-} e^{-\frac{ipx}{\hbar}}\) describes a particle moving away from the potential barrier, we can think of this as a \textbf{reflected particle}.
    \item The term \(C_{+} e^{\frac{ipx}{\hbar}}\) describes a particle travelling away from the potential barrier, and we can think of it as the \textbf{transmitted particle}.
    \item The term \(C_{-} e^{-\frac{ipx}{\hbar}}\) describes a particle travelling from the right towards the potential barrier. As we set up the thought experiment for a particle coming from the left in Region \(1\) we set this term to \(0\) i.e. set \(C_{-} = 0\).
\end{itemize}

Therefore, the full time-independent solution for a given \(p\) (equivalently \(E\)) is 
\[\begin{aligned}
    \Psi_p(x,t) = e^{\frac{-i \frac{p^2}{2m} t }{\hbar}} \cdot \begin{cases}
        e^{\frac{ipx}{\hbar}} + A_{-}e^{-\frac{ipx}{\hbar}} &\text{for } x < - \frac{L}{2} \\
        B_{+} e^{\frac{ip_2 x}{\hbar}} + B_{-}e^{-\frac{ip_2 x}{\hbar}} &\text{for } - \frac{L}{2} \leq x \leq \frac{L}{2}\\
        C_{+} e^{\frac{ipx}{\hbar}}  &\text{for } x > \frac{L}{2}.
    \end{cases}
\end{aligned}\]

\subsubsection{Interpreting the scattering amplitudes}

In the `quantum realm' we deal with statistical interpretation of the happenings i.e. the state of the particle is not definite, until the particle is observed the particle is both transmitted and reflected by the quantum barrier. 

\begin{mdthm}
    For a particle travelling in a potential barrier:
    \begin{itemize}
        \item the \textbf{transmission probability} is given by 
        \[T(p) = \abs{C_{+}(p)}^2,\]
        \item the \textbf{reflection probability} is given by 
        \[R(p) = \abs{A_{-}(p)}^2,\]
    \end{itemize}
    with the condition that 
    \[R(p)+T(p)=1.\]
\end{mdthm}

\section{Hilbert spaces}

\subsection{Vector spaces}

\begin{definition}
    A \textbf{vector space} \(V\), over a field \(K\), is a non-empty set (the elements of which are called vectors), together with two binary operations \(+ : V\times V \to V\) and \(\cdot : K \times V \to V\) which we call \textit{vector addition} and \textit{scalar multiplication} respectively.
\end{definition}

\begin{definition}
    A vector space, \(V\), is said to be \textbf{finite-dimensional} if there exists a finite set of vectors \(\bm{e}_1, \ldots, \bm{e}_n\) such that any vector \(\bm{v} \in V\) can be written as 
    \[\bm{v} = \alpha_1 \bm{e}_1 + \cdots + \alpha_n \bm{e}_n\]
    for some \(\alpha_1,\ldots, \alpha_n \in \CC\). Such a set is called \textbf{complete}. 
\end{definition}

\begin{mdremark}
    That is, a spanning set.
\end{mdremark}

\begin{definition}
    The smallest possible size \(n\) of a complete set is called the \textbf{dimension} of the vector space \(V\) and is denoted \(\dim(V)\). If no complete finite set exists, \(V\) is called \textbf{infinite-dimensional}.
\end{definition}

\begin{mdremark}
    For convenience, the dimension of the vector space \(V =\{0\}\) is defined to be \(0\).
\end{mdremark}

\begin{theorem}
    In a finite dimensional vector space \(V\) the size of the linearly independent set cannot be larger than \(\dim(V)\).
\end{theorem}

\begin{corollary}
    Any linearly independent set with \(\dim(V)\) elements is complete.
\end{corollary}

\begin{definition}
    A finite set of vectors \(\{\bm{v}_1 , \ldots ,\bm{v}_n\}\)     is called \textbf{linearly independent} is there exists \(\alpha_1, \ldots, \alpha_n \in \CC\), for which the following holds
    \[\alpha_1 \bm{v}_1 + \cdots + \alpha_n \bm{v}_n =0 \iff \alpha_1 = \cdots = \alpha_n = 0.\] 
    Otherwise, if not all \(\alpha_i\) are zero then we call such set \textbf{linearly dependent}.
\end{definition}

\begin{mdexample}[The vector space of all functions]
    Let \(X\) be a set, and let \(V = \CC^X\) be the set of all functions from \(X \to \CC\). We define the addition and scalar multiplication for \(f,f_1,f_2 \in V\) and \(a \in \CC\) as 
    \[\begin{aligned}
        (f_1+f_2)(x) &\equiv f_1(x)+f_2(x) \\
        (af)(x) &\equiv af(x).
    \end{aligned}\]
    If \(X = \{1,2, \ldots n\}\)  we can identify \(V\) with the vector space \(\CC^n\). Whereas, if \(X\) has infinitely many elements, then \(V\) is not a finite-dimensional space. 
    \begin{proof}
        Choose \(m \in \ZZ_{\geq 0}\) and select \(m\) distinct elements \(x_1, \ldots, x_m \in X\) (which is possible for any \(m\) since \(X\) has infinitely many elements). Define functions \(f_1, \ldots, f_m\) so that \(f_i(x)=1\) if \(x=x_i\) and \(f_i(x)=0\) otherwise. Consider the function
        \[f = \alpha_1 f_1 + \cdots + \alpha_m f_m,\]
        where \(\alpha_i \in \CC\) are arbitrary constants. By construction, \(f(x_i) = \alpha_i\) therefore, if \(f=0\) then \(\alpha_i=0\) and thus \(\{f_1,\ldots,f_m\}\) is a linearly independent set with \(m\) elements. Since, \(m\) was arbitrary, we deduce that \(V\) cannot have a finite dimension -- otherwise \(m\) would have to be bounded by \(\dim(V)\).
    \end{proof}
\end{mdexample}

\subsection{Inner products}

\begin{definition}
    A \textbf{norm} on \(V\) is a function \(\norm{\, \cdot \,} :V \to \CC\) with the following properties:
    \begin{itemize}
        \item \(\norm{\bm{v}} \geq 0\) for all \(\bm{v} \in V\),
        \item \(\norm{\bm{v}} = 0 \iff \bm{v}= 0\),
        \item \textbf{Homogeneity:} \(\norm{\alpha \bm{v}}= \abs{\alpha} \norm{\bm{v}}\) for all \(\alpha \in \CC\) and \(\bm{v} \in V\),
        \item \textbf{Triangle inequality:} \(\norm{\bm{v}_1+\bm{v}_2} \leq \norm{\bm{v}_1}+\norm{\bm{v}_2}\) for all \(\bm{v}_1,\bm{v}_2 \in V\).
    \end{itemize}
\end{definition}

\begin{mdremark}
    As the norm defines the notion of distance in a vector space it also defines a topology (i.e. a notion of a limit) in on \(V\). For example, we say that a sequence \(\bm{v}_n \in V\) of vectors converges to \(v \in V\) and write 
    \[\lim_{n \to \infty} \bm{v}_n =\bm{v},\]
    if for all \(\eps >0\) there exists an \(N \in \NN\) such that \(\norm{\bm{v}_n-\bm{v}}<\eps\) for all \(n \geq N\).
\end{mdremark}

\begin{definition}
    A vector space equipped with a norm is called a \textbf{normed vector space}.
\end{definition}

\begin{definition}
    A \textbf{Hermitian inner product} on \(V\) is a function \((\cdot, \cdot) : V \times V \to \CC\) which satisfies the following properties
    \begin{enumerate}
        \item Positive definite:
        \[(\bm{v},\bm{v}) \geq 0 \quad \forall \bm{v} \in V\]
        and \((\bm{v},\bm{v}) \iff \bm{v}=0\).
        \item Hermiticity:
        \[(\bm{u},\bm{v}) = (\bm{v},\bm{u})^* \quad \forall \bm{u},\bm{v} \in V,\]
        \item linearity in the second argument,
        \[(\bm{u},\alpha \bm{v} + \beta \bm{w}) = \alpha (\bm{u},\bm{v}) +\beta (\bm{u},\bm{w}) \quad \forall \bm{u},\bm{v},\bm{w} \in V \text{ and } \forall \alpha,\beta \in \CC,\]
        \item anti-linearity in the first argument 
        \[(\alpha \bm{u} +\beta \bm{v}, \bm{w}) = \alpha^* (\bm{u},\bm{w})+ \beta^*(\bm{v},\bm{w}) \quad \forall \bm{u},\bm{v},\bm{w} \in V \text{ and } \forall \alpha,\beta \in \CC.\]
    \end{enumerate}
\end{definition}

\begin{mdthm}
    One can define a norm by 
    \[\norm{\bm{v}} = \sqrt{(\bm{v},\bm{v})}.\]
\end{mdthm}

\begin{mdlemma}[Cauchy-Schwarz inequality]
    Let \(\bm{u},\bm{v} \in V\) and \((\cdot,\cdot)\) be a Hermitian inner product on \(V\). Then 
    \[\abs{(\bm{u},\bm{v})}^2 \leq (\bm{u},\bm{u})(\bm{v},\bm{v}) = \norm{\bm{u}}^2 \norm{\bm{v}}^2.\]
\end{mdlemma}

\begin{mdremark}
    The inequality is an \textbf{equality} when \(\bm{u}\) is a scalar multiple of \(\bm{v}\).
\end{mdremark}

\subsection{Hilbert spaces}

To recap the previous section we have that 
\[\text{Vector spaces} \supseteq \text{normed vector spaces} \supseteq \text{Hermitian inner product vector spaces}.\]

With a new property this will turn into 
\[\text{Vector spaces} \supseteq \text{Banach spaces} \supseteq \text{Hilbert spaces}.\]

\begin{definition}
    A \ul{normed vector space} is called \textbf{complete} if every Cauchy sequence in this space has a limit.
\end{definition}

\begin{definition}
    A \textbf{Banach space} is a complete normed vector space.
\end{definition}

\begin{definition}
    A \textbf{Hilbert space} is a complete Hermitian inner product space (where the topology is derived from the norm defined by the inner product). The Hilbert space is denoted by \(\mathcal{H}\).
\end{definition}

\begin{mdexample}
    Some examples of Hilbert space:
    \begin{itemize}
        \item \(\CC^n = \left\{ \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \right\}\) with inner product \(\left( \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix}, \begin{pmatrix} x_1 \\ \vdots \\ x_n \end{pmatrix} \right) = a_1^* b_1 + \cdots + a_n^* b_n\).
        \item \(L^2(\RR) = \{\text{the vector space of square integrable functions from } \RR \to \CC\}\).
    \end{itemize}
\end{mdexample}

\begin{mdremark}
    We need to refine the definition of \(L^2(\RR)\) as we can choose a function for which \((f,f)=0\) but \(f \neq 0\), for example define \(f(x)=1\) for \(x=1\) and \(f(x)=0\) otherwise.
\end{mdremark}

\begin{definition}
    The vector space \(L^2(\RR)\) is defined to be the set of equivalence classes of square integral functions \(\RR \to \CC\) (two square integrable functions, \(f_1\) and \(f_2\), are considered to be equivalent if \(\norm{f_1-f_2}=0\) `almost everywhere'). The vector addition and scalar multiplication are defined on \(L^2(\RR)\) are 
    \[\begin{aligned}
        (f_1+f_2)(x) &\equiv f_1(x) + f_2(x), \\
        (af)(x) &\equiv af(x),
    \end{aligned}\]
    for \(f,f_1,f_2 \in L^2(\RR)\) and for all \(a \in \CC\).
\end{definition}

\begin{mdremark}
    By "square integrable" we mean that for \(f \in L^2(\RR)\) the integral 
    \[\int_{-\infty}^{\infty} \abs{f(x)}^2 \, dx < \infty,\]
    i.e. it is finite.
\end{mdremark}

\begin{proof}
    We prove that \(f+g \in L^2(\RR)\). Suppose \(f,g \in L^2(\RR)\) then 
    \[\begin{aligned}
        \abs{f(x)+g(x)}^2 &\leq \left( \abs{f(x)}+\abs{g(x)} \right)^2 \\
        &= \abs{f(x)}^2 \abs{g(x)}^2 + 2\abs{f(x)} \abs{g(x)}.
    \end{aligned}\]
    By the AM-GM (AM \(\geq\) GM) inequality we have that 
    \[\begin{aligned}
        \abs{f(x)} \abs{g(x)} &= \sqrt{\abs{f(x)}^2 \abs{g(x)}^2} \\
        &\leq \frac{\abs{f(x)}^2 + \abs{g(x)}^2}{2}.
    \end{aligned}\]
    Therefore, 
    \[2\abs{f(x)}\abs{g(x)} \leq \abs{f(x)}^2 \abs{g(x)}^2,\]
    which implies 
    \[\abs{f(x)+g(x)}^2 \leq 2\left( \abs{f(x)}^2 \abs{g(x)}^2 \right).\]
    We know the integral of the RHS is finite then, clearly the integral of the LHS is also finite.
\end{proof}

\begin{definition}
    We define the Hermitian inner product on \(L^2(\RR)\) as:
    \[(f,g) = \int_{-\infty}^{\infty} f^*(x) g(x) \, dx.\]
\end{definition}

\begin{mdremark}
    With this definition we can write 
    \[\langle \wh{\mathcal{O}} \rangle = \int_{-\infty}^{\infty} \Psi^*(x) (\wh{\mathcal{O}}\Psi)(x) \, dx = (\Psi,\wh{\mathcal{O}}\Psi).\]
\end{mdremark}

\begin{proof}
    This inner product is well-defined:
    \[\begin{aligned}
        \abs{\int_{-\infty}^{\infty} f^*(x) g(x) \, dx} &\leq \int_{-\infty}^{\infty} \abs{f(x)} \abs{g(x)} \, dx \\
        &\leq \half \int_{-\infty}^{\infty} \abs{f(x)}+\abs{g(x)} \, dx \\
        &<\infty.
    \end{aligned}\]
\end{proof}

\begin{mdthm}
    All finite dimensional vector spaces and \(L^2(\RR)\) are complete, and they are all Hilbert spaces.
\end{mdthm}

\begin{mdremark}
    In particular \(L^2(X)\) is complete and hence, Hilbert for any metric space \(X\).
\end{mdremark}

\subsection{Dual vector space}

\begin{definition}
    A \textbf{linear functional}, \(\alpha\), on a complex vector space \(V\) is a function \(\alpha: V \to \CC\) which is linear i.e.
    \begin{enumerate}
        \item for all \(c \in \CC\) and \(\bm{v} \in V\), \(\alpha(c\bm{v}) = c\alpha(\bm{v})\),
        \item for all \(\bm{v}_1,\bm{v}_2 \in V\) we have \(\alpha(\bm{v}_1+\bm{v}_2) = \alpha(\bm{v}_1)+\alpha(\bm{v}_2)\).
    \end{enumerate}
\end{definition}

\begin{mdnote}
    We can think of linear functionals as linear maps which take in vectors and output scalars in \(\CC\).
\end{mdnote}

\begin{definition}
    Let \(\mathcal{H}\) be a Hilbert space. The \textbf{(continuous) dual space} \(\mathcal{H}^*\) is the vector space of all continuous linear functionals on \(\mathcal{H}\).
\end{definition}

\begin{mdremark}
    For a general vector space \(V\) we do not need continuity in the definition.
\end{mdremark}

\begin{mdexample}
    Suppose \(\mathcal{H} = \CC^n\), if we have \(\alpha \in (\CC^n)^*\) and \(\bm{v} \in \CC^n\) such that
    \[\bm{v} = \begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix}.\]
    Then, we can define the linear functional \(\alpha\) as the row vector
    \[\alpha = (\alpha_1, \cdots, \alpha_n)\]
    so that 
    \[\alpha(\bm{v}) = (\alpha_1, \cdots, \alpha_n)\begin{pmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{pmatrix} = \alpha_1 v_1 + \cdots \alpha_n v_n.\]
    That is, \(\alpha: \CC^n \to (\CC^n)^*\). In conclusion, elements of \((\CC^n)^*\) are row vectors i.e. 
    \[(\CC^n)^* = \{(\alpha_1, \ldots , \alpha_n) : \alpha_1,\ldots,\alpha_n \in \CC\}.\]
\end{mdexample}

\begin{mdthm}
    Let \(\mathcal{H}\) be a Hilbert space. Then there is a canonical anti-linear bijection from \(\mathcal{H} \to \mathcal{H}^*\). This map is given by the inner product: given a vector \(\bm{u} \in \mathcal{H}\), define \(\alpha_{\bm{u}} \in \mathcal{H}^*\) by 
    \[\alpha_{\bm{u}}(\bm{v}) = (\bm{u},\bm{v}).\]
\end{mdthm}

\begin{mdexample}
    Suppose \(\mathcal{H} = \CC^n\) and let 
    \[\bm{u} = \begin{pmatrix} u_1 \\ \vdots \\ u_n \end{pmatrix},\]
    then, \(\alpha_{\bm{u}} \in \mathcal{H}^*\) is given by the row vector
    \[\begin{aligned}
        \alpha_{\bm{u}} &= \bm{u}^{\dagger} \\
        &= (u_1^*, \ldots, u_n^*),
    \end{aligned}\]
    where \(`\dagger'\) means `conjugate transpose'.
    Indeed, for any \(\bm{v} \in \mathcal{H} = \CC^n\) we have
    \[\begin{aligned}
        \alpha_{\bm{u}}(\bm{v}) &= u_1^* v_1 + \cdots + u_n^*v_n \\
        &= (\bm{u},\bm{v}).
    \end{aligned}\]
\end{mdexample}

\begin{example}
    In the case of \(\mathcal{H}=L^2(\RR)\) the theorem says that any linear functional \(\alpha \in \mathcal{H}^*\) can be represented as 
    \[\alpha_g(f) = \int_{-\infty}^{\infty} g^*(x) f(x) \, dx\]
    for some \(g \in L^2(\RR)\).
\end{example}

\subsection{Dirac's notation}

\begin{definition}
    Elements of the Hilbert space, \(\mathcal{H}\), are denoted by \(\ket{v} \in \mathcal{H}\), we call this `\textbf{ket}'.
\end{definition}

\begin{definition}
    Linear functionals in the dual of the Hilbert space, \(\mathcal{H}^*\) are denoted by \(\bra{\alpha} \in \mathcal{H}^*\), we call this `\textbf{bra}'.
\end{definition}

\begin{theorem}
    The value of a linear functional \(\bra{\alpha}\) on a vector \(\ket{v}\) is denoted by 
    \[\bra{\alpha} \ket{v} \in \CC.\]
\end{theorem}

\begin{mdthm}[Relation between bra-ket]
    The bra vector is related to the ket vector by conjugate transpose:
    \[\bra{u} = (\ket{u})^{\dagger}.\]
    Therefore, the inner product can be written as
    \[(\ket{u},\ket{v}) = \bra{u} \ket{v}.\]
\end{mdthm}

\begin{proof}
    The bra vector \(\bra{u}\) (for any label) denotes the linear functional that corresponds to \(\ket{u}\), i.e. \(\bra{u} = \alpha_{\ket{u}}\). Recall, the relation \(\alpha_{\ket{u}}^{\dagger} = (\ket{u})^{\dagger}\) so, \(\bra{u} = (\ket{u})^{\dagger}\). Furthermore, for any \(\ket{v} \in \mathcal{H}\) we have 
    \[\begin{aligned}
        \alpha_{\ket{u}}(\ket{v}) &= (\ket{u},\ket{v}) \\
        &= \bra{u}\ket{v}.
    \end{aligned}\]
\end{proof}

\subsection{Orthonormal basis in \texorpdfstring{\(\mathcal{H}\)}{TEXT}}

\begin{definition}
    Let \(\mathcal{H}\) be an infinite dimensional Hilbert space and let \\ \(\{\ket{e_1},\ket{e_2}, \ldots\} \subset \mathcal{H}\) be a sequence of vectors. We say that the vectors \(\ket{e_k}\) are \textbf{orthonormal} if 
    \[\begin{aligned}
        \bra{e_i} \ket{e_j} = \delta_{ij} = \begin{cases}
            0 &\text{if } i\neq j \\
            1 &\text{if } i=j.
        \end{cases}
    \end{aligned}\]
    The \(\delta_{ij}\) represents the Kronecker \(\delta\) symbol.
\end{definition}

\begin{theorem}
    Let \(\{a_k\}_{k=1}^{\infty}\) be a sequence of complex numbers and \(\ket{e_i} \in \mathcal{H}\). The infinite sum 
    \[\sum_{k=1}^{\infty} a_k \ket{e_k} = \lim_{N \to \infty} \sum_{k=1}^{N} a_k \ket{e_k},\]
    exists and converges in \(\mathcal{H}\) if and only if the sum
    \[\sum_{k=1}^{\infty} \abs{a_k}^2 <\infty\]
    (that is, the sum above converges and is finite).
\end{theorem}

\begin{mdnote}
    This theorem allows us to write \(\ket{v}\) as a linear combination of the basis of the vector space.
\end{mdnote}

\begin{proof}
    This proof is NOT examinable hence, we present a sketch of the proof. It is enough to prove \(\sum_{k=1}^{\infty} a_k \ket{e_k}\) is a Cauchy sequence then, since \(\mathcal{H}\) is complete, by definition all Cauchy sequences converge to a limit. Thus, the sum must exist.
\end{proof}

\begin{definition}
    Hilbert spaces, \(\mathcal{H}\), with have elements which can be written in terms of an orthonormal basis are called \textbf{separable}.
\end{definition}

\begin{mdremark}
    In this course, all Hilbert spaces will be separable.
\end{mdremark}

\subsubsection{Computing coefficients of a basis}

Suppose \(V\) is a finitely dimensional vector space with an orthonormal basis \\ \(\{\bm{v}_1, \ldots, \bm{v}_n\}\), that is 

\[\begin{aligned}
    \bm{v}_i \cdot \bm{v}_j = \delta_{ij} = \begin{cases}
        0 &\text{if } i\neq j \\
        1 &\text{if } i=j.
    \end{cases}
\end{aligned}\]

Then, any vector \(\bm{x} \in V\) can be expressed in terms of the basis vectors:
\[\bm{x} = a_1 \bm{v}_1 + \cdots + a_i \bm{v}_i + \cdots + a_n \bm{v}_n.\]

Since the basis is orthonormal if we consider \(\bm{v}_i \cdot \bm{x}\) we will obtain the following 

\[\begin{aligned}
    \bm{v}_i \cdot \bm{x} &= a_1 \underbrace{\bm{v}_i \cdot \bm{v}_1}_{\bm{0}} + \cdots + a_i \bm{v}_i \cdot \bm{v}_i + \cdots + a_n  \underbrace{\bm{v}_i \cdot \bm{v}_n}_{\bm{0}}\\
    &= a_i \underbrace{\bm{v}_i \cdot \bm{v}_i}_{\bm{1}} \\
    &= a_i.
\end{aligned}\]

We now extend this idea to Hilbert spaces. Suppose that \(\mathcal{H}\) is separable then,
\[\ket{v} = \sum_{k=1}^{\infty} a_k \ket{e_k}.\]

Consider, the inner product 
\[\begin{aligned}
    \bra{e_i}\ket{v} &= \bra{e_i} \sum_{k=1}^{\infty} a_k \ket{e_k} \\
    &= \sum_{k=1}^{\infty} a_k \bra{e_i} \ket{e_k} \\
    &= \sum_{k=1}^{\infty} a_k \delta_{ik} \\
    &= a_i;
\end{aligned}\]
since \(\bra{e_i} \in \mathcal{H}^*\) is a \textit{continuous} linear functional, we can exchange the sum with the action of \(\bra{e_i}\).

\begin{mdthm}
    If \(\mathcal{H}\) has an orthonormal basis then every \(\ket{v} \in \mathcal{H}\) can be written as 
    \[\ket{v} = \sum_{k=1}^{\infty} a_k \ket{e_k},\]
    where the coefficients \(a_i\) are given by 
    \[a_i = \bra{e_i} \ket{v}.\]
\end{mdthm}

\begin{corollary}
    If \(\mathcal{H}\) has an orthonormal basis then the vectors \(\ket{v} \in \mathcal{H}\) are also written as 
    \[\begin{aligned}
        \ket{v} &= \sum_{k=1}^{\infty} \bra{e_k} \ket{v} \ket{e_k} \\
        &= \sum_{k=1}^{\infty} \ket{e_k} \bra{e_k}\ket{v}.
    \end{aligned}\]
\end{corollary}

\begin{mdremark}
    The expression \(\ket{e_k} \bra{e_k}\) can be understood as a linear operator via the map 
    \[\begin{aligned}
        \ket{v} &\mapsto \left( \ket{e_k} \bra{e_k} \right) \ket{v}\\
        &\mapsto \ket{e_k} \bra{e_k} \ket{v}.
    \end{aligned}\]
    Therefore, the expression 
    \[\sum_{k=1}^{\infty} \ket{e_k}\bra{e_k}=1;\]
    the identity operator of this space.
\end{mdremark}

\subsection{Delta-normalisable states}

\subsubsection{Motivation}

In this section we introduce a new piece of technology that we will need. In the previous section we discussed a basis composed of vector \(\ket{e_k} \), with \(k\) as a discrete label. In this section, we will discuss the case when \(k\) is a continuous label. A motivating example: consider \(\mathcal{H} = L^2(\RR)\) with the functions 
\[e_k(x)= \frac{1}{\sqrt{2\pi}} e^{ikx}\]
where \(k \in \RR\). These functions are not actually elements of \(\mathcal{H}\) because they are not square-integrable as,

\[\int_{-\infty}^{\infty} \abs{e_k(x)}^2 \, dx = \frac{1}{2\pi} \int_{-\infty}^{\infty} dx = \infty.\]

Nevertheless, we pretend that these functions are elements of \(\mathcal{H}\) and try to understand what their inner product is:
\[\begin{aligned}
    \bra{e_{k_1}} \ket{e_{k_2}} &= \frac{1}{2\pi} \int_{-\infty}^{\infty} \left( e^{ik_1 x} \right)^* e^{ik_2 x} \, dx \\
    &= \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{i(k_2-k_1)}x \, dx.
\end{aligned}\]

Clearly, this integral does not converge in the usual sense. However, we can define 

\[\int_{-\infty}^{\infty} e^{ikx} \, dx = 2\pi \delta(k),\]

where \(\delta(k)\) is the Dirac delta function. Therefore,

\[\begin{aligned}
    \bra{e_{k_1}} \ket{e_{k_2}} &= \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{i(k_2-k_1)}x \, dx \\
    &= \delta(k_2-k_1)\\ 
    &= \delta(k_1-k_2).
\end{aligned}\]

Strictly speaking \(\ket{e_k} \not\in \mathcal{H}\), but it is possible to form elements of \(\mathcal{H}\) using them. By defining 
\[\ket{\Psi} = \int_{-\infty}^{\infty} a(k) \ket{e_k} \, dk\]

for some function \(f(k)\), we have that \(\ket{\Psi} \in \mathcal{H}\) if \(f(k)\) is square-integrable. In fact 

\[\begin{aligned}
    \bra{\Psi}\ket{\Psi} &=(\bra{\Psi})^{\dagger} \ket{\Psi} \\
    &= \left( \int_{-\infty}^{\infty} a(k') \ket{e_{k'}} \, dk' \right)^{\dagger} \int_{-\infty}^{\infty} a(k) \ket{e_{k}} dk \\
    &= \int_{-\infty}^{\infty} a^*(k') \bra{e_{{k'}}} \, dk' \int_{-\infty}^{\infty} a(k) \ket{e_{k}} \, dk \\
    &= \int_{-\infty}^{\infty} a^*(k') \, dk' \int_{-\infty}^{\infty} a(k) \bra{e_{k'}} \ket{e_{k}} \, dk \\
    &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} a^*(k') a(k) \delta(k'-k) \, dk' \, dk.
\end{aligned}\]

Consider the following identity

\[\int_{-\infty}^{\infty} f(k')\delta(k'-k) \, dk' =f(k'),\]

by changing the variable \(k' \to k'+k\) the integral becomes 

\[\int_{-\infty}^{\infty} f(k'+k) \delta(k') \, dk' = f(k),\]

as such we can write 

\[\int_{\infty}^{\infty} a^*(k') \delta(k'-k) \, dk' \to \int_{\infty}^{\infty} a^*(k'+k) \delta(k') \, dk' = a^*(k).\]


Therefore, 

\[\begin{aligned}
    \bra{\Psi}\ket{\Psi} &=\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} a^*(k') a(k) \delta(k'-k) \, dk' \, dk \\
    &= \int_{-\infty}^{\infty} a(k) a^*(k) \, dk \\
    &= \int_{-\infty}^{\infty} \abs{a(k)}^2 \, dk,
\end{aligned}\]

which is finite if \(a(k)\) is square integrable.

\subsubsection{The Dirac delta function}

\begin{definition}
    The \textbf{Dirac delta function} is defined as 
    \[\begin{aligned}
        \delta(k) = \begin{cases}
            \infty &\text{if } k=0 \\
            0 &\text{if } k\neq 0
        \end{cases}
    \end{aligned}\]
    with the property that 
    \[\int_{-\infty}^{\infty} \delta(k) \, dk =1.\]
\end{definition}

\begin{mdremark}
    Although, it is called a `function' it is not truly a function, rather a distribution. 
\end{mdremark}

\begin{mdthm}
    Properties of the \(\delta\)-function:
    \begin{itemize}
        \item \(\delta(x) \in \RR\),
        \item \(\delta(x) \geq 0\),
        \item \(\delta(x)=\delta(-x)\) (even).
        \item \(\int_{-\infty}^{\infty} \delta(k) f(k) \, dk = f(0)\) (sometimes we use the notation \(\delta(f)= f(0)\));
        \item \(\int_{-\infty}^{\infty} \delta(k-a)f(k) \, dk = f(a)\);
        \item \(\int_{-\infty}^{\infty} e^{ikx} \, dx = 2\pi \delta(k)\);
        \item \(\bra{e_x}\ket{e_y}=\delta(x-y)\).
    \end{itemize}
\end{mdthm}

\subsubsection{Stocktaking}

\begin{definition}
    We say that \(\ket{e_x}\) and \(\ket{e_y}\) are \textbf{delta-normalisable} if 
    \[\bra{e_x}\ket{e_y}=\delta(x-y).\]
\end{definition}

\begin{definition}
    We say that \(\ket{e_k}\) for \(k \in \RR\) form a \textbf{delta-normalisable basis} of \(\mathcal{H}\) if any \(\ket{\Psi} \in \mathcal{H}\) can be represented in the form
    \[\ket{\Psi} = \int_{-\infty}^{\infty} a(k) \ket{e_k} \, dk,\]
    for some function square-integrable function \(a(k)\) (i.e. \(a(k) \in L^2(\RR)\)).
\end{definition}

\begin{mdremark}
    Strictly, speaking \(\ket{e_k} \not\in \mathcal{H}\) as they are not square integrable.
\end{mdremark}

\begin{mdnote}
    We can think of \(a(k)\) as the analogous to the coefficients \(a_k\).
\end{mdnote}

\begin{mdexample}
    Vectors of the form 
    \[\ket{e_k} = \frac{1}{\sqrt{2\pi}} e^{ikx}\]
    form a delta-normalisable basis.
\end{mdexample}

\begin{mdthm}
    If \(\mathcal{H}\) has a delta-normalisable basis the `coefficients' \(a(k)\) are given by 
    \[a(k) = \bra{e_k} \ket{\Psi}.\]
\end{mdthm}

\begin{proof}
    Consider a \(\ket{\Psi} \in \mathcal{H}\) where \(\mathcal{H}\) has a delta-normalisable basis, \(\{\ket{e_j}\}\), then we can write
    \[\ket{\Psi} = \int_{-\infty}^{\infty} a(j) \ket{e_j} \, dj.\]
    We have that 
    \[\begin{aligned}
        \bra{e_k} \ket{\Psi} &= \bra{e_k} \int_{-\infty}^{\infty} a(j) \ket{e_j} \, dj \\
        &= \int_{-\infty}^{\infty} \bra{e_k} a(j) \ket{e_j} \, dj \\
        &= \int_{-\infty}^{\infty} a(j) \bra{e_k}\ket{e_j} \, dj\\
        &= \int_{-\infty}^{\infty} a(j) \delta(k-j) \, dj \\
        &= a(k).
    \end{aligned}\]
\end{proof}

\begin{corollary}
    If \(\mathcal{H}\) has a delta-normalisable basis then every \(\ket{\Psi} \in \mathcal{H}\) can be represented as 
    \[\begin{aligned}
        \ket{\Psi} &= \int_{-\infty}^{\infty} \bra{e_k} \ket{\Psi} \ket{e_k} \, dk \\
        &= \int_{-\infty}^{\infty} \ket{e_k} \bra{e_k} \ket{\Psi} \, dk.
    \end{aligned}\]
\end{corollary}

\begin{mdremark}
    Thus, the expression 
    \[\int_{-\infty}^{\infty} \ket{e_k} \bra{e_k} \, dk =1.\]
\end{mdremark}

\section{Linear operators and observables}

\subsection{Linear operators}

\begin{definition}
    A \textbf{linear operator} \(\wh{\mathcal{O}}\) on a vector space \(V\) is a linear map \(\wh{\mathcal{O}} : V \to V\). 
\end{definition}

\begin{mdnote}
    By linear we mean the usual linear conditions are met:
    \[\wh{\mathcal{O}} \left( \alpha\ket{v} +\beta\ket{u} \right) = \alpha \wh{\mathcal{O}}\ket{v}+\beta\wh{\mathcal{O}}\ket{u}.\]
\end{mdnote} 

\begin{definition}
    The \textbf{identity operator}, denote by \(\wh{1}\), is defined as 
    \[\wh{1} \ket{v} = \ket{v}.\]
\end{definition}

\begin{corollary}
    Scalars can be written as operators. For example, the operator \(\wh{\mathcal{O}}+2\) acts on vectors by 
    \[\left( \wh{\mathcal{O}}+2 \right) \ket{v} = \wh{\mathcal{O}} \ket{v} + 2 \cdot \wh{1} \ket{v}.\]
\end{corollary}

\begin{theorem}
    Operators form a vector space.
\end{theorem}

\begin{proof}
    Addition and scalar multiplication is closed i.e.
    \[\begin{aligned}
        \left( \wh{\mathcal{O}}_1+\wh{\mathcal{O}}_2 \right) \ket{v} &= \wh{\mathcal{O}}_1\ket{v} +\wh{\mathcal{O}}_2 \ket{v}, \\
        \left( \alpha \wh{\mathcal{O}} \right) \ket{v} &= \alpha \wh{\mathcal{O}} \ket{v}.
    \end{aligned}\]
\end{proof}

\begin{theorem}
    The product of operators \(\wh{\mathcal{O}}_1 \wh{\mathcal{O}}_2\) is defined as the composition of operators (maps)  
    \[\left( \wh{\mathcal{O}}_1\wh{\mathcal{O}}_2 \right) \ket{v} = \wh{\mathcal{O}}_1 \left( \wh{\mathcal{O}}_2 \ket{v} \right).\]
\end{theorem}

\begin{mdremark}
    Since, composition of maps is not always commutative the same applies for composition of operators.
\end{mdremark}

\begin{mdremark}
    Recall if \(\ket{\Psi} \in L^2(\RR)\) is the function \(\Psi(x)\) then, \(\wh{p} \ket{\Psi}\) is the function \(-i\hbar \diffp{}{x} \Psi(x)\). However, not every function in \(L^2(\RR)\) is differentiable, and even if they are their derivative is not necessarily in \(L^2(\RR)\). The operator \(\wh{p}\), as well as most other operator in QM are not well-defined linear operators, they are called \textbf{densely-defined} linear operators. 

    A densely defined linear operator \(\wh{\mathcal{O}}\) on a Hilbert space \(\mathcal{H}\) with domain \(D\) is a linear function \(D \to \mathcal{H}\), where \(D \subseteq \mathcal{H}\) is a dense linear subspace. Note that the domain \(D\) need to be supplied as a part of the definition of \(\wh{\mathcal{O}}\). Thus, just saying \(\wh{p}\) does not strictly make sense. 
\end{mdremark}

\subsection{Expectation values and matrix elements}

\begin{mdthm}
    Let \(\wh{\mathcal{O}} : \mathcal{H} \to \mathcal{H}\) be a linear operator on a Hilbert space \(\mathcal{H}\). Then, the \textbf{expectation value} of \(\wh{\mathcal{O}} \) in a state \(\ket{\Psi} \in L^2(\RR)\) is defined as 
    \[\langle \wh{\mathcal{O}}  \rangle = \frac{\bra{\Psi} \wh{\mathcal{O}} \ket{\Psi}}{\bra{\Psi} \ket{\Psi}}.\]
\end{mdthm}

\begin{proof}
    Recall the expectation of an operator is defined as 
    \[\langle \wh{\mathcal{O}} \rangle = \int_{-\infty}^{\infty} \Psi^*(x) \left( \wh{\mathcal{O}}\Psi \right)(x) \, dx.\]
    Furthermore, if \(\mathcal{H} = L^2(\RR)\) the inner product is defined as 
    \[(f,g) = \int_{-\infty}^{\infty} f^*(x) g(x) \, dx.\]
    If \(\Psi\) is not normalised then, we can normalise it by dividing by \(\abs{\Psi}^2 = \bra{\Psi} \ket{\Psi}\) thus, the definition of \(\langle \wh{\mathcal{O}} \rangle\) follows as above.
\end{proof}

\begin{corollary}
    If the state \(\ket{\Psi}\) is normalised, \(\bra{\Psi} \ket{\Psi}=1\) then, the expectation value is  
    \[\langle \wh{\mathcal{O}}\rangle  = \bra{\Psi} \wh{\mathcal{O}} \ket{\Psi}.\]
\end{corollary}

\subsubsection{Matrix elements}

Suppose that \(\{\ket{e_1}, \ket{e_2}, \ldots\} \subset \mathcal{H}\) is an orthonormal of \(\mathcal{H}\). Then, every vector (by the previous discussion) can be written as 

\[\begin{aligned}
    \ket{\Psi} &= \sum_{k=1}^{\infty} a_k \ket{e_k} \\
    &= \sum_{k=1}^{\infty} \bra{e_k} \ket{\Psi} \ket{e_k}.
\end{aligned}\]

Since \(\ket{\Psi}\) is a (column) vector, we can think of the coefficients \(a_k = \bra{e_k}\ket{\Psi}\) as the coordinates (components) of \(\ket{\Psi}\) in the basis \(\{\ket{e_1}, \ket{e_2}, \ldots\}\). Applying the same argument to \(\wh{\mathcal{O}} \ket{\Psi}\) we get 

\[\begin{aligned}
    \wh{\mathcal{O}} \ket{\Psi} &= \sum_{k=1}^{\infty} b_k \ket{e_k} \\
    &= \sum_{k=1}^{\infty} \bra{e_k} \wh{\mathcal{O}} \ket{\Psi} \ket{e_k}.
\end{aligned}\]

The coordinates of \(\wh{\mathcal{O}} \ket{\Psi}\) in the same basis are \(\bra{e_k} \wh{\mathcal{O}} \ket{\Psi}\). Using the previous expression we can express 

\[\begin{aligned}
    \bra{e_k} \wh{\mathcal{O}} \ket{\Psi} &= \bra{e_k} \sum_{j=1}^{\infty} \bra{e_j} \wh{\mathcal{O}} \ket{\Psi}\ket{e_j} \\
    &= \sum_{j=1}^{\infty} \bra{e_k} \bra{e_j} \wh{\mathcal{O}} \ket{\Psi}\ket{e_j} \\
    &= \sum_{j=1}^{\infty} \bra{e_k} \wh{\mathcal{O}} \ket{\Psi}\underbrace{\bra{e_j}\ket{e_j}}_{1}.
\end{aligned}\]

Now substituting the expression for \(\ket{\Psi}\) we have 
\[\begin{aligned}
    \bra{e_k} \wh{\mathcal{O}} \ket{\Psi} &= \sum_{j=1}^{\infty} \bra{e_k} \wh{\mathcal{O}} \bra{e_j} \ket{\Psi} \ket{e_j} \\
    &= \sum_{j=1}^{\infty} \bra{e_k} \wh{\mathcal{O}} \ket{e_j} \bra{e_j}\ket{\Psi}.
\end{aligned}\]

The coordinates of the vector can be arranged into a column vector:

\[\begin{pmatrix}
    \bra{e_1} \ket{\Psi} \\
    \bra{e_2} \ket{\Psi} \\
    \vdots 
\end{pmatrix} \quad \text{and} \quad \begin{pmatrix}
    \bra{e_1} \wh{\mathcal{O}} \ket{\Psi} \\
    \bra{e_2} \wh{\mathcal{O}} \ket{\Psi} \\
    \vdots 
\end{pmatrix}.\]

Then we can write the identity for \(\bra{e_k} \wh{\mathcal{O}} \ket{\Psi}\) as the following matrix product:

\[
    \begin{pmatrix}
        \bra{e_1} \wh{\mathcal{O}} \ket{\Psi} \\
        \bra{e_2} \wh{\mathcal{O}} \ket{\Psi} \\
        \vdots 
    \end{pmatrix}
    =
    \begin{pmatrix}
        \bra{e_1} \wh{\mathcal{O}} \ket{e_1} & \bra{e_1} \wh{\mathcal{O}} \ket{e_2} & \cdots \\
        \bra{e_2} \wh{\mathcal{O}} \ket{e_1} & \bra{e_2} \wh{\mathcal{O}} \ket{e_2} & \cdots \\
        \vdots & \vdots & \ddots
    \end{pmatrix}
    \begin{pmatrix}
        \bra{e_1} \ket{\Psi} \\
        \bra{e_2} \ket{\Psi} \\
        \vdots 
    \end{pmatrix}.
\]

\begin{mdremark}
    If \(\mathcal{H}\) is a finite vector space then clearly, the matrix become finite too.
\end{mdremark}

\subsection{Adjoint operator}

\begin{definition}
    Let \(\wh{\mathcal{O}} : \mathcal{H} \to \mathcal{H}\) be a linear operator. The \textbf{adjoint} of \(\wh{\mathcal{O}}\), denoted by \({\wh{\mathcal{O}}}^{\dagger}\), is an operator such that 
    \[\left( \ket{u}, \wh{\mathcal{O}}\ket{v} \right) = \left( {\wh{\mathcal{O}}}^{\dagger} \ket{u}, \ket{v} \right) \quad \forall \ket{u},\ket{v} \in \mathcal{H}.\]
\end{definition}

\begin{mdnote}
    Emphasis: the brackets denote the inner product.
\end{mdnote}

\begin{mdthm}
    The \textbf{Hermitian conjugate}, denoted by \(\dagger\), can act on expressions involving numbers, linear operator and bra-ket vectors. It turns 
    \begin{itemize}
        \item numbers into their complex conjugate;
        \item bra into ket vectors and vice versa;
        \item operators into their adjoint;
    \end{itemize}
    Furthermore, the Hermitian conjugate reverses the order of all products.
\end{mdthm}

\begin{mdexample}
    Consider the number \(\bra{u} \wh{\mathcal{O}}_1 \wh{\mathcal{O}}_2 \ket{v}\) then,
    \[\begin{aligned}
        \left( \bra{u} \wh{\mathcal{O}}_1 \wh{\mathcal{O}}_2 \ket{v} \right)^* &= \left( \bra{u} \wh{\mathcal{O}}_1 \wh{\mathcal{O}}_2 \ket{v} \right)^{\dagger} \\
        &= \left( \ket{v} \right)^{\dagger} \wh{\mathcal{O}}_2^{\dagger} \wh{\mathcal{O}}_1^{\dagger} \left( \bra{u} \right)^{\dagger} \\
        &= \bra{v} \wh{\mathcal{O}}_2^{\dagger} \wh{\mathcal{O}}_1^{\dagger} \ket{u}.
    \end{aligned}\]
\end{mdexample}

\begin{mdprop}
    Properties of the adjoint operator:
    \begin{itemize}
        \item \(\left( {\wh{\mathcal{O}}}^{\dagger} \right)^{\dagger} = \wh{\mathcal{O}}\);
        \item \(\left( \alpha\wh{\mathcal{O}}_1 +\beta \wh{\mathcal{O}}_2  \right)^{\dagger} = \alpha^* \wh{\mathcal{O}}_1^{\dagger} + \beta^* \wh{\mathcal{O}}_2^{\dagger}\);
        \item \(\left( \wh{\mathcal{O}}_1 \wh{\mathcal{O}}_2 \right)^{\dagger} = \wh{\mathcal{O}}_2^{\dagger} \wh{\mathcal{O}}_1^{\dagger}\).
    \end{itemize}
\end{mdprop}

\subsection{Self-adjoint (Hermitian) operators}

\begin{definition}
    Let \(\mathcal{H}\) be a Hilbert space. A linear operator \(\wh{\mathcal{O}} : \mathcal{H} \to \mathcal{H}\) is called \textbf{Hermitian} (or \textbf{self-adjoint}) if \(\wh{\mathcal{O}} = {\wh{\mathcal{O}}}^{\dagger}\).
\end{definition}

\begin{mdremark}
    The domain of a Hermitian operator \(\wh{\mathcal{O}}^{\dagger}\) is the same as the domain of the operator \(\wh{\mathcal{O}}\).
\end{mdremark}

\begin{definition}
    An observable on \(\mathcal{H}\) is a Hermitian operator on \(\mathcal{H}\).
\end{definition}

\begin{mdthm}
    Let \(\wh{\mathcal{O}}\) be a Hermitian operator. The properties of Hermitian operators are as follows:
    \begin{itemize}
        \item matrix elements of \(\wh{\mathcal{O}}\) form a Hermitian matrix i.e. \(M=M^{\dagger}\);
        \item the expectation values \(\wh{\mathcal{O}}\) are real i.e. \(\wh{\mathcal{O}} \in \RR\);
        \item the eigenvalues of \(\wh{\mathcal{O}}\) are real;
        \item the eigenvectors with distinct eigenvalues are orthogonal to each other i.e. if \(\ket{\Psi_1}\) and \(\ket{\Psi_2}\) are eigenvectors of with eigenvalues \(\lambda_1\) and \(\lambda_2\) respectively with \(\lambda_1 \neq \lambda_2\) then \(\bra{\Psi_1} \ket{\Psi_2} =0\).
    \end{itemize}
\end{mdthm}

\begin{proof}
    We prove each statement in turn.
    \begin{itemize}
        \item The matrix elements of \(\wh{\mathcal{O}}\) are the numbers \(\bra{e_k} \wh{\mathcal{O}} \ket{e_j}\) so, consider 
        \[\begin{aligned}
            \left( \bra{e_k} \wh{\mathcal{O}} \ket{e_j} \right)^* &= \left( \bra{e_k} \wh{\mathcal{O}} \ket{e_j} \right)^{\dagger} \\
            &=  (\ket{e_j})^{\dagger}\wh{\mathcal{O}}^{\dagger} (\bra{e_k})^{\dagger} \\
            &= \bra{e_j} \wh{\mathcal{O}} \ket{e_k}.
        \end{aligned}\]
        We can exchange \(* \leftrightarrow \dagger\) since \(\bra{e_k} \wh{\mathcal{O}} \ket{e_j}\) is just a number.
        \item The expectation of \(\langle \wh{\mathcal{O}} \rangle = \bra{\Psi} \wh{\mathcal{O}} \ket{\Psi}\) then, consider 
        \[\begin{aligned}
            \langle \wh{\mathcal{O}} \rangle^* &= \left( \bra{\Psi} \wh{\mathcal{O}} \ket{\Psi} \right)^{\dagger} \\
            &= \bra{\Psi} \wh{\mathcal{O}} \ket{\Psi}.
        \end{aligned}\]
        Therefore, \(\langle \wh{\mathcal{O}} \rangle = \langle \wh{\mathcal{O}} \rangle^*\).
        \item Recall what is an eigenvector: \(\ket{\Psi}\) of \(\wh{\mathcal{O}}\) is an eigenvector if 
        \[\wh{\mathcal{O}}\ket{\Psi} = \lambda \ket{\Psi} \quad \text{for } \ket{\Psi} \neq \bm{0}.\]
        Taking the Hermitian conjugate \((\dagger)\) of this equation we have 
        \[\begin{aligned}
            \left( \wh{\mathcal{O}}\ket{\Psi} \right)^{\dagger}&= \left(  \lambda \ket{\Psi} \right)^{\dagger} \\
            \then \bra{\Psi} {\wh{\mathcal{O}}}^{\dagger} &= \lambda^* \bra{\Psi} \\
            \then \bra{\Psi} \wh{\mathcal{O}} &= \lambda^* \bra{\Psi}.
        \end{aligned}\]
        Thus, if \(\ket{\Psi}\) is a `ket eigenvector' of \(\wh{\mathcal{O}}\) with eigenvalue \(\lambda\) then, \(\bra{\Psi}\) is a `bra eigenvector' of \(\wh{\mathcal{O}}\) with eigenvalue \(\lambda^*\). Now consider the equation
        \[\left( \bra{\Psi}\wh{\mathcal{O}} \right) \ket{\Psi}=\bra{\Psi} \left( \wh{\mathcal{O}} \ket{\Psi} \right),\]
        and apply the definition of an eigenvector, we obtain 
        \[\lambda^* \bra{\Psi}\ket{\Psi} = \lambda \bra{\Psi}\ket{\Psi}.\] 
        If \(\ket{\Psi} \neq 0\) then \(\bra{\Psi} \ket{\Psi} \neq 0\) which implies
        \[\lambda^*=\lambda.\]
        \item Suppose \(\ket{\Psi_1}\) and \(\ket{\Psi_2}\) are eigenvectors of the Hermitian operator \(\wh{\mathcal{O}}\) with distinct eigenvalues \(\lambda_1 \neq \lambda_2\). Consider,
        \[\left( \bra{\Psi_1} \wh{\mathcal{O}} \right) \ket{\Psi_2} = \bra{\Psi_1} \left( \wh{\mathcal{O}} \ket{\Psi_2} \right).\]
        Using the definition of an eigenvector we obtain 
        \[\begin{aligned}
            \lambda_1 \bra{\Psi_1} \ket{\Psi_2} &= \lambda_2 \bra{\Psi_1} \ket{\Psi_2} \\
            \then (\lambda_1-\lambda_2) \bra{\Psi_1} \ket{\Psi_2} &= 0.
        \end{aligned}\]
        If \(\lambda_1 \neq \lambda_2\), we can divide by \((\lambda_1-\lambda_2)\) and conclude 
        \[\bra{\Psi_1}\ket{\Psi_2} =0.\]
    \end{itemize}
\end{proof}

\begin{mdexample}
    The following are examples of Hermitian operators i.e. observables:
    \begin{itemize}
        \item the position operator, \(\wh{x}\);
        \item the momentum operator, \(\wh{p}\);
        \item the Hamiltonian operator, \(\wh{H}\).
    \end{itemize}
\end{mdexample}

\section{The spectral theorem}

\begin{mdthm}[The spectral theorem]
    Let \(\mathcal{H}\) be a Hilbert space and let \(\wh{\mathcal{O}}\) be a self-adjoint linear operator on \(\mathcal{H}\). Then 
    \begin{itemize}
        \item (Any two eigenvectors of \(\wh{\mathcal{O}}\) with distinct eigenvalues are orthogonal to each other);
        \item the set of all eigenvalues of \(\wh{\mathcal{O}}\), which we call the \textbf{spectrum}, is a subset of \(\RR\) i.e. all the eigenvalues of \(\wh{\mathcal{O}}\) are real;
        \item the eigenvectors of \(\wh{\mathcal{O}}\) form a complete basis of the Hilbert space \(\mathcal{H}\).
    \end{itemize}
\end{mdthm}

\begin{mdnote}
    In practice, we need to distinguish between a \textit{discrete} and \textit{continuous} spectrum of \(\wh{\mathcal{O}}\).
\end{mdnote}

\subsection{Discrete spectrum}

Suppose \(\wh{\mathcal{O}}\) is a Hermitian operator with eigenvalues (with discrete labels) \(\{\lambda_1,\lambda_2,\ldots\}\). By the spectral theorem \(\{\lambda_1,\lambda_2,\ldots\}\). Let us denote the corresponding eigenvectors by 
\[\ket{\lambda_1}, \ket{\lambda_2}, \ldots\]

(assuming for simplicity that each eigenvalue has just one corresponding eigenvector). Furthermore, imposing the normalisation condition on \(\ket{\lambda_k}\) we have \(\bra{\lambda_k} \ket{\lambda_k}=1\) thus, by the spectral theorem the set of \(\ket{\lambda_k}\) form an orthonormal basis of the Hilbert space. This means that \(\bra{\lambda_k}\ket{\lambda_j}=\delta_{jk}\) and any vector \(\ket{\Psi} \in \mathcal{H}\) can be written as 
\[\ket{\Psi} = \sum_{k=1}^{\infty} c_k \ket{\lambda_k} \quad \text{where } c_k = \bra{\lambda_k} \ket{\Psi}.\]

\begin{mdexample}
    The Hamiltonian operator, \(\wh{H} = \frac{\wh{p}}{2m}\) acting \(L^2([0,L])\) has eigenvectors
    \[\ket{\Psi_n} \to \Psi_n(x) = \sqrt{\frac{2}{L}} \sin\left( \frac{\pi x}{L} n\right)\]
    with corresponding eigenvalues
    \[\ket{E_n} \to E_n =\frac{\hbar^2 \pi^2 n^2}{2L^2 m}.\]
    Therefore, by the spectral theorem the eigenvectors \(\ket{\Psi_n}\) form an orthonormal basis of \(\wh{H}\).
\end{mdexample}

\begin{mdremark}
    Above, we assume the multiplicity of each eigenvalue to be \(1\), (by multiplicity we mean how many eigenvectors are associated to each eigenvalue). If the number of eigenvectors with eigenvalue \(\lambda_k\) is \(n_k\) then, we can denote them by \(\ket{\lambda_k,a}\) where \(a \in \{1,\ldots, n_k\}\). In general, we have 
    \[\bra{\lambda_i, a} \ket{\lambda_k,b}=0 \quad \text{for } i \neq k.\]
    If \(i=k\), these inner products are not constrained and depend on the choice of \(\ket{\lambda_k,a}\) for fixed \(k\). We can choose an orthonormal basis of eigenvectors for a given eigenvalue \(\lambda_k\),
    \[\bra{\lambda_k,a} \ket{\lambda_k,b} = \delta_{ab}\]
    so, all together we have 
    \[\bra{\lambda_i,a} \ket{\lambda_k,b} =\delta_{ik} \delta_{ab}.\]
    Thus, any vector can be written as 
    \[\ket{\Psi} = \sum_{i=1}^{\infty} \sum_{a=1}^{n_i} c_{i,a} \ket{\lambda_i,a} \quad \text{where } c_{i,a} = \bra{\lambda_i,a} \ket{\Psi}.\]
\end{mdremark}

\subsection{Continuous spectrum}

The spectrum of \(\wh{\mathcal{O}}\) can also be continuous. For example, it could be the interval 
\[[\lambda_{\min}, \lambda_{\max}] \subset \RR.\]

Again, denote the corresponding eigenvectors by \(\lambda\) with \(\lambda \in [\lambda_{\min}, \lambda_{\max}]\). Imposing the normalisation condition we have 

\[\bra{\lambda} \ket{\lambda'} = \delta(\lambda-\lambda')\]

hence, by the spectral theorem each \(\ket{\lambda}\) forms delta-normalisable basis of \(\mathcal{H}\) so, any \(\ket{\Psi} \in \mathcal{H}\) can be written as 

\[\ket{\Psi} = \int_{\lambda_{\min}}^{\lambda_{\max}} c(\lambda) \ket{\lambda} \, d\lambda \quad \text{where } c(\lambda)=\bra{\lambda} \ket{\Psi}.\]

\begin{mdexample}
    Consider the momentum operator, \(\wh{p} = -i \hbar \diffp{}{x}\) acting on \(L^2(\RR)\). We evaluate the spectrum of \(\wh{p}\):
    \[\begin{aligned}
        \wh{p} \ket{\Psi} = \lambda \ket{\Psi} &\iff -i\hbar \diffp{\Psi(x)}{x} = \lambda \Psi(x) \\
        &\then \Psi(x) = C_{\lambda} e^{\frac{i\lambda}{\hbar}x}.
    \end{aligned}\]
    We have two ways of determining the spectrum: 
    \begin{itemize}
        \item we know \(\wh{p}\) is a Hermitian operator, so it has real eigenvalues;
        \item if \(\lambda \in \CC\) then the exponential term diverges when imposing the normalisation condition thus, it is not useful on the other hand, when \(\lambda \in \RR\) \(\Psi(x)\) oscillates and can be delta-normalised.
    \end{itemize}
    In conclusion, the eigenvalues of \(\wh{p}\) must be real. Therefore, the spectrum of \(\wh{p}\) is whole real line i.e.
    \[\text{spectrum} = \RR.\]
    We know, check the delta-normalisation condition:
    \[\begin{aligned}
        \bra{\lambda} \ket{\lambda'} &= \int_{-\infty}^{\infty} \left( C_{\lambda} e^{\frac{i\lambda}{\hbar}x} \right)^* \left( C_{\lambda'} e^{\frac{i\lambda}{\hbar}x} \right) \, dx \\
        &= \int_{-\infty}^{\infty} C_{\lambda}^* C_{\lambda'} e^{ix\frac{(\lambda'-\lambda)}{\hbar}} \, dx.
    \end{aligned}\]
    Using the identity \(\int_{-\infty}^{\infty} e^{ikx} \, dx =2\pi \delta(k)\) and making the change of variable \(x=\hbar y\) we have that 
    \[\begin{aligned}
        \bra{\lambda} \ket{\lambda'} &= C_{\lambda}^* C_{\lambda'} \int_{-\infty}^{\infty} e^{iy(\lambda'-\lambda)} \, dy \\
        &= C_{\lambda}^* C_{\lambda'} \, \hbar \, 2\pi \, \delta(\lambda'-\lambda).
    \end{aligned}\]
    Therefore, we set 
    \[\ket{\lambda} = \frac{1}{\sqrt{2\pi \hbar}} e^{\frac{i\lambda}{\hbar}x}\]
    so that it is delta-normalised.
\end{mdexample}

\begin{mdremark}
    It is common to denote the eigenvector of an operator by the label of the operator. For example, the eigenvectors of \(\wh{p}\) would be denoted by \(\ket{p}\); similarly the eigenvalues of \(\wh{p}\) are denoted by \(p\). As such we have expression as these:
    \begin{itemize}
        \item \(\wh{p} \ket{p} = p \ket{p}\),
        \item \(\bra{p} \ket{p'} = \delta(p-p')\).
    \end{itemize}
    Therefore, a basis of the momentum operator can be written as 
    \[\ket{\Psi} = \int_{-\infty}^{\infty} f(p) \ket{p} \, dp \quad \forall \ket{p} \in \mathcal{H},\]
    as a function 
    \[\Psi(x) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \hbar}} f(p) e^{\frac{ip}{\hbar}x} \, dp.\]
\end{mdremark}

\begin{mdremark}
    Similarly to the discrete case, we could have multiple eigenvectors for a given eigenvalue \(\lambda\), which we could label as \(\ket{\lambda,a}\) where \(a \in \{1,\ldots, n_{\lambda}\}\). In this case, the above would generalise to 
    \[\begin{aligned}
        \bra{\lambda,a} \ket{\lambda',b} &= \delta_{ab} \delta(\lambda-\lambda'), \\
        \ket{\Psi} = \int_{\min}^{\max} \sum_{a=1}^{n_{\lambda}} c(\lambda,a) \, \ket{\lambda} \, d\lambda, &\quad \text{where } c(\lambda,a) = \bra{\lambda,a} \ket{\Psi}.
    \end{aligned}\]
\end{mdremark}

\begin{mdremark}
    The spectrum can be a mixture of discrete and continuous eigenvalues. For example,
    \[\text{spectrum} = [\lambda_{\min},\lambda_{\max}] \cup \{\lambda_1,\lambda_2,\ldots\}.\]
    In this case we have 
    \[\ket{\Psi} = \sum_{i=1}^{\infty} \sum_{a=1}^{n_i} c_{i,a} \ket{\lambda_i,a}+\int_{\lambda_{\text{min}}}^{\lambda_{\text{max}}} \sum_{a=1}^{n_{\lambda}} c(\lambda,a) \ket{\lambda} \, d\lambda.\]
\end{mdremark}

\subsection{Measurements}

A key axiom of quantum theory is that if a state \(\ket{\Psi}\) is an eigenvector of an operator \(\wh{\mathcal{O}}\) with eigenvalue \(\lambda\), 
\[\wh{\mathcal{O}} \ket{\Psi} = \lambda \ket{\Psi}\]
then, measuring the observable described by \(\wh{\mathcal{O}}\) in the state \(\ket{\Psi}\) will always give the result \(\lambda\). Thus, we say \(\wh{\mathcal{O}}\) has the definite value \(\lambda\) in the state \(\ket{\Psi}\).

Suppose, we carry out a measurement in a more general state \(\ket{\Psi}\) which is not an eigenvector of \(\wh{\mathcal{O}}\). In general, the measurement will cause the system to transition from \(\ket{\Psi}\) to some new state \(\ket{\Phi}\). Assume, the outcome of this measurement is some number \(\lambda\). We employ the following physical intuition: if \textit{immediately} after performing the measurement of \(\wh{\mathcal{O}}\) we perform a second measurement of \(\wh{\mathcal{O}}\), we should always get the same result \(\lambda\) (that is, the result of the first measurement may be random, but the result of the second measurement must be the same as the first). The first measurement happens in the state \(\ket{\Psi}\), which transitions into a state \(\ket{\Phi}\), as such the second measurement happens in the state \(\ket{\Phi}\). By the above, we expect that \(\wh{\mathcal{O}}\) has a definite value in the state \(\ket{\Phi}\) thus, we expect \(\ket{\Phi}\) to be an eigenvector of \(\wh{\mathcal{O}}\) with eigenvalue \(\lambda\), i.e. 

\[\wh{\mathcal{O}} \ket{\Phi} = \lambda \ket{\Phi}.\]

In conclusion: the only possible outcomes of measuring \(\wh{\mathcal{O}}\) are the eigenvalues of \(\wh{\mathcal{O}}\); if we measure \(\wh{\mathcal{O}}\) in a state \(\ket{\Psi}\) and get a result \(\lambda\) then, the system will have transitioned to a state \(\ket{\Phi}\) which is an eigenvector of \(\wh{\mathcal{O}} \) with the eigenvalue \(\lambda\) in the process of the measurement.

\subsection{Probabilities}

Let \(\wh{\mathcal{O}}\) be a Hermitian operator that describes a physical observable. Since \(\wh{\mathcal{O}}\) is a Hermitian operator, by the spectral theorem the eigenvectors of \(\wh{\mathcal{O}}\) form a complete basis. Assume for simplicity that the spectrum is discrete and that each eigenvalue has multiplicity \(1\). Thus, we can write any \(\ket{\Psi}\) as 
\[\ket{\Psi} = \sum_{i=1}^{\infty} c_i \ket{\lambda_i},\]

where \(\ket{\lambda_i}\) are the normalised eigenvectors of \(\wh{\mathcal{O}}\) corresponding to the eigenvalues \(\lambda_i\). If \(\ket{\Psi}\) is normalised i.e. \(\bra{\Psi} \ket{\Psi} =1\), implies that 

\[\begin{aligned}
    1 &= \bra{\Psi} \ket{\Psi} \\
    &= \sum_{\substack{i=1 \\ j = 1}}^{\infty} c^*_j c_i \bra{\lambda_j} \ket{\lambda_i} \\
    &= \sum_{i=1}^{\infty} \abs{c_i}^2.
\end{aligned}\]

As such, we interpret the numbers \(\abs{c_i}^2\) as the probabilities of obtaining the result \(\lambda_i\) when measuring \(\wh{\mathcal{O}}\) in the state \(\ket{\Psi}\).

\begin{mdthm}
    If \(\ket{\Psi}\) is normalised we can write \(\ket{\Psi} = \sum_{i=1}^{\infty} c_i \ket{\lambda_i},\)
    and we interpret the numbers 
    \[\abs{c_i}^2\]
    as the probabilities of obtaining the result \(\lambda_i\) when measuring \(\wh{\mathcal{O}}\) in the state \(\ket{\Psi}\). The numbers \(c_i\) are called the \textbf{amplitudes} of \(\ket{\Psi}\).
\end{mdthm}

\begin{mdthm}
    In the case of a continuous spectrum, the coefficients represent the probability density.
\end{mdthm}

\begin{example}
    Consider the position operator \(\wh{x}\); we evaluate the eigenvalues and eigenvectors. Recall, the action of \(\wh{x}\) is 
    \[(\wh{x} \Psi)(y)= y\Psi(y).\]
    We solve the eigenvalue equation 
    \[\begin{aligned}
        \wh{x} \ket{\Psi} &= \lambda \ket{\Psi} \\
        \then x\Psi(x) &= \lambda \Psi(x) \\
        \then (x-\lambda) \Psi(x) =0. 
    \end{aligned}\]
    If \(\lambda \in \CC\) and \(\lambda \not\in \RR\) then \((x-\lambda) \neq 0\) which implies \(\Psi(x)=0\). Similarly, if \(\lambda \in \RR\) and \(x \neq \lambda\) then, \(\Psi(x)=0\). We conclude, \(\lambda \in \RR\) and \(x \neq \lambda\) so, we set 
    \[\Psi(x) =\delta(x-\lambda).\]
    Denoting the corresponding vector by \(\ket{\lambda}\), we compute 
    \[\bra{\lambda} \ket{\lambda} = \int_{-\infty}^{\infty} \delta(x-\lambda)\delta(x-\lambda')\, dx.\]
    To evaluate this integral we recall the identity
    \[\int_{-\infty}^{\infty} \delta(x-y)f(x) \, dx =f(y)\]
    so, setting \(f(x)=\delta(x-\lambda')\) we have 
    \[\begin{aligned}
        \bra{\lambda} \ket{\lambda} &= \int_{-\infty}^{\infty} \delta(x-\lambda)\delta(x-\lambda')\, dx \\
        &= \delta(\lambda-\lambda').
    \end{aligned}\]
    Therefore, \(\ket{\lambda}\) form a delta-normalisable basis. Since, we could choose \(\lambda\) to be any real number, the spectrum of \(\wh{x}\) is \(\RR\).
    
    Now that we have a basis each vector \(\ket{\Psi} \in \mathcal{H}\) can be expressed as 
    \[\ket{\Psi} = \int_{-\infty}^{\infty} c(\lambda) \ket{\lambda} \, d\lambda.\]
    In terms of the wave function this becomes 
    \[\begin{aligned}
        \Psi(x) &= \int_{-\infty}^{\infty} c(\lambda) \delta(x-\lambda) \, d\lambda \\
        &= c(\lambda).
    \end{aligned}\]
\end{example}

Therefore, the original expression in terms of vectors can be written as 
\[\ket{\Psi} = \int_{-\infty}^{\infty} \Psi(\lambda) \ket{\lambda}\]

however, it is common to rename the integration variable \(\lambda \to x\), so that we have 

\[\ket{\Psi} = \int_{-\infty}^{\infty} \Psi(x) \ket{x} \, dx.\]

We conclude the coefficients, \(c(x)\), of the expansion of \(\ket{\Psi}\) into the basis of the eigenvectors of \(\wh{x}\) are given by the wave function, i.e. 
\[c(x)= \Psi(x).\]

By the theorem above we have that 
\[\abs{c(x)}^2 = \abs{\Psi(x)}^2\]

gives the probability density of obtaining the result \(x\) after measuring \(\wh{x}\), which is indeed how we interpreted the wave function before.

\begin{example}
    Suppose \(\mathcal{H} =\CC^2\), since this is a finite dimensional Hilbert space we can represent the operators as matrices. Consider the operator 
    \[\wh{\mathcal{O}} = \begin{pmatrix} 0 & 1 \\ 1 & 0\end{pmatrix},\]
    this operator is clearly Hermitian, as \(\wh{\mathcal{O}}^{\dagger}=\wh{\mathcal{O}}\). As per our discussion above we are interested in the eigenvalues and corresponding eigenvectors of a Hermitian operator.
    \begin{itemize}
        \item Finding the eigenvalues of \(\wh{\mathcal{O}}\):
        \[\begin{aligned}
            \det\begin{pmatrix} -\lambda & 1 \\ 1 & -\lambda\end{pmatrix} &= 0 \\
            \then \lambda^2-1&=0 \\
            \then \lambda &= \pm 1.
        \end{aligned}\]
        Therefore, the spectrum of \(\wh{\mathcal{O}}\) is given by \(\{-1,+1\}\).
        \item Finding the eigenvectors of \(\wh{\mathcal{O}}\).
        \begin{itemize}
            \item For \(\lambda =1\):\\
            recall the eigenvector equation 
            \[\wh{\mathcal{O}} \ket{\Psi} = \lambda \ket{\Psi},\]
            we choose our ansatz as 
            \[\ket{\Psi} = \begin{pmatrix} a \\b\end{pmatrix}.\]
            Substituting into the ansatz into eigenvector equation we have
            \[\begin{aligned}
                \begin{pmatrix} 0 & 1 \\ 1 & 0\end{pmatrix}\begin{pmatrix} a \\b\end{pmatrix} &= 1 \cdot \begin{pmatrix} a \\b\end{pmatrix} \\
                \begin{pmatrix} b \\a\end{pmatrix} &= \begin{pmatrix} a \\b\end{pmatrix}.
            \end{aligned}\]
            We conclude that \(a=b\) thus, 
            \[\ket{\Psi} = \begin{pmatrix} a \\a\end{pmatrix}.\]
            To fix \(a\) we impose that \(\ket{\Psi}\) must be normalised so,
            \[\begin{aligned}
                1 &= \bra{\Psi}\ket{\Psi} \\
                &= \left( \ket{\Psi} \right)^{\dagger} \ket{\Psi} \\
                &= \begin{pmatrix} a^* & a^* \end{pmatrix} \begin{pmatrix} a \\a\end{pmatrix} \\
                &= 2\abs{a}^2.
            \end{aligned}\] 
            We choose \(a = \frac{1}{\sqrt{2}}\) (we could have chosen any value \(a \in \CC\) such that \(2\abs{a}^2=1\)). We conclude that 
            \[\ket{\Psi} = \frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\1\end{pmatrix}.\]
            By the discussion above, we recall that the eigenvector with eigenvalue \(\lambda\) is denoted by \(\ket{\lambda}\) so, this eigenvector is denoted as 
            \[\ket{+1} =\frac{1}{\sqrt{2}} \begin{pmatrix} 1 \\1\end{pmatrix}.\]
            \item For \(\lambda = -1\):
            By a similar process as above we conclude 
            \[\ket{-1} = \frac{1}{\sqrt{2}} \begin{pmatrix} 1\\-1 \end{pmatrix}.\]
        \end{itemize}
    \end{itemize}
    By the spectral theorem we know the eigenvectors, \(\ket{+1}\) and \(\ket{-1}\) form a basis of \(\wh{\mathcal{O}}\):
    \[\ket{\Psi} = c_{+1} \ket{+1}+ c_{-1} \ket{-1}.\]
    We recall that the coefficients are given as follows:
    \[\begin{aligned}
        c_{+1} &= \bra{+1}\ket{\Psi} \\
        &= \frac{1}{\sqrt{2}} \begin{pmatrix} 1 & 1\end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \\
        &= \frac{1}{\sqrt{2}},
    \end{aligned}\]
    and
    \[\begin{aligned}
        c_{-1} &= \bra{-1}\ket{\Psi} \\
        &= \frac{1}{\sqrt{2}} \begin{pmatrix} 1 & -1\end{pmatrix} \begin{pmatrix} 1 \\ 0 \end{pmatrix} \\
        &= \frac{1}{\sqrt{2}}
    \end{aligned}\]
    so,
    \[\ket{\Psi} = \frac{1}{\sqrt{2}} \ket{+1}+ \frac{1}{\sqrt{2}} \ket{-1}.\]
    Suppose we measure \(\wh{\mathcal{O}}\) in the state \(\ket{\Psi}\), and we obtain \(\lambda=+1\) then, the state changes from 
    \[\ket{\Psi} \to \ket{+1},\]
    and the probability of obtaining \(\lambda=+1\) is 
    \[\abs{\frac{1}{\sqrt{2}}}^2 = \half.\]
\end{example}

\section{Unitary and time evolution}

Recall the time-dependent Schrödinger equation, for a wave function which depends on both \(x\) and \(t\):
\[\begin{aligned}
    i\hbar \diffp{}{t}\Psi(x,t) &= \left( \wh{H}\Psi \right)(x,t) \\
    &= -\frac{\hbar^2}{2m} \diffp[2]{}{x}\Psi(x,t) +U(x) \Psi(x,t).
\end{aligned}\]

We now want to interpret this equation with the bra-ket notation, for this we need a Hilbert space.

\begin{mdthm}
    The time-dependent Schrödinger equation for any \(\mathcal{H}\) is given by 
    \[i\hbar \diffp{}{t} \ket{\Psi(t)} = \wh{H} \ket{\Psi(t)}.\]
\end{mdthm}

\begin{mdremark}
    We can write the Schrödinger equation in this form for any \(\mathcal{H}\) in which the operator \(\wh{H}\) is defined.
\end{mdremark}

The Schrödinger is powerful as by just an initial value for \(\ket{\Psi(t)}\) we can determine \(\ket{\Psi}\) at a (small) later time. Consider \(\ket{\Psi(t+\delta t)}\) for small \(\delta t\) then, by the Taylor expansion we have
\[\begin{aligned}
    \ket{\Psi(t+\delta t)} &\approx \ket{\Psi(t)} +\delta t \diffp{}{t}\ket{\Psi(t)} \\
    &\approx \ket{\Psi(t)} - \delta t\frac{i}{\hbar}\wh{H} \ket{\Psi(t)}.
\end{aligned}\]

\begin{example}
    Suppose \(\mathcal{H} = \CC\) then, \(\ket{\Psi}\to \Psi(t) \in \CC\) (i.e. it is a number) and similarly \(\wh{H}\) becomes a \(1 \times 1\) matrix (again just a number in \(\CC\)). As such we can solve the Schrödinger equation:
    \[i\hbar \diffp{}{t} \Psi(t) = H\Psi(t),\]
    and we have 
    \[\Psi(t) = Ce^{\frac{-iH}{\hbar}t}.\]
    To fix the constant \(C\) we need a condition for \(\Psi\) so, we suppose that at \(t=0\) we have \(\Psi(0)\). Then, \(C=\Psi(0)\) and we have 
    \[\Psi(t) = \Psi(0)e^{\frac{-iH}{\hbar}t}\]
\end{example}

\begin{mdthm}
    The time-dependent Schrödinger equation is solved by 
    \[\ket{\Psi(t)} = e^{-\frac{i\wh{H}}{\hbar}t} \ket{\Psi(0)}.\]
\end{mdthm}

\begin{mdremark}
    We can think of the \(e^{-\frac{i\wh{H}}{\hbar}t}\) term as adding time \(t\) to \(\ket{\Psi(0)}\).
\end{mdremark}

\section{Unitary operators and time evolution}

\subsection{Exponential of operators}

\begin{definition}
    If \(\wh{\mathcal{O}}\) is an operator then we define
    \[\begin{aligned}
        e^{\wh{\mathcal{O}}} &= \sum_{n=0}^{\infty} \frac{\wh{\mathcal{O}}^n}{n!} \\
        &= \mathbbm{1}+ \wh{\mathcal{O}}+ \frac{\wh{\mathcal{O}}^2}{2!} + \cdots + \frac{\wh{\mathcal{O}}^n}{n!} +\cdots,
    \end{aligned}\]
    where \(\mathbbm{1}\) represents the identity operator.
\end{definition}

\begin{mdnote}
    This definition is obtained from the Taylor series of \(e^x\).
\end{mdnote}

\begin{example}
    Suppose \(\mathcal{H}=\CC^2\) and let \(\wh{\mathcal{O}} = \begin{pmatrix} 0 & 1 \\ 0 & 0\end{pmatrix}\) then,
    \[\begin{aligned}
        e^{\wh{\mathcal{O}}} &= \mathbbm{1} + \begin{pmatrix} 0 & 1 \\ 0 & 0\end{pmatrix} + \half \begin{pmatrix} 0 & 1 \\ 0 & 0\end{pmatrix}\begin{pmatrix} 0 & 1 \\ 0 & 0\end{pmatrix} +\cdots \\
        &= \begin{pmatrix} 1 &0 \\ 0 & 1\end{pmatrix} + \begin{pmatrix} 0 & 1 \\ 0 & 0\end{pmatrix} \\
        &= \begin{pmatrix} 1 & 1 \\ 0 & 1\end{pmatrix}.
    \end{aligned}\]
    Since, we are in a finite dimensional vector space we can identify operators with matrices thus, \(\mathbbm{1}\) represents the identity matrix. Furthermore, we notice that \(\wh{\mathcal{O}}^2 = \begin{pmatrix} 0 & 0 \\ 0 & 0\end{pmatrix}\) which implies \(\wh{\mathcal{O}}^n=\begin{pmatrix} 0 & 1 \\ 0 & 0\end{pmatrix}\) as well.
\end{example}

\begin{mdexample}
    Suppose \(\mathcal{H}=\CC^2\) and let \(\wh{\mathcal{O}} = \begin{pmatrix} 1 & 0 \\ 0 & 2\end{pmatrix}\) then 
    \[\begin{aligned}
        e^{t\wh{\mathcal{O}}} &= \begin{pmatrix} 1 & 0 \\ 0 & 1\end{pmatrix} + t\begin{pmatrix} 1 & 0 \\ 0 & 2\end{pmatrix} +\cdots + \frac{t^n}{n!}\begin{pmatrix} 1 & 0 \\ 0 & 2\end{pmatrix}^n +\cdots \\
        &= \begin{pmatrix} 1 & 0 \\ 0 & 1\end{pmatrix} + \begin{pmatrix} t & 0 \\ 0 & 2t\end{pmatrix} +\cdots + \begin{pmatrix} \frac{t^n}{n!} & 0 \\ 0 & \frac{(2t)^n}{n!}\end{pmatrix}\ +\cdots
    \end{aligned}\]
    Consider the first entry of \(e^{t\wh{\mathcal{O}}}\):
    \[1+t+\frac{t^2}{2!} +\cdots \frac{t^n}{n!}+\cdots = e^t\]
    therefore,
    \[e^{t\wh{\mathcal{O}}} =\begin{pmatrix} e^t & 0 \\ 0 & e^{2t}\end{pmatrix}.\]
    Generally, if \(\wh{\mathcal{O}} = \begin{pmatrix} a & 0 \\ 0 & b\end{pmatrix}\) then,
    \[e^{t\wh{\mathcal{O}}} = \begin{pmatrix} e^{at} & 0 \\ 0 & e^{bt} \end{pmatrix}.\]
\end{mdexample}

\begin{mdthm}
    Let \(\wh{\mathcal{O}}\) be a Hermitian operator and let \(\{\lambda_n\}_{n=1}^{\infty}\) be the eigenvector basis of \(\wh{\mathcal{O}}\). We define \(e^{\alpha \wh{\mathcal{O}}}\) by its action on all \(\ket{\Psi} \in \mathcal{H}\): for all 
    \[\ket{\Psi} = \sum_{n=1}^{\infty} c_n \ket{\lambda_n}\]
    with 
    \[e^{\alpha \wh{\mathcal{O}}} \ket{\Psi} = \sum_{n=1}^{\infty} c_n e^{\alpha \lambda_n} \ket{\lambda_n}.\]
\end{mdthm}

\begin{proof}
    First notice that \(\wh{\mathcal{O}}^n \ket{v_k}= \lambda_k^n \ket{\lambda_k}\) then, using the Taylor series definition we have:
    \[\begin{aligned}
        e^{\alpha \wh{\mathcal{O}}} &= \left( \mathbbm{1}+\alpha \wh{\mathcal{O}} + \frac{(\alpha \wh{\mathcal{O}})^2}{2!} +\cdots \right)\ket{\Psi} \\
        &= \left( \mathbbm{1}+\alpha \wh{\mathcal{O}} + \frac{(\alpha \wh{\mathcal{O}})^2}{2!} +\cdots \right) \sum_{n=1}^{\infty} c_n \ket{\lambda_n} \\
        &= \sum_{n=1}^{\infty} c_n \left( \mathbbm{1}+\alpha \wh{\mathcal{O}} + \frac{(\alpha \wh{\mathcal{O}})^2}{2!} +\cdots \right) \ket{\lambda_n} \\
        &= \sum_{n=1}^{\infty} c_n \left( 1 +\alpha \lambda_n + \frac{(\alpha \lambda_n)^2}{2!} + \cdots \right) \ket{\lambda_n} \\
        &= \sum_{n=1}^{\infty} c_n e^{\alpha \lambda_n} \ket{\lambda_n}.
    \end{aligned}\]
\end{proof}

\begin{mdremark}
    For simplicity, we have assumed the spectrum of \(\wh{\mathcal{O}}\) is discrete and that all the eigenvalues have multiplicity \(1\).
\end{mdremark}

\begin{mdthm}[Properties of exponentials of operators]
    For any operator \(\wh{\mathcal{O}}\) in a Hilbert space \(\mathcal{H}\) and \(\alpha \in \CC\) we have:
    \begin{itemize}
        \item \(\diffp{}{\alpha}e^{\alpha \wh{\mathcal{O}}}= \wh{\mathcal{O}}e^{\alpha \wh{\mathcal{O}}} = e^{\alpha \wh{\mathcal{O}}}\wh{\mathcal{O}}\).
        \item \(e^{\alpha \wh{\mathcal{O}}} e^{\beta \wh{\mathcal{O}}} = e^{(\alpha+\beta) \wh{\mathcal{O}}}\).
    \end{itemize}
\end{mdthm}

\begin{proof}
    We prove each point.
    \begin{itemize}
        \item By considering the Taylor series of \(e^{\alpha\wh{\mathcal{O}}}\):
        \[\begin{aligned}
            \diffp{}{\alpha} e^{\alpha \wh{\mathcal{O}}} &= \diffp{}{\alpha} \left( \mathbbm{1} + \alpha \wh{\mathcal{O}}+ \frac{(\alpha \wh{\mathcal{O}})^2}{2!} + \cdots \right) \\
            &= 0 + \wh{\mathcal{O}} +\alpha \wh{\mathcal{O}}^2+ \cdots \\
            &= \wh{\mathcal{O}} \left( \mathbbm{1}+ \alpha \wh{\mathcal{O}}+ \frac{(\alpha \wh{\mathcal{O}})^2}{2!} +\cdots \right) \\
            &=\left( \mathbbm{1}+ \alpha \wh{\mathcal{O}}+ \frac{(\alpha \wh{\mathcal{O}})^2}{2!} +\cdots \right) \wh{\mathcal{O}} \\
            &= \wh{\mathcal{O}}e^{\alpha \wh{\mathcal{O}}} \\
            &= e^{\alpha \wh{\mathcal{O}}} \wh{\mathcal{O}}.
        \end{aligned}\]
        \item We present an easier proof. Suppose \(\{\ket{\lambda_n}\}\) is a basis of \(\wh{\mathcal{O}}\) then, we compute 
        \[\begin{aligned}
            e^{\alpha \wh{\mathcal{O}}} e^{\beta \wh{\mathcal{O}}} \ket{\Psi} &= e^{\alpha \wh{\mathcal{O}}} e^{\beta \wh{\mathcal{O}}} \sum_{n=1}^{\infty} c_n \ket{\lambda_n} \\
            &= e^{\alpha \wh{\mathcal{O}}} \sum_{n=1}^{\infty} c_n e^{\beta \lambda_n} \ket{\lambda_n} \\
            &= \sum_{n=1}^{\infty} c_n e^{\beta \lambda_n} e^{\alpha \lambda_n} \ket{\lambda_n} \\
            &= \sum_{n=1}^{\infty} c_n e^{(\alpha+\beta)\lambda_n} \ket{\lambda_n} \\
            &= e^{(\alpha+\beta)\wh{\mathcal{O}}}\sum_{n=1}^{\infty} c_n\ket{\lambda_n} \\
            &= e^{(\alpha+\beta) \wh{\mathcal{O}}} \ket{\Psi}.
        \end{aligned}\]
    \end{itemize}
\end{proof}

\subsection{Evolution operator}

In this section we verify that 
\[\ket{\Psi(t)} = e^{-\frac{i\wh{H}}{\hbar}t} \ket{\Psi(0)}\]

solves the time-dependent Schrödinger equation: 
\[i\hbar \diffp{}{t}\ket{\Psi(t)} = \wh{H} \ket{\Psi(t)}.\]

We compute 
\[\begin{aligned}
    \diffp{}{t}\ket{\Psi(t)} &= \diffp{}{t} e^{-\frac{i\wh{H}}{\hbar}t} \ket{\Psi(0)} \\
    &= -\frac{i\wh{H}}{\hbar} e^{-\frac{i\wh{H}}{\hbar}t} \ket{\Psi(0)}.
\end{aligned}\]
Clearly, 
\[\begin{aligned}
    i\hbar \left( -\frac{i\wh{H}}{\hbar} e^{-\frac{i\wh{H}}{\hbar}t} \ket{\Psi(0)} \right) &=  -i^2 \wh{H}e^{-\frac{i\wh{H}}{\hbar} t} \ket{\Psi(0)} \\
    &=\wh{H} \ket{\Psi}.
\end{aligned}\]

\begin{definition}
    We call the operator \(e^{-\frac{i\wh{H}}{\hbar}t}\) the \textbf{time evolution operator} and is denoted by 
    \[\wh{U}(t)=e^{-\frac{i\wh{H}}{\hbar}t}.\]
\end{definition}

\begin{mdthm}
    Properties of \(\wh{U}(t)\):
    \begin{itemize}
        \item \(\wh{U}(t_2)\wh{U}(t_1)=\wh{U}(t_1+t_2)\).
        \item \(\left( \wh{U}(t) \right)^{\dagger} = \wh{U}(-t)\).
    \end{itemize}
\end{mdthm}

\begin{proof}
    We split the proof in parts:
    \begin{itemize}
        \item By the definition of \(\wh{U}(t)\) the result follows:
        \[\begin{aligned}
            \wh{U}(t_2)\wh{U}(t_1) &= e^{-\frac{i\wh{H}}{\hbar}t_2} e^{-\frac{i\wh{H}}{\hbar}t_1} \\
            &= e^{-\frac{i\wh{H}}{\hbar}(t_1+t_2)} \\
            &= \wh{U}(t_1+t_2).
        \end{aligned}\]
        \item Firstly, note that \(\wh{H}\) is Hermitian and \(\left( \wh{\mathcal{O}}^n \right)^{\dagger} = \left( \wh{\mathcal{O}}^{\dagger} \right)^n\) Using the series definition we have:
        \[\begin{aligned}
            \left( \wh{U}(t) \right)^{\dagger} &= \left( e^{-\frac{i\wh{H}}{\hbar}t} \right) = \left( \mathbbm{1} + \left( -\frac{i\wh{H}}{\hbar} \right) + \half \left(  -\frac{i\wh{H}}{\hbar} \right)^2 + \frac{1}{3!} \left( -\frac{i\wh{H}}{\hbar} \right)^3+\cdots \right)^{\dagger} \\
            &= \mathbbm{1} + \left( -\frac{i\wh{H}}{\hbar} \right)^{\dagger} + \half \left( \left( -\frac{i\wh{H}}{\hbar} \right)^2 \right)^{\dagger} + \cdots \\
            &= \mathbbm{1} + \frac{i\wh{H}}{\hbar} + \half \left( \frac{i\wh{H}}{\hbar} \right)^2 +\cdots \\
            &= e^{\frac{i\wh{H}}{\hbar}t} \\
            &= \wh{U}(-t).
        \end{aligned}\]
    \end{itemize}
\end{proof}

\subsection{Unitary}

\begin{definition}
    An operator, \(\wh{A}\), which satisfies the property
    \[\wh{A}^{\dagger}\wh{A} = \mathbbm{1},\]
    where \(\mathbbm{1}\) is the identity operator, are called \textbf{unitary}.
\end{definition}

\begin{mdthm}
    The time evolution operator, \(\wh{U}(t)\) is unitary.
\end{mdthm}

\begin{proof}
    We know \(\left( \wh{U}(t) \right)^{\dagger}=\wh{U}(-t)\) therefore,
    \[\begin{aligned}
        \left( \wh{U}(t) \right)^{\dagger} \wh{U}(t) &= \wh{U}(-t)\wh{U}(t) \\
        &= \wh{U}(0) \\
        &=\mathbbm{1}.
    \end{aligned}\]
    (We have used \(\wh{U}(0)=e^0 = \mathbbm{1}\)).
\end{proof}

\begin{mdthm}
    Unitary operators preserve the inner product i.e.
    \[\left( \wh{U}(t) \ket{\Psi(t)},\wh{U}(t)\ket{\Phi(t)} \right) = (\ket{\Psi(t)},\ket{\Phi(t)}).\]
\end{mdthm}

\begin{proof}
    Consider two vectors \(\ket{\Psi},\ket{\Phi} \in \mathcal{H}\), by computing the inner product we have 
    \[\begin{aligned}
        \left( \wh{U}(t) \ket{\Psi(t)},\wh{U}\ket{\Phi(t)} \right) &= \left( [\wh{U}(t)]^{\dagger} \wh{U}(t)\ket{\Psi},\ket{\Phi} \right) \\
        &= (\ket{\Psi(t)},\ket{\Phi(t)}).
    \end{aligned}\]
\end{proof}

\begin{corollary}
    As a result we have 
    \[\norm{\,\ket{\Psi(t)}\,}^2= \norm{\,\ket{\Psi(0)}\,}^2;\]
    in particular if \(\norm{\,\ket{\Psi(0)}\,}^2=1\) then \(\norm{\,\ket{\Psi(t)}\,}^2=1\) for all \(t\).
\end{corollary}

\begin{proof}
    By definition 
    \[\begin{aligned}
        \norm{\,\ket{\Psi(t)}\,}^2 &= \bra{\Psi(t)} \ket{\Psi(t)} = (\ket{\Psi(t)},\ket{\Psi(t)}) \\
        &= \left( \wh{U}(t) \ket{\Psi(0)}, \wh{U}(t) \ket{\Psi(0)} \right) \\
        &= \left( \ket{\Psi(0)},\ket{\Psi(0)} \right) \\
        &= \norm{\,\ket{\Psi(0)}\,}^2.
    \end{aligned}\]
\end{proof}

\subsection{Evolution in the energy basis}

Suppose we are given the Hamiltonian \(\wh{H}\) and some initial state \(\ket{\Psi(0)}\), and we want to find the state \(\ket{\Psi(t)}\) at some \(t \in \RR\). We can do so by the following algorithm:
\begin{enumerate}
    \item Find the eigenvalues, \(E_n\), and the orthonormal basis of eigenvectors \(\{\ket{n}\}_{n=0}^{\infty}\) of \(\wh{H}\).
    \item Find the expression for \(\ket{\Psi(0)}\) in this basis of eigenvectors,
    \[\ket{\Psi(0)} = \sum_{n=0}^{\infty} c_n \ket{n},\]
    where \(c_n = \bra{n}\ket{\Psi(0)}\) if the basis \(\{\ket{n}\}\) is orthonormal.
    \item Act with \(\wh{U}(t)=e^{-\frac{i\wh{H}}{\hbar}t}\),
    \[\begin{aligned}
        \ket{\Psi(t)} &= \wh{U}(t) \ket{\Psi(0)} \\
        &= e^{-\frac{i\wh{H}}{\hbar}t}\sum_{n=0}^{\infty} c_n \ket{n} \\
        &= \sum_{n=0}^{\infty} c_n e^{-\frac{i E_n}{\hbar}t} \ket{n}.
    \end{aligned}\]
\end{enumerate}

\begin{mdremark}
    This algorithm is for the case where the spectrum of \(\wh{H}\) is discrete and all the eigenvalues have multiplicity \(1\).
\end{mdremark}

This gives us a concrete expression for \(\ket{\Psi(t)}\). Of course, this algorithm is not always simple to carry out: firstly, we require a complete solution of the time-independent Schrödinger equation, which is just the eigenvector equation for \(\wh{H}\),
\[\wh{H} \ket{n} = E_n \ket{n}.\]

\begin{example}
    In the case of a particle in a box we determined the eigenvalues to be 
    \[E_n = \frac{\pi^2 n^2 \hbar^2}{2mL^2}\]
    and the corresponding eigenvectors 
    \[\ket{E_n} \sim \Psi_n(x) = \sqrt{\frac{2}{L}} \sin\left( \frac{\pi n}{L}x \right).\]
    Therefore, 
    \[\Psi(x,t) = \sum_{n=1}^{\infty} c_n e^{-\frac{iE_n}{\hbar}t} \Psi_n(x),\]
    where 
    \[c_n = \int_{0}^L \Psi^*_n(x)\Psi(x,0) \, dx.\]
\end{example}

\section{Non-commutativity and uncertainty}

\subsection{Implications for measurements of operators}

Let \(\wh{A}\) and \(\wh{B}\) be Hermitian operators with non-vanishing commutator:
\[[\wh{A},\wh{B}]=\wh{C} \neq 0.\]

Here \(\wh{C}\) is a new operator (not necessarily Hermitian). We now ask: is it possible for \(\wh{A}\) and \(\wh{B}\) to simultaneously take definite values in a state \(\ket{\Psi}\)? In other words, can \(\ket{\Psi}\) be an eigenvector of \(\wh{A}\) and \(\wh{B}\) simultaneously? That is, is the following set of equations possible:
\[\begin{aligned}
    \wh{A} \ket{\Psi} &= a \ket{\Psi} \\
    \wh{B} \ket{\Psi} &= b \ket{\Psi},
\end{aligned}\]

for \(a,b \in \CC\). Suppose this is true then, we can compute 

\[\begin{aligned}
    \wh{C} \ket{\Psi} &= [\wh{A},\wh{B}] \ket{\Psi} \\
    &= (\wh{A}\wh{B}- \wh{B}\wh{A})\ket{\Psi} \\
    &= \wh{A}\left(\wh{B}\ket{\Psi} \right)- \wh{B} \left(\wh{A}\ket{\Psi}\right) \\
    &= b\wh{A}\ket{\Psi} -a\wh{B}\ket{\Psi} \\
    &= ba\ket{\Psi}-ab\ket{\Psi} \\
    &=0.
\end{aligned}\]

Therefore, \(\ket{\Psi}\) is an eigenvector of \(\wh{A}\) and \(\wh{B}\) simultaneously if and only if \(\wh{C}\ket{\Psi}=0\), i.e. \(\wh{A}\) and \(\wh{B}\) commute.

\subsection{Heisenberg's uncertainty principle}

\begin{definition}
    The \textbf{uncertainty} of a \underline{normalised} Hermitian operator \(\wh{A}\), denoted by \(\Delta \wh{A}\), is defined by 
    \[\begin{aligned}
        ( \Delta \wh{A} )^2 &= \left\langle \left(\wh{A}-\langle \wh{A} \rangle\right)^2 \right\rangle \\
        &= \bra{\Psi} \left( \wh{A}-\langle \wh{A}\rangle \right)^2 \ket{\Psi}.
    \end{aligned}\]
\end{definition}

\begin{mdnote}
    Operators need to be Hermitian as these are the operators which are used to measure quantities in Quantum Mechanics.
\end{mdnote}

\begin{theorem}
    The uncertainty of an operator \(\wh{A}\) is real and non-negative i.e. \(\Delta \wh{A} \geq 0\).
\end{theorem}

\begin{proof}
    We have 
    \[\begin{aligned}
        (\Delta \wh{A})^2 &= \bra{\Psi} \left( \wh{A}-\langle \wh{A}\rangle \right)^2 \ket{\Psi} \\
        &= \bra{\Psi} (\wh{A}-\langle \wh{A}\rangle) (\wh{A}-\langle \wh{A}\rangle) \ket{\Psi} \\
        &= \left[ (\wh{A}-\langle \wh{A}\rangle) \ket{\Psi} \right]^{\dagger} (\wh{A}-\langle \wh{A}\rangle) \ket{\Psi} \\
        &= \norm{ \left(\wh{A}-\langle \wh{A}\rangle \right) \ket{\Psi}}^2 \\
        &\geq 0
    \end{aligned}\]
    by the axioms of norms. In the last equality we have used the fact that \(\wh{A}\) is a Hermitian operator.
\end{proof}

\begin{corollary}
    Let \(\wh{A}\) be an operator then, \(\Delta \wh{A} =0\) in a state \(\ket{\Psi}\) if and only if \(\ket{\Psi}\) is an eigenvector of \(\wh{A}\).
\end{corollary}

\begin{proof}
    Recall the definition of uncertainty
    \[\begin{aligned}
        (\Delta \wh{A})^2 &= \bra{\Psi} \left( \wh{A}-\langle \wh{A}\rangle \right)^2 \ket{\Psi} \\
        &= \norm{ \left(\wh{A}-\langle \wh{A}\rangle \right) \ket{\Psi}}^2,
    \end{aligned}\]
    by the axioms of the norm it is \(0\) if and only if 
    \[\left( \wh{A}- \langle \wh{A} \rangle \right) \ket{\Psi} =0.\]
    We can write this condition as
    \[\wh{A} \ket{\Psi} = \langle \wh{A} \rangle \ket{\Psi},\]
    i.e. \(\ket{\Psi}\) is an eigenvector (since \(\langle \wh{A} \rangle\) is a real number).
\end{proof}

\begin{mdthm}
    Let \(\wh{A}\) and \(\wh{B}\) be Hermitian operators and \(\Delta \wh{A}\) and \(\Delta \wh{B}\) their respective uncertainty in a state \(\ket{\Psi}\). Then, we have
    \[\Delta \wh{A} \, \Delta \wh{B} \geq \half \abs{ \left\langle \left[\wh{A},\wh{B}\right] \right\rangle }.\]
\end{mdthm}

\begin{proof}
    Define 
    \[\wh{A}_0 = \wh{A}-\langle \wh{A} \rangle \quad \text{and}\quad \wh{B}_0 = \wh{B}-\langle \wh{B} \rangle.\] 
    We first compute 
    \[\begin{aligned}
        [\wh{A}_0,\wh{B}_0] &= [\wh{A}-\langle \wh{A}\rangle,\wh{B}-\langle \wh{B}\rangle] \\
        &= [\wh{A},\wh{B}-\langle \wh{B}\rangle]-[\langle \wh{A} \rangle, \wh{B}-\langle \wh{B}\rangle] \\
        &= [\wh{A},\wh{B}]-[\wh{A},\langle \wh{B}\rangle]- \left( [\langle \wh{A}\rangle,\wh{B}]-[\langle \wh{A}\rangle,\langle \wh{B}\rangle] \right) \\
        &= [\wh{A},\wh{B}]+[\langle \wh{B}\rangle,\wh{A}]+[\wh{B},\langle \wh{A}\rangle] + [\langle \wh{A}\rangle,\langle \wh{B}\rangle].
    \end{aligned}\]
    Notice that \(\langle \wh{A}\rangle\) and \(\langle \wh{B}\rangle\) are real numbers since \(\wh{A}\) and \(\wh{B}\) are Hermitian operators thus, their commutators with anything are zero. As such we have that 
    \[[\wh{A}_0,\wh{B}_0]=[\wh{A},\wh{B}].\]
    Furthermore, it follows that
    \[(\Delta \wh{A})^2 = \langle \wh{A}_0^2 \rangle \quad \text{and} \quad (\Delta \wh{B})^2 = \langle \wh{B}_0^2 \rangle.\]
    Consider
    \[\begin{aligned}
        (\Delta \wh{A})^2 (\Delta \wh{B})^2 &= \bra{\Psi} \wh{A}_0^2 \ket{\Psi} \bra{\Psi} \wh{B}_0^2 \ket{\Psi} \\
        &= (\wh{A}_0 \ket{\Psi})^{\dagger} (\wh{A}_0 \ket{\Psi}) \cdot (\wh{B}_0 \ket{\Psi})^{\dagger}(\wh{B}_0 \ket{\Psi}) \\
        &= \norm{\wh{A}_0 \ket{\Psi}}^2 \norm{\wh{B}_0 \ket{\Psi}}^2.
    \end{aligned}\]
    Now, recall the Cauchy-Schwarz inequality 
    \[\abs{\bra{u} \ket{v}}^2 \leq \norm{\, \ket{u} \,}^2 \, \norm{\, \ket{v} \,}^2.\]
    Using this in the above expression and, we obtain 
    \[\begin{aligned}
        (\Delta \wh{A})^2 (\Delta \wh{B})^2 &=\norm{\wh{A}_0 \ket{\Psi}}^2 \norm{\wh{B}_0 \ket{\Psi}}^2 \\
        &\geq \abs{\bra{\Psi} \wh{A}_0\wh{B}_0 \ket{\Psi}}^2.
    \end{aligned}\]
    Using the fact that for any complex number \(z\) we have \(\abs{z}^2 \geq \abs{\Img(z)}^2 = \frac{1}{4}\abs{z-z^*}^2\), we get
    \[\begin{aligned}
        (\Delta \wh{A})^2 (\Delta \wh{B})^2 &=\norm{\wh{A}_0 \ket{\Psi}}^2 \norm{\wh{B}_0 \ket{\Psi}}^2 \\
        &\geq \abs{\bra{\Psi} \wh{A}_0\wh{B}_0 \ket{\Psi}}^2 \\
        &\geq \frac{1}{4} \abs{\bra{\Psi} \wh{A}_0\wh{B}_0 \ket{\Psi} - \left( \bra{\Psi} \wh{A}_0\wh{B}_0 \ket{\Psi} \right)^*}^2.
    \end{aligned}\]
    Using the fact 
    \[\begin{aligned}
        \left( \bra{\Psi} \wh{A}_0\wh{B}_0 \ket{\Psi} \right)^* &= \left( \bra{\Psi} \wh{A}_0\wh{B}_0 \ket{\Psi} \right)^{\dagger} \\
        &= \ket{\Psi}^{\dagger} \wh{B}_0^{\dagger} \wh{A}_0^{\dagger} \bra{\Psi}^{\dagger} \\
        &= \bra{\Psi} \wh{B}_0 \wh{A}_0 \ket{\Psi},
    \end{aligned}\]
    \[\begin{aligned}
        (\Delta \wh{A})^2 (\Delta \wh{B})^2 &\geq \frac{1}{4} \abs{\bra{\Psi} \wh{A}_0\wh{B}_0 \ket{\Psi}-\bra{\Psi} \wh{B}_0 \wh{A}_0 \ket{\Psi}}^2 \\
        &= \frac{1}{4} \abs{\bra{\Psi} \left[ \wh{A}_0,\wh{B}_0 \right] \ket{\Psi}}^2 \\
        &= \frac{1}{4}  \abs{\bra{\Psi} \left[ \wh{A},\wh{B} \right] \ket{\Psi}}^2.
    \end{aligned}\]
    Taking square roots of both sides we obtain 
    \[\Delta \wh{A} \, \Delta \wh{B} \geq \half \abs{ \left\langle \left[\wh{A},\wh{B}\right] \right\rangle }.\]
\end{proof}

\begin{mdexample}
    Applying this to \(\wh{A}=\wh{x}\) and \(\wh{B}=\wh{p}\) we have 
    \[\begin{aligned}
        \Delta\wh{x} \, \Delta\wh{p} &\geq \half \abs{ \left\langle \left[\wh{x},\wh{p}\right] \right\rangle } \\
        &= \half \abs{\langle i\hbar \rangle} \\
        &= \frac{\hbar}{2}.
    \end{aligned}\] 
\end{mdexample}

\section{Harmonic oscillator}

\subsection{Classical solution}

\begin{definition}
    A system with Hamiltonian 
    \[H = \frac{p^2}{2m}+\frac{m\omega^2 x}{2}\]
    is known as a \textbf{harmonic oscillator}.
\end{definition}

\begin{example}
    A particle attached to a string abides this Hamiltonian.
\end{example}

We now solve for the equation of motion and momentum the harmonic oscillator. The Hamilton equations of motion for this system are 
\[\begin{aligned}
    \dot{x} = \diffp{H}{p} &= \frac{p}{m}, \\
    \dot{p} = -\diffp{H}{x} &= -m\omega^2x.
\end{aligned}\]

Differentiating the first equation we can substitute an expression for \(\dot{p}\) and so, we have 

\[\ddot{x} = - \omega^2 x\]

hence, the general solution is of the form 

\[x(t) = Ae^{i\omega t}+A^*e^{-i\omega t}.\]

\begin{mdnote}
    We use \(A^*\) as the coefficient of the second term to ensure that \(x(t)\) is real.
\end{mdnote}

Similarly, we have that 
\[\begin{aligned}
    p(t) &= m \dot{x} \\
    &= im\omega\left( Ae^{i\omega t}+A^*e^{-i\omega t} \right).
\end{aligned}\]

We now consider a particular linear combination of \(x(t)\) and \(p(t)\), namely 

\[\begin{aligned}
    a(t) = \frac{p(t)-im\omega \, x(t)}{\sqrt{2m\omega \hbar}} &= - i \left( \sqrt{\frac{2m\omega}{\hbar}} \right) A^* e^{-i\omega t} \\
    a^*(t) = \frac{p(t)+im\omega \, x(t)}{\sqrt{2m\omega \hbar}} &=  i \left( \sqrt{\frac{2m\omega}{\hbar}} \right) A e^{i\omega t}.
\end{aligned}\]

These quantities satisfy the first-order equations 

\[\diffp{}{t}a(t)= -i\omega a(t) \quad \text{and} \quad \diffp{}{t}a^*(t)=i\omega a^*(t).\]

These quantities play a prominent role in the `quantum solution' of the Harmonic oscillator.

\subsection{Quantum solution}

\begin{definition}
    A system with Hamiltonian operator 
    \[\wh{H}=\frac{\wh{p}^2}{2m}+\frac{m\omega^2 \wh{x}^2}{2}\]
    is called a \textbf{harmonic oscillator}.
\end{definition}

As alluded to in the previous section, it will be useful to consider the operators:
\[\wh{a} = \frac{\wh{p}-im\omega \, \wh{x}}{\sqrt{2m\omega \hbar}} 
\quad \text{and} \quad
\wh{a}^{\dagger} = \frac{\wh{p}+im\omega \, \wh{x}}{\sqrt{2m\omega \hbar}}.\]

\begin{proposition}
    The value of 
    \[[\wh{a},\wh{a}^{\dagger}]=1.\]
\end{proposition}

\begin{proof}
    Use properties of commutators and the fact \([\wh{x},\wh{p}]=i\hbar\).
\end{proof}

\begin{mdthm}
    The Hamiltonian operator, \(\wh{H}=\frac{\wh{p}^2}{2m}+\frac{m\omega^2 \wh{x}^2}{2}\) can be expressed as
    \[\wh{H}=\hbar \omega \left( \wh{a}^{\dagger} \wh{a} + \half \right).\]
\end{mdthm}

\begin{proof}
    Consider the following 
    \[\begin{aligned}
        \hbar \omega \wh{a}^{\dagger}\wh{a} &=\hbar \omega \left( \frac{\wh{p}+im\omega \, \wh{x}}{\sqrt{2m\omega \hbar}} \right) \left( \frac{\wh{p}-im\omega \, \wh{x}}{\sqrt{2m\omega \hbar}} \right) \\
        &=\frac{1}{2m} ( \wh{p}+im\omega \, \wh{x} )(\wh{p}+im\omega \, \wh{x}) \\
        &= \frac{1}{2m} (\wh{p}^2 -im\omega \wh{p}\wh{x} +im\omega \wh{x}\wh{p}+ m^2\omega^2 \wh{x}^2) \\
        &= \frac{1}{2m} (\wh{p}^2 +im\omega \wh{x}\wh{p}  -im\omega \wh{p}\wh{x}+m^2\omega^2 \wh{x}^2) \\
        &= \frac{1}{2m} (\wh{p}^2+m^2\omega^2\wh{x}^2+im\omega[\wh{x},\wh{p}]) \\
        &= \frac{\wh{p}^2}{2m} + \frac{m\omega^2 \wh{x}^2}{2}-\frac{\hbar \omega}{2}.
    \end{aligned}\]
    Recall that \(\wh{H}=\frac{\wh{p}^2}{2m} + \frac{m\omega^2 \wh{x}^2}{2}\) so, 
    \[\hbar \omega \wh{a}^{\dagger}\wh{a} = \wh{H}- \frac{\hbar \omega}{2}.\]
    As such we can rewrite, 
    \[\wh{H}=\hbar \omega \left( \wh{a}^{\dagger} \wh{a} + \half \right).\]
\end{proof}

We would like to determine the spectrum and the eigenstates of\(\wh{H}\) i.e., solve the time-independent Schrödinger equation 
\[\wh{H} \ket{\Psi} = E \ket{\Psi}.\]

We can write
\[\wh{H}=\hbar\omega \left(\wh{N} +\half\right) \quad \text{where } \wh{N}=\wh{a}^{\dagger}\wh{a}.\]

\begin{proposition}
    Properties of \(\wh{N}=\wh{a}^{\dagger}\wh{a}\).
    \begin{itemize}
        \item \(\wh{N}\) is Hermitian i.e. \(\wh{N}^{\dagger}=\wh{N}\);
        \item \(\ket{\Psi}\) is an eigenvector of \(\wh{H}\) if and only if \(\ket{\Psi}\) is an eigenvector of \(\wh{N}\).
    \end{itemize}
\end{proposition}

\begin{mdnote}
    The eigenvectors are the same, but the eigenvalues are different.
\end{mdnote}

\begin{proof}
    We prove each bullet point in turn.
    \begin{itemize}
        \item Proof is trivial. 
        \item Suppose \(\ket{\Psi}\) is an eigenvector of \(\wh{N}\) with eigenvalue \(n\) i.e. \(\wh{N} \ket{\Psi} = n \ket{\Psi}\). Then, consider
        \[\begin{aligned}
            \wh{H} \ket{\Psi} &= \hbar \omega \wh{N} \ket{\Psi}+\frac{\hbar \omega}{2} \ket{\Psi} \\
            &= \hbar \omega \left( n+\half \right) \ket{\Psi}.
        \end{aligned}\]
        Therefore, \(\ket{\Psi}\) is an eigenvector of \(\wh{H}\) with eigenvalue \(\hbar\omega\left(n+\half\right)\). The proof of the converse direction follows similarly.
    \end{itemize}
\end{proof}

\begin{mdlemma}
    If \(\lambda\) is an eigenvalue of \(\wh{N}\) then \(\lambda \geq 0\).
\end{mdlemma}

\begin{proof}
    Let \(\ket{\Psi}\) be an eigenvector of \(\wh{N}\) with eigenvalue \(\lambda\) then,
    \[\wh{N} \ket{\Psi} = \lambda \wh{\Psi} \then \bra{\Psi} \wh{N}\ket{\Psi} = \lambda \bra{\Psi}\ket{\Psi}.\]
    Thus, we can write 
    \[\begin{aligned}
        \lambda &= \frac{\bra{\Psi}\wh{N}\ket{\Psi}}{\bra{\Psi}\ket{\Psi}} = \frac{\bra{\Psi}\wh{a}^{\dagger} \wh{a}\ket{\Psi}}{\bra{\Psi}\ket{\Psi}} \\
        &= \frac{\left( \wh{a} \ket{\Psi}\right)^{\dagger} \left( \wh{a}\ket{\Psi} \right)}{\bra{\Psi}\ket{\Psi}} \\
        &= \frac{\norm{ \wh{a} \ket{\Psi}}^2}{\bra{\Psi} \ket{\Psi}}.
    \end{aligned}\]
    (We have used the fact \(\wh{a}\ket{\Psi}\) is a number thus, \((\wh{a}\ket{\Psi})^{\dagger} (\wh{a}\ket{\Psi})= \norm{\wh{a}\ket{\Psi}}^2\)). Since, \(\bra{\Psi}\ket{\Psi} = \norm{\ket{\Psi}}^2 >0\) (since \(\ket{\Psi}\neq 0\)) and \(\norm{\wh{a}\ket{\Psi}}^2\geq 0\), we conclude \(\lambda \geq 0\).
\end{proof}

\begin{mdlemma}
    Let \(\ket{\Psi}\) be an eigenvector of \(\wh{N}\) with the eigenvalue \(\lambda\). Then exactly one of the statements holds:
    \begin{enumerate}
        \item \(\lambda=0\) \underline{and} \(\wh{a} \ket{\Psi}=0\) OR, 
        \item \(\lambda \neq 0\), \(\wh{a} \ket{\Psi} \neq 0\) and \(\wh{a} \ket{\Psi}\) is an eigenvector of \(\wh{N}\) with eigenvalue \(\lambda-1\).
    \end{enumerate}
\end{mdlemma}

\begin{proof}
    First, we compute 
    \[\begin{aligned}
        [\wh{N},\wh{a}] = [\wh{a}^{\dagger}\wh{a},\wh{a}] &= \wh{a}^{\dagger}\wh{a}\wh{a}-\wh{a}\wh{a}^{\dagger}\wh{a} \\
        &= \left( \wh{a}^{\dagger}\wh{a}-\wh{a}\wh{a}^{\dagger} \right)\wh{a} \\
        &= [\wh{a}^{\dagger},\wh{a}] \wh{a} \\\
        &= -[\wh{a},\wh{a}^{\dagger}] \wh{a} \\
        &= - \wh{a}.
    \end{aligned}\]
    Now, suppose \(\ket{\Psi}\) is an eigenvector of \(\wh{N}\) with eigenvalue \(\lambda\) i.e.,
    \[\wh{N}\ket{\Psi} = \lambda \ket{\Psi}.\] 
    Notice that \(\wh{N}\wh{a}-\wh{a}\wh{N}=[\wh{N},\wh{a}]\) allows us to write, \(\wh{N}\wh{a}=\wh{a}\wh{N}+[\wh{N},\wh{a}]\). Consider the vector \(\wh{a}\ket{\Psi}\) and act on it with \(\wh{N}\): 
    \[\begin{aligned}
        \wh{N} \wh{a} \ket{\Psi} &= \left( \wh{a}\wh{N}+[\wh{N},\wh{a}] \right) \ket{\Psi} \\
        &= \left( \wh{a}\lambda-\wh{a} \right)\ket{\Psi} \\
        &= (\lambda -1) \wh{a} \ket{\Psi}.
    \end{aligned}\]
    Thus, we have two possibilities.
    \begin{enumerate}
        \item \(\wh{a}\ket{\Psi}=0\). \\
        Supposing this is true then,
        \[\begin{aligned}
            \wh{a} \ket{\Psi} &= 0 \\
            \then \wh{a}^{\dagger} \wh{a} \ket{\Psi} &= 0 \\
            \then \wh{N} \ket{\Psi} &= 0.
        \end{aligned}\]
        But, since we assumed \(\wh{N} \ket{\Psi}=\lambda \ket{\Psi}\) then, \(\ket{\Psi} \neq 0\) (by the definition of an eigenvector) so, \(\lambda =0\). Conversely, if \(\lambda= 0\) then, \(0=\lambda \ket{\Psi}= \wh{N} \ket{\Psi}\) and thus
        \[0=\bra{\Psi}\wh{N} \ket{\Psi} = \bra{\Psi} \wh{a}^{\dagger}\wh{a}\ket{\Psi} = \norm{\wh{a}\ket{\Psi}}^2\]
        which implies (by the axioms of the norm) \(\wh{a} \ket{\Psi}=0\).
        \item \(\wh{a}\ket{\Psi}\) is an eigenvector of \(\wh{N}\) with eigenvalue \(\lambda-1\).
    \end{enumerate}
\end{proof}

\begin{mdlemma}
    Let \(\ket{\Psi}\) be an eigenvector of \(\wh{N}\) with eigenvalue \(\lambda\). Then, \(\lambda \in \ZZ_{\geq 0}\) and \(\ket{\Psi}, \, \wh{a}\ket{\Psi}, \\\wh{a}^2\ket{\Psi}, \ldots ,  \wh{a}^{\lambda} \ket{\Psi}\) are eigenstates of \(\wh{N}\) with the eigenvalues \(\lambda, \lambda-1,\lambda-2, \ldots, 0\) respectively. Finally, \(\wh{a}^{\lambda+1} \ket{\Psi}=0\).
\end{mdlemma}

\begin{proof}
    By iteration of the previous lemma we get the infinite sequence of eigenvalues and their corresponding eigenstates. \\
    We prove \(\lambda \in \ZZ_{\geq 0}\). \\
    For the sake of contradiction, assume \(\lambda\) is not a non-negative integer. Then, none of the numbers \(\{\lambda,\lambda-1,\lambda-2,\cdots\}\) (which are eigenvalues) are \(0\). This means, from one eigenvalue we get an infinite sequence of eigenvalues \(\{\lambda,\lambda-1,\lambda-2,\cdots\}\) however, at eventually we will get negative eigenvalues which is forbidden by a previous lemma. Whereas, if \(\lambda\) is a non-negative integer we get a finite sequence of eigenvalues which terminates at \(0\). Hence, \(\lambda\) must be a non-negative integer.
\end{proof}

\begin{mdlemma}
    Let \(\ket{\Psi}\) be an eigenvector of \(\wh{N}\) with the eigenvalue \(\lambda\). Then, \(\wh{a}^{\dagger}\ket{\Psi} \neq 0 \) and \(\wh{a}^{\dagger}\ket{\Psi}\) is an eigenvector of \(\wh{N}\) with eigenvalue \(\lambda+1\).
\end{mdlemma}

\begin{mdnote}
    To remember that \({\wh{a}}^{\dagger}\) increases the eigenvalue by \(1\) we can think of \(\dagger\) as \(+\).
\end{mdnote}

\begin{proof}
    Note that \([\wh{A},\wh{B}]^{\dagger} = - [\wh{A}^{\dagger},\wh{B}^{\dagger}]\) so, applying this identity to \([\wh{N},\wh{a}]=-\wh{a}\), and we have
    \[\begin{aligned}
        [\wh{N},\wh{a}]^{\dagger} = -\wh{a}^{\dagger} &=  -[\wh{N}^{\dagger},\wh{a}^{\dagger}] \\
        &= -[\wh{N},\wh{a}^{\dagger}].
    \end{aligned}\]
    We conclude, \([\wh{N},\wh{a}^{\dagger}]=\wh{a}^{\dagger}\). Furthermore, notice that \(\wh{N}\wh{a}-\wh{a}\wh{N}=[\wh{N},\wh{a}]\) which  allows us to write, \(\wh{N}\wh{a}=\wh{a}\wh{N}+[\wh{N},\wh{a}]\).
    Suppose \(\ket{\Psi}\) is an eigenvector of \(\wh{N}\) with eigenvalue \(\lambda\). We can compute 
    \[\begin{aligned}
        \wh{N}\wh{a}^{\dagger} \ket{\Psi} &= \left( \wh{a}^{\dagger}\wh{N}+[\wh{N},\wh{a}^{\dagger}] \right) \ket{\Psi} \\
        &= (\wh{a}^{\dagger}\wh{N}+\wh{a}^{\dagger}) \ket{\Psi} \\
        &= \wh{a}^{\dagger}\wh{N} \ket{\Psi} +\wh{a}^{\dagger} \ket{\Psi} \\
        &= \wh{a}^{\dagger}\lambda \ket{\Psi} + \wh{a}^{\dagger}\ket{\Psi} \\
        &= (\lambda+1)\wh{a}^{\dagger}\ket{\Psi}.
    \end{aligned}\]
    Thus, we have two possibilities.
    \begin{enumerate}
        \item \(\wh{a}^{\dagger}\ket{\Psi}=0\). \\
        Supposing this is true then also \(\wh{a}\wh{a}^{\dagger}\ket{\Psi}=0\). But we can compute 
        \[\begin{aligned}
            0 = \wh{a}\wh{a}^{\dagger}\ket{\Psi} &= \left( \wh{a}^{\dagger}\wh{a}+[\wh{a},\wh{a}^{\dagger}]  \right) \ket{\Psi} \\
            &= (\wh{N}+1)\ket{\Psi} \\
            &=(\lambda+1) \ket{\Psi}.
        \end{aligned}\]
        Since \(\lambda\) is an eigenvalue of \(\wh{N}\) by a previous lemma we have that \(\lambda\geq 0\) so, \(\lambda +1 \neq 0\). However, this implies that \(\ket{\Psi} =0\), which contradicts the assumption that \(\ket{\Psi}\) is an eigenvector (as by definition of an eigenvector \(\ket{\Psi}\neq 0\)).
        \item \(\wh{a}^{\dagger}\ket{\Psi}\) is an eigenvector of \(\wh{N}\) with eigenvalue \(\lambda+1\).
    \end{enumerate}
    Therefore, only the second statement is possible.
\end{proof}

\begin{mdnote}
    In conclusion, we see that we can lower the eigenvalue of \(\wh{N}\) by applying \(\wh{a}\) to the eigenstate, and similarly we can raise it by applying \(\wh{a}^{\dagger}\). For this reason \(\wh{a}\) and \(\wh{a}^{\dagger}\) are called the \textbf{lowering} and the \textbf{raising} operators. 
\end{mdnote}

\begin{mdlemma}
    Let \(\ket{\Psi}\) be an eigenvector of \(\wh{N}\) with the eigenvalue \(\lambda\). Then, the following statements hold 
    \begin{enumerate}
        \item \(\lambda \in \ZZ_{\geq 0}\),
        \item there exists a state \(\ket{\Phi} \equiv \wh{a}^{\lambda}\ket{\Psi} \in \mathcal{H}\) which satisfies \(\wh{a}\ket{\Phi}=0\) (in particular, it is an eigenstate of \(\wh{N}\) with eigenvalue \(0\)),
        \item we have \(\ket{\Psi} = C (\wh{a}^{\dagger})^{\lambda}\ket{\Phi}\) for some \(C \in \CC\) where \(C \neq 0\).
    \end{enumerate}
\end{mdlemma}

\begin{mdnote}
    This lemma allows us to undo the action of \(\wh{a}\) with \(\wh{a}^{\dagger}\) and vice versa. For example consider the state \(\wh{a}\ket{\Psi}\) with eigenvalue \(\lambda-1\), and then apply \(\wh{a}^{\dagger}\) to get back to the eigenvalue \(\lambda\). We find the state 
    \[\begin{aligned}
        \wh{a}^{\dagger} \left( \wh{a}\ket{\Psi} \right) &= \wh{a}^{\dagger}\wh{a}\ket{\Psi} \\
        &= \wh{N} \ket{\Psi} \\
        &= \lambda \ket{\Psi}.
    \end{aligned}\]
    Thus, unless \(\lambda=0\), we can undo the action of \(\wh{a}\) by acting with \(\wh{a}^{\dagger}\).
\end{mdnote}

\begin{mdexample}
    Suppose that \(\ket{\Psi}\) is the eigenstate of \(\wh{N}\) with the eigenvalue \(\lambda =2\). Then by Lemma \(9.3\) we can define the eigenstates of \(\wh{N}\)
    \[\ket{\Phi_0} = \wh{a}^2 \ket{\Psi} \quad \text{and} \quad \ket{\Phi_1}=\wh{a}\ket{\Psi},\]
    which have eigenvalues \(0\) and \(1\) respectively. We have 
    \[\begin{aligned}
        \wh{a}^{\dagger} \ket{\Phi_1} &= \wh{a}^{\dagger}\wh{a}\ket{\Psi} \\
        &= \wh{N}\ket{\Psi} \\
        &= 2\ket{\Psi}.
    \end{aligned}\]
    Moreover,
    \[\begin{aligned}
        \wh{a}^{\dagger} \ket{\Phi_0} &= \wh{a}^{\dagger} \wh{a}\ket{\Psi} \\
        &= \wh{a}^{\dagger}\wh{a}\wh{a}\ket{\Psi} \\
        &= \wh{a}^{\dagger}\wh{a}\ket{\Phi_1} \\
        &= \wh{N} \ket{\Phi_1} \\
        &= \ket{\Phi_1}.
    \end{aligned}\]
    In the last two equations we used that \(\Psi\) and \(\Phi_1\) are eigenstates of \(\wh{N}\) with the eigenvalues \(2\) and \(1\) respectively. We get from these equations 
    \[\begin{aligned}
        \ket{\Psi} &= \half \wh{a}^{\dagger} \ket{\Phi_1} \\
        &= \half \wh{a}^{\dagger} (\wh{a}^{\dagger}\ket{\Phi_0}) \\
        &= \half (\wh{a}^{\dagger})^2 \ket{\Phi_0}.
    \end{aligned}\]
\end{mdexample}

\begin{mdlemma}
    Let \(\ket{\Phi}\) be such that 
    \[\wh{a}\ket{\Phi}=0 \quad \text{and} \quad \bra{\Phi}\ket{\Phi}=1.\]
    Then, (up to a possible overall phase) \(\ket{\Phi}\) is described by the wave function 
    \[\Phi(x)=\left( \frac{m\omega}{\pi \hbar} \right)^{\frac{1}{4}} e^{-\frac{m\omega x^2}{2\hbar}}.\]
\end{mdlemma}

\begin{mdnote}
    This lemma tells us that there exists a unique normalised state \(\ket{\Phi}\) which satisfies \(\wh{a} \ket{\Phi}=0\).
\end{mdnote}

\begin{proof}
    Recall the definition of \(\wh{a}=\frac{\wh{p}-im\omega \wh{x}}{\sqrt{2m\omega \hbar}}\) then we can write 
    \[\begin{aligned}
        \wh{a} \ket{\Phi} =0 &\then (\wh{p}-im\omega \wh{x}) \ket{\Phi} =0 \\
        &\then -i\hbar \diffp{}{x}\Phi(x)-im\omega x\Phi(x)=0 \\
        &\then \diffp{}{x}\Phi(x) = -\frac{m\omega}{\hbar} \cdot x\Phi(x) \\
        &\then \Phi(x) = Ae^{-\frac{m\omega x^2}{2\hbar}},
    \end{aligned}\]
    with \(A \in \CC\). We can determine \(A\) (up to a phase) from the normalisation condition:
    \[\begin{aligned}
        \bra{\Phi} \ket{\Phi} &= \int_{-\infty}^{\infty} \abs{\Phi(x)}^2 \, dx \\
        &= \abs{A}^2 \int_{-\infty}^{\infty} e^{-\frac{m\omega x^2}{\hbar}}.
    \end{aligned}\]
    Using the change of variable \(x= y\sqrt{\frac{\hbar}{m\omega}}\) and the fact \(\int_{-\infty}^{\infty} e^{-y^2} \, dy =\sqrt{\pi}\), we have 
    \[\begin{aligned}
        \bra{\Phi} \ket{\Phi} &= \sqrt{\frac{\hbar}{m\omega}} \int_{-\infty}^{\infty} e^{-y^2} \, dy \\
        &= \abs{A}^2 \sqrt{\frac{\hbar}{m\omega}}.
    \end{aligned}\]
    Since, we must have \(\bra{\Phi} \ket{\Phi} =1\) it follows that 
    \[A = \left( \frac{m\omega}{\pi \hbar} \right)^{\frac{1}{4}}.\]
\end{proof}

\begin{definition}[Notation]
    We identify the state \(\ket{\Phi} \equiv \ket{0}\) where \(\ket{\Phi}\) is defined as above. The state \(\ket{0}\) is called the \textbf{ground state} (or vacuum state).
\end{definition}

\begin{mdremark}
    \(\ket{0} \neq 0\) and indeed \(\ket{0}\) is an eigenvector.
\end{mdremark}

\begin{mdnote}
    We call \(\ket{0}\) the \textit{ground state}  because it has the lowest possible energy.
\end{mdnote}

\begin{mdthm}
    From \(\ket{0}\) we can construct the state for \(n \in \ZZ_{\geq 0}\),
    \[\ket{n} \equiv \frac{1}{\sqrt{n!}} (\wh{a}^{\dagger})^n \ket{0},\]
    and \(\bra{n}\ket{n}=1\).
\end{mdthm}

\begin{proof}
    Recall that 
    \[\ket{n} \equiv A_n (\wh{a}^{\dagger})^n \ket{0};\]
    in this proof we illustrate how to determine the constant \(A_n\) from the condition \(\bra{n}\ket{n}=1\). We will do this in two ways to demonstrate two different approaches. First, we choose \(A_n\) to be real and positive. Second, we have the following useful results 
    \[\begin{aligned}
        \bra{0} \wh{a}^{\dagger} = \left( \wh{a} \ket{0} \right)^{\dagger}=0 \quad & \quad \bra{0} \wh{a}^{\dagger} \ket{0} =0 \\
        \bra{0} \wh{a}= \left( \wh{a}^{\dagger} \ket{0} \right)^{\dagger} \quad & \quad \bra{0} \wh{a} \ket{0}=0.
    \end{aligned}\]
    Next, we can write
    \[\begin{aligned}
        \bra{n}\ket{n} &= A_n \bra{n} \left( \wh{a}^{\dagger} \right)^n \ket{0} \\
        &= A_n^2 \bra{0} \left( \wh{a} \right)^n \left( \wh{a}^{\dagger} \right)^n \ket{0}.
    \end{aligned}\]
    \begin{mdnote}
        The trick is to move \(\wh{a}\) past \(\wh{a}^{\dagger}\) so, that it becomes \(\wh{N}\), and we know how this acts on \(\ket{0}\).
    \end{mdnote}
    We compute \(\bra{n}\ket{n}\) for certain values of \(n\).
    \begin{itemize}
        \item Suppose \(n=1\) then,
        \[\begin{aligned}
            \bra{1} \ket{1} &= A_1^2 \bra{0} \wh{a}\wh{a}^{\dagger}\ket{0} \\
            &= A_1^2 \bra{0} \wh{a}^{\dagger} \wh{a}+[\wh{a},\wh{a}^{\dagger}] \ket{0} \\
            &= A_1^2\bra{0} \left( \wh{a}^{\dagger}\wh{a}+1 \right) \ket{0} \\
            &= A_1^2\bra{0} \wh{a}^{\dagger} \wh{a}\ket{0}+A_1^2\bra{0}1\ket{0} \\
            &= A_1^2.
        \end{aligned}\]
        Therefore, \(A_1 =1\).
        \item Suppose \(n=2\) then,
        \[\begin{aligned}
            \bra{2}\ket{2} &= A_2^2\bra{0} \wh{a}\wh{a}\wh{a}^{\dagger}\wh{a}^{\dagger} \ket{0} \\
            &= A_2^2\bra{0} \wh{a} \left( \wh{a}^{\dagger}\wh{a}+[\wh{a},\wh{a}^{\dagger}] \right) \wh{a}^{\dagger} \ket{0} \\
            &= A_2^2\bra{0} \wh{a} \left( \wh{a}^{\dagger}\wh{a}+1 \right)\wh{a}^{\dagger}\ket{0} \\
            &= A_2^2\bra{0} \wh{a}\wh{a}^{\dagger}\wh{a}\wh{a}^{\dagger}\ket{0}+A_2^2 \underbrace{\bra{0}\wh{a}\wh{a}^{\dagger}\ket{0}}_{\bra{1}\ket{1}=1} \\
            &= A_2^2 \bra{0} \wh{a} \wh{a}^{\dagger} \left( \wh{a}^{\dagger}\wh{a}+1 \right)\ket{0} +A_2^2 \\
            &= \underbrace{\bra{0}\wh{a}}_{0}\wh{a}^{\dagger}\wh{a}^{\dagger}\underbrace{\wh{a}\ket{0}}_{0} +A_2^2 \\
            &= 2A_2^2.
        \end{aligned}\]
        Therefore, \(A_2 =\frac{1}{2}\).
    \end{itemize}
    For the general case \(n\), we first consider 
    \[\begin{aligned}
        \wh{a}(\wh{a}^{\dagger})^n &= \wh{a}\wh{a}^{\dagger}(\wh{a}^{\dagger})^{n-1}  \\
        &= \left( \wh{a}^{\dagger}\wh{a}+[\wh{a},\wh{a}^{\dagger}] \right)(\wh{a}^{\dagger})^{n-1} \\
        &= \wh{a}^{\dagger}\wh{a}(\wh{a}^{\dagger})^{n-1}+(\wh{a}^{\dagger})^{n-1}.
    \end{aligned}\]
    Consider the term 
    \[\begin{aligned}
        \wh{a}^{\dagger} \wh{a}(\wh{a}^{\dagger})^{n-1} &= \wh{a}^{\dagger}\wh{a}\wh{a}^{\dagger}(\wh{a}^{\dagger})^{n-2} \\
        &= \wh{a}^{\dagger} (\wh{a}^{\dagger}\wh{a}+1) (\wh{a}^{\dagger})^{n-2} \\
        &= (\wh{a}^{\dagger})^2 \, \wh{a} \, (\wh{a}^{\dagger})^{n-2} +(\wh{a}^{\dagger})^{n-1}.
    \end{aligned}\]
    Therefore,
    \[\wh{a}(\wh{a}^{\dagger})^n=(\wh{a}^{\dagger})^2 \, \wh{a} \, (\wh{a}^{\dagger})^{n-2}+2(\wh{a}^{\dagger})^{n-1}.\]
    Iterating this process \(k\) times we have 
    \[\wh{a}(\wh{a}^{\dagger})^n = (\wh{a}^{\dagger})^k \, \wh{a} \, (\wh{a}^{\dagger})^{n-1} +k (\wh{a}^{\dagger})^{n-1}.\]
    Then iterating the process \(n\) times i.e. when \(k=n\) we have 
    \[\wh{a}(\wh{a}^{\dagger})^n= (\wh{a}^{\dagger})^n \wh{a}+n(\wh{a}^{\dagger})^{n-1}.\]
    With this identity we can compute 
    \[\begin{aligned}
        \bra{n}\ket{n} &= A_n^2 \bra{0} \wh{a}^n (\wh{a}^{\dagger})^n \ket{0} \\
        &= A_n^2 \bra{0} \wh{a}^{n-1}\wh{a} (\wh{a}^{\dagger})^n \ket{0} \\
        &= A_n^2\bra{0} \wh{a}^{n-1} \left( (\wh{a}^{\dagger})^n\wh{a}+n(\wh{a}^{\dagger})^{n-1} \right) \ket{0} \\
        &= A_n^2 \bra{0} \wh{a}^{n-1} (\wh{a}^{\dagger})^n \underbrace{\wh{a} \ket{0}}_{0} +A_n^2n \bra{0} \wh{a}^{n-1} (\wh{a}^{\dagger})^{n-1} \ket{0} \\
        &= A_n^2n\bra{0} \wh{a}^{n-1} (\wh{a}^{\dagger})^{n-1} \ket{0} \\
        &= A_n^2n(n-1)\bra{0} \wh{a}^{n-2} (\wh{a}^{\dagger})^{n-2} \ket{0} \\
        &= \cdots \\
        &= A_n^2n(n-1)(n-2)\cdots 1\bra{0} \wh{a}^0 (\wh{a}^{\dagger})^0 \ket{0} \\
        &= A_n^2n! \bra{0}\ket{0} \\
        &=A_n^2n!.
    \end{aligned}\]
    Therefore, 
    \[A_n = \frac{1}{\sqrt{n!}}.\]
\end{proof}

\begin{proof}
    We present a quicker method to compute \(A_n\). Consider
    \[\ket{n+1}=C_n \wh{a}^{\dagger}\ket{n}\]
    for some \(C_n>0\). We can determine \(C_n\) by imposing the normalisation condition.
    \[\begin{aligned}
        \bra{n+1} \ket{n+1} &=\abs{C_n}^2 \bra{n}\wh{a} \wh{a}^{\dagger} \ket{n} \\
        &= C_n^2 \bra{n} \wh{a}^{\dagger}\wh{a}+[\wh{a},\wh{a}^{\dagger}]\ket{n} \\
        &= C_n^2 \bra{n} \wh{N}+1\ket{n} \\
        &= C_n^2(n+1)\bra{n}\ket{n} \\
        &= C_n^2 (n+1).
    \end{aligned}\]
    Therefore, \(C_n = \frac{1}{\sqrt{n+1}}\) and 
    \[\ket{n+1}=\frac{\wh{a}^{\dagger}}{\sqrt{n+1}}\ket{n}.\]
    We can then start from \(\ket{0}\),
    \[\ket{1} = \wh{a}^{\dagger} \ket{0}, \quad \ket{2}=\frac{\wh{a}^{\dagger}}{\sqrt{2}}\ket{1} = \frac{(\wh{a}^{\dagger})^2}{\sqrt{2}}\ket{0}, \cdots\]
    to conclude 
    \[\ket{n} = \frac{1}{\sqrt{n}}(\wh{a}^{\dagger})^n \ket{0}.\]
\end{proof}

\begin{corollary}
    The state \(\ket{n}\) is an eigenstate of \(\wh{N}\) with the eigenvalue \(n\), i.e.
    \[\wh{N}\ket{n}= n\ket{n}.\]
\end{corollary}

\begin{proof}
    Use Lemma \(9.4\).
\end{proof}

\begin{mdthm}
    Let \(\ket{\Psi}\) be an eigenstate of \(\wh{N}\) with the eigenvalue \(\lambda\). Then, the following statements hold 
    \begin{enumerate}
        \item \(\lambda = n \in \ZZ_{\geq 0}\),
        \item \(\ket{\Psi} = C \ket{n}\) for some \(C \in \CC\) where \(C \neq 0\).
    \end{enumerate}
\end{mdthm}

\begin{mdthm}
    The eigenstates of \(\wh{H}\) are the states \(\ket{n}\), and the corresponding eigenvalues are 
    \[E_n = \hbar \omega \left( n+\half \right).\]
    The spectrum of \(\wh{H}\) is therefore, 
    \[\left\{ \frac{\hbar \omega}{2},\frac{3\hbar \omega}{2},\frac{5\hbar \omega}{2},\cdots \right\}\]
\end{mdthm}

\pagebreak

\appendix

\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}

\section{Eigenvalue and eigenvectors}

\begin{definition}
    Let \(V\) be a vector space. If \(T:V \to V\) is a linear operator then, we call a vector \(\bm{v} \in V\) an \textbf{eigenvector} for \(T\) if 
    \[\bm{v} \neq \bm{0} \quad \text{and} \quad T(\bm{v}) = \lambda\bm{v} \text{ for some \(\lambda \in \CC\).}\]
    The scalar \(\lambda\) is called an \textbf{eigenvalue} of \(T\).
\end{definition}

\begin{lemma}
    Let $T$ be a linear operator on a vector space $V$ and $\bm{v} \in V$ and $\lambda \in \CC.$ Then $\bm{v}$ is an eigenvector of $T$ with eigenvalue $\lambda$ if and only if $$\bm{v} \neq \bm{0} \quad \text{and} \quad \bm{v} \in \ker{(T-\lambda I)}.$$
    \end{lemma}

\begin{mdthm}
    The scalar \(\lambda \in \CC\) is an eigenvalue of \(T\) is and only if \(\det(T-\lambda I)=0\).
\end{mdthm}

\begin{example}
    Determine all eigenvalues and eigenvectors of the matrix 
    \[A = \begin{pmatrix} 1 & -10 \\ -5 & -4 \end{pmatrix}.\]
    \begin{solution}
        To find the eigenvalues we begin by solving the characteristic polynomial given by \(p_A(\lambda)=\det(A-\lambda I)=0\). We have that 
        \[A-\lambda I = \begin{pmatrix} 1-\lambda & -10 \\ -5 & -4-\lambda \end{pmatrix},\]
        and so 
        \[\det(A-I\lambda)=(\lambda-6)(\lambda+9).\]
        Thus, the eigenvalues of \(A\) are \(\lambda_1 = 6\) and \(\lambda_2 = -9\). \\
        To find the eigenvectors of \(A\), we have to solve the system \((A-\lambda I)\bm{v} = \bm{0}\) for \(\lambda=6,-9\). Solving the system: 
        \[(A-6I)\bm{v} = \begin{pmatrix} -5 & -10 \\ -5 & -10 \end{pmatrix} \begin{pmatrix} x \\ y \end{pmatrix} = \begin{pmatrix} 0 \\0 \end{pmatrix},\]
        and so we have the system 
        \[\begin{cases}
            -5x-10y&=0 \\
            -5x-10y&=0.
        \end{cases}\]
        We conclude the eigenvector is of the form \(x=-2y\) thus, by choosing \(x=1\) the eigenvector 
        \[\bm{v}_1 = \begin{pmatrix} -2 \\ 1 \end{pmatrix}.\]
        By a similar process the eigenvector 
        \[\bm{v}_2 = \begin{pmatrix} 1 \\1 \end{pmatrix}.\]
    \end{solution}
\end{example}

\begin{mdexample}
    Here we present an alternative method to finding the eigenvectors of the matrix \(A\). From out previous work we know \(A\) has eigenvalues \(\lambda=6,-9\). We can choose an ansatz for the eigenvectors to be of the form 
    \[\bm{v} = \begin{pmatrix} 1 \\ \alpha \end{pmatrix},\]
    and we know this eigenvector must satisfy the following equation 
    \[\begin{pmatrix} 1 & -10 \\ -5 & -4 \end{pmatrix} \begin{pmatrix} 1 \\ \alpha \end{pmatrix} = \lambda \begin{pmatrix} 1 \\ \alpha \end{pmatrix}.\]
    Therefore, we have the following set of equations:
    \[\begin{cases}
        1-10\alpha &= \lambda \\
        -5-4\alpha &= \lambda \alpha;
    \end{cases}\]
    from the first equation it follows that 
    \[\alpha = \frac{1}{10}(1-\lambda).\]
    We conclude the eigenvectors of \(A\) are 
    \[\bm{v}_{\lambda =6} = \begin{pmatrix} 1 \\ -0.5 \end{pmatrix} \quad \text{and} \quad \bm{v}_{\lambda=-9} = \begin{pmatrix} 1 \\1\end{pmatrix}\]
\end{mdexample}

\subsection{Diagonalisation}

Diagonal matrices have nice properties which makes working with them easy.

\begin{proposition}
    Properties of diagonal matrices:
    \begin{itemize}
        \item The determinant of \(\text{diag}(a_1, \ldots, a_n)\) is the product \(a_1 \cdots a_n\).
        \item Matrix multiplication: \(\text{diag}(a_1, \ldots, a_n) \text{diag}(b_1, \ldots, b_n) = \text{diag}(a_1b_1, \ldots, a_nb_n).\)
        \item The diagonal matrix \(\text{diag}(a_1, \ldots, a_n)\) is invertible if and only if the entries \(a_1, \ldots, a_n\) are all non-zero. In this case, we have
        \[\text{diag}(a_1, \ldots, a_n) = \text{diag}(a_1\inv, \ldots, a_n\inv).\]
    \end{itemize}
\end{proposition}

\begin{mdthm}
    To diagonalise a matrix \(A\) we need a matrix \(P\) such that 
    \[A = PDP\inv\]
    where \(D\) is a diagonal matrix. It follows that 
    \[D = \begin{pmatrix}
        \lambda_1 & 0 &\cdots & 0 \\
        0 &\lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & 0 \\
        0 & 0 & \cdots & \lambda_n
    \end{pmatrix},\]
    where \(\lambda_i\) are the eigenvalues of \(A\) and, 
    \[P = (\bm{v}_1, \cdots, \bm{v}_n)\]
    where \(\bm{v}_i\) are the corresponding eigenvectors.
\end{mdthm}

\section{Multiple eigenvalue multiplicity}

\begin{figure}[H]
     \begin{center}
         \includegraphics[width=\textwidth]{./Resources/Discrete e-val multiplicity.png}
     \end{center}
\end{figure}

\section{Links}

\begin{itemize}
    \item \href{https://www.youtube.com/watch?v=Y8y965ZAmQE}{Dirac delta function}
\end{itemize}

\end{document}