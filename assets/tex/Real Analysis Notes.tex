\documentclass[12pt, a4paper]{article}
\usepackage{francesco}
\usepackage[colorlinks=true,
            hidelinks,
            pdfauthor={Francesco N. Chotuck},
            pdftitle={Real Analysis Notes}
            ]{hyperref}
\hypersetup{urlcolor=RubineRed,linktoc=all, linkcolor=black}

%\pgfplotsset{width=\textwidth}

\pagestyle{fancy}
\lhead{Francesco Chotuck}
\rhead{5CCM221A Real Analysis}
\setlength{\headheight}{15pt}

\title{Real Analysis Notes}
\date{}
\author{Francesco Chotuck}
\begin{document}
\maketitle

\begin{abstract}
    This is KCL undergraduate module 5CCM221A, instructed by Benjamin \\ Krause. The formal name for this class is "Real Analysis". 
\end{abstract}

\tableofcontents

\pagebreak

\section{Preliminaries}

\subsection{Sets on the real line}

\begin{definition}
    A set \(A \subset \RR\) is said to be \textbf{bounded above} if \(A \subset (-\infty, M]\) for some \(M \in \RR\). \\
    A set \(A \subset \RR\) is said to be \textbf{bounded below} if \(A \subset [m, \infty)\) for some \(m \in \RR\). \\
    If \(A\) is both bounded below and bounded above, it is called \textbf{bounded}; otherwise it is called \textbf{unbounded}.
\end{definition}

\begin{definition}
    If \(M\) is an upper bound for \(S\) and \(M \in S\) then \(M\) is called the \textbf{maximum element} of \(S\).
\end{definition}

\begin{definition}
    If \(m\) is a lower bound for \(S\) and \(m \in S\) then \(m\) is called the \textbf{minimum element} of \(S\).
\end{definition}

\begin{definition}
    For a set \(S \subseteq \RR, M \in \RR\) is called the \textbf{supremum} of \(S\) (the \textbf{least upper bound} of \(S\)) if 
    \begin{itemize}
        \item \(M\) is an upper bound i.e. \(\forall x \in S: x\leq M\);
        \item \(M\) is the least upper bound i.e. \(\forall \eps>0, \exists s\in S : M-\eps <s\).
    \end{itemize}
\end{definition}

\begin{mdnote}
    The second property implies that \(M-\eps\) is not an upper bound of \(S\) hence, \(M\) must be the least upper bound of \(S\).
\end{mdnote}

\begin{definition}
    For a set \(S \subseteq \RR, m \in \RR\) is called the \textbf{infimum} of \(S\) (the \textbf{greatest lower bound} of \(S\)) if 
    \begin{itemize}
        \item \(m\) is a lower bound i.e. \(\forall x \in S: x\geq m\);
        \item \(m\) is the greatest lower bound i.e. \(\forall \eps>0, \exists s\in S : m+\eps >s\).
    \end{itemize}
\end{definition}

\begin{mdnote}
    Similarly, the second property implies that \(m+\eps\) is not a lower bound for \(S\) hence, \(m\) must be the greatest lower bound of \(S\).
\end{mdnote}

\begin{mdremark}
    The definition of the infimum and supremum switches the inequality.
\end{mdremark}

% \begin{definition}
%     A number \(M\) is called the \textbf{least upper bound}, or  the \textbf{supremum} of \(A\), if \(M\) is an upper bound of \(A\) and if there is no \(M'<M\) which is also an upper bound of \(A\).
% \end{definition}

% \begin{definition}
%     A number \(m\) is called the \textbf{greatest lower bound} of \(A\) or an \textbf{infimum} of \(A\), if \(m\) is a lower bound of \(A\) and no \(m'>m\) is a lower bound.
% \end{definition}

\begin{mdthm}[Completeness axiom]
Every set of real number that is bounded above has a supremum. \\
Every set of real number that is bounded below has an infimum.
\end{mdthm}

\begin{definition}
    A set \(A\) on the real line is called \textbf{countable,} if it's finite or there is a one-to-one map between \(A\) and \(\NN\). \\
    If \(A\) is not countable, it is called \textbf{uncountable}.
\end{definition}

\begin{example}
    \(\NN, \ZZ, \QQ\) are countable and \(\RR\) and \([a,b]\) are uncountable.
\end{example}

\begin{definition}
    Let \(\eps >0\) and \(x\in \RR\). We say that \((x-\eps,x+\eps)\) is an \(\eps\)-neighbourhood of \(x\). \\
    \(U\) is a neighbourhood of \(x\) if \(U\) contains an \(\eps\)-neighbourhood of \(x\) for some \(\eps>0.\)
\end{definition}

\begin{definition}
   A point \(x\in A\) is called an \textbf{interior point} of \(A\), if \(A\) contains a neighbourhood of \(x\).
\end{definition}

\begin{definition}
    \(A \subset \RR\) is said to be \textbf{open} if \(A\) contains a neighbourhood of every point i.e. every point of \(A\) is an interior point.
\end{definition}

\begin{definition}
    \(B \subset \RR\) is said to be \textbf{closed} if \(B^C = \RR \backslash B\) is open i.e. the complement is open.
\end{definition}

\begin{mdremark}
    Some sets are neither open nor closed.
\end{mdremark}

\begin{example} 
    Some examples of open and closed sets.
    \begin{itemize}
        \item \([a,b]\) is a closed set.
        \item \((a,\infty)\) is an open set.
        \item \([a,\infty)\) is a closed set. Note the complement of the set is \((-\infty,a)\).
        \item \(\RR\) is open and closed because the complement of \(\RR\) is the empty set which is (trivially) open.
        \item \(\ZZ\) is closed.
        \item \(\QQ\) is neither closed nor open.
    \end{itemize}
\end{example}

\subsection{Sequences}

\begin{definition}
    A sequence of real numbers \(\{a_n\}_{n=1}^{\infty}\) is said to \textbf{converge} to a \textbf{limit}, \(L \in \RR\) if 
    \[\forall \eps >0, \exists N\in \NN, \forall n\geq N : \abs{a_n-L}<\eps.\]
\end{definition}

\begin{definition}
    If \(\{a_n\}_{n=1}^{\infty}\) is a sequence of real numbers and if \(\{n_k\}_{k=1}^{\infty}\) is a \textbf{strictly increasing} sequence of natural numbers (i.e. \(n_{k+1}>n_k\) for all \(k\)) then, \(\{a_{n_k}\}_{n=1}^{\infty}\) is said to be a \textbf{subsequence} of \(\{a_n\}_{n=1}^{\infty}\).
\end{definition}

\begin{definition}
    We say \(x_*\) is a \textbf{limit point} of \(\{x_n\}_{n=1}^{\infty}\) if there is a subsequence of \(\{x_n\}_{n=1}^{\infty}\) converging to \(x_*\).
\end{definition}

\begin{definition}
    A sequence \(\{a_n\}_{n=1}^{\infty}\) is said to be a \textbf{Cauchy sequence} if
    \[\forall \eps >0, \exists N\in \NN, \forall n,m>N : \abs{a_n-a_m}<\eps\]
\end{definition}

\begin{definition}
    Consider a sequence of real numbers \(\{a_n\}_{n=1}^{\infty}\), such sequence is called 
    \begin{itemize}
        \item \textbf{non-decreasing} if \(a_{n+1} \geq a_n\) for all \(n\);
        \item \textbf{non-increasing} if \(a_{n+1} \leq a_n\) for all \(n\).
    \end{itemize}
\end{definition}

\begin{theorem}
    If a sequence of real numbers \(\{a_n\}_{n=1}^{\infty}\) is non-decreasing \textbf{and} bounded above then it converges. \\
    If a sequence of real numbers \(\{a_n\}_{n=1}^{\infty}\) is non-increasing \textbf{and} bounded below then it converges.
\end{theorem}

\begin{mdthm}[Bolzano-Weierstrass]
    Let \(\{x_n\}_{n=1}^{\infty}\) be a sequence of real numbers such that \(x_n \in [a,b]\) for all \(n \in \NN\). Then this sequence has a limit point in \([a,b]\). Furthermore, there exists a subsequence \(\{x_{n_k}\}^{\infty}_{n=1}\) which converges to a limit \(\ell \in [a,b]\).
\end{mdthm}

\begin{mdthm}[Cauchy's criterion]
A sequence converges if and only if is a Cauchy sequence.
\end{mdthm}

\subsection{Functions of real variable}

\begin{definition}
    If a function \(f(x)\) is given by an explicit formula, its \textbf{natural domain} is the set of values \(x\in \RR\) such that the formula 'makes sense'; i.e. there is \textbf{no} division by zero, all squares roots and logarithms are taken of non-negative numbers, etc.
\end{definition}

\begin{mdremark}
    The \textit{natural domain} is not a rigorous notion but a rather convenient notion to use in concrete examples.
\end{mdremark}

\begin{example}
    \hphantom{wahoo}
    \begin{itemize}
        \item \(f(x)=\sqrt{4-x^2} \\ 4-x^2 \geq 0\) i.e. \(|x| \leq 2\) so, the natural domain is \([-2,2]\).
        \item \(f(x)=\frac{\log(x-2)}{x-5}\) so the natural domain is \((2,5) \cup (5,\infty)\)
    \end{itemize}
\end{example}

\subsection{Boundedness}

\begin{definition}
    A function is said to be \textbf{bounded} if its range is a bounded set subset of \(\RR\) i.e. if there exists a number \(R>0\) such that \(|f(x)| \leq R \) for all \(x\) in the domain of \(f\). Otherwise, it is said to be \textbf{unbounded}.
\end{definition}

\subsection{Monotonicity}

\begin{definition}
    Let \(f\) be a function on an interval \(\Delta \subset \RR\). Then:
    \begin{itemize}
        \item \(f\) is called \textbf{increasing} on \(\Delta\), if for any \(x_1<x_2\) on \(\Delta\) we have \(f(x_1)<f(x_2)\)
        \item \(f\) is called \textbf{decreasing} on \(\Delta\), if for any \(x_1<x_2\) on \(\Delta\) we have \(f(x_1)>f(x_2)\);
        \item \(f\) is called \textbf{non-increasing} on \(\Delta\), if for any \(x_1 < x_2\) on \(\Delta\) we have \(f(x_1)\leq f(x_2)\);
        \item \(f\) is called \textbf{non-decreasing} on \(\Delta\), if for any \(x_1<x_2\) on \(\Delta\) we have \(f(x_1)\geq f(x_2)\)
    \end{itemize}
\end{definition}

\begin{definition}
    If \(f\) is either non-increasing or non-decreasing on \(\Delta\), it is called \textbf{monotonic}.
\end{definition}

\subsection{Limit of a function}

We define \(c \in \RR\) and let \(f\) be a function defined on a \textbf{punctured neighbourhood} of \(c\), i.e. the function need not be defined at the point \(c\). To exclude \(x=c\) it must be written that \(0<\abs{x-c}<\delta\) which describes the punctured neighbourhood \((c-\delta) \cup (c+\delta)\) of \(c\).

\begin{definition}
    Suppose, \(f : \Delta \to \RR\). We say that \(f(x)\) converges to \(L\) as \(x \to c\) and write \(\limit{x}{c} f(x)=L\), if 
    \[\forall \eps>0, \exists \delta>0, \forall x : \abs{x-c}<\delta \then \abs{f(x)-L}<\eps.\]
\end{definition}

\begin{mdremark}
    A rephrasing of the statement above using neighbourhoods:
    \[\forall \eps>0, \exists \delta>0, \forall x \in (c-\delta, c+\delta) : \abs{f(x)-L} <\eps.\]
\end{mdremark}

\begin{example}
    The function \(\frac{\sin x}{x}\) is defined for all \(x\neq 0\) and \(\limit{x}{0} \frac{\sin x}{x}=1.\)
\end{example}

\begin{definition}
    Let \(f\) be a function defined on \((a,\infty)\) for some \(a\in \RR\). We write \(\limit{x}{\infty} f(x) =L\), if
    \[\forall \eps>0, \exists R\in \RR, \forall x \geq R : \abs{f(x)-L}<\eps.\]
\end{definition}

\subsection{Types of limits}

\begin{definition}
    Let \(f\) be a function on an interval \((a,b)\) and let \(c \in (a,b)\). Then
    \begin{itemize}
        \item \(\limit{x}{{c}_+} f(x)=L_+\) means that \[\forall \eps>0, \exists \delta>0 : c<x<c+\delta \then \abs{f(x)-L_+}<\eps;\]
        \item \(\limit{x}{{c}_-} f(x)=L_-\) means that \[\forall \eps>0, \exists \delta>0 : c-\delta<x<c \then \abs{f(x)-L_-}<\eps.\]
    \end{itemize}
    The values \(L_+\) and \(L_-\) are real numbers called the \textbf{right} and \textbf{left limit} respectively.
\end{definition}

\begin{mdremark}
    The left and right limits do not necessarily have to exist.
\end{mdremark}

\subsection{The \texorpdfstring{\(O\)}{TEXT} and \texorpdfstring{\(o\)}{TEXT} notation}

Let \(f\) and \(g\) be two functions defined on an open interval \(\Delta=(a,b) \in \RR\), and let \(c \in \Delta\). Suppose \(g(x)\neq 0\) for small \(x-c\neq 0.\)

\begin{definition}
    We write \(f(x)=O(g(x))\) as \(x\to c\), if \(\frac{f(x)}{g(x)}\) is bounded for small \(x-c\); i.e. \(\abs{f(x)} \leq M\abs{g(x)}\) for some constant \(M \in \RR\).
\end{definition}

\begin{mdnote}
    Intuitively this means that \(f\) grows slower than \textbf{some} multiple of \(g\).
\end{mdnote}

\begin{theorem}
    The properties of big \(O\).
    \begin{enumerate}
        \item If the function \(f\) can be written as a finite sum of other functions, then the fastest growing one determines the order of \(f(x)\).
        \item Sum: If \(f=O(s)\) and \(g=O(t)\) then \(f+g = O(\max(s,t))\).
        \item Product \(1\): If \(f=O(s)\) and \(g=O(t)\) then \(fg=O(st)\).
        \item Product \(2\): \(f\cdot O(g)=O(fg)\).
        \item Multiplication by a constant: for \(k \neq 0\in \RR\) if \(f=O(g)\) then \(kf=O(g)\).
    \end{enumerate}
\end{theorem}

\begin{mdexample}
    An example of property \(1\); let 
    \[f(n)=9\log n+5(\log n)^4+3n^2+2n^3\]
    then \(f(n)=O(n^3)\) as \(n\to \infty\).
\end{mdexample}

\begin{definition}
    We write \(f(x)=o(g(x))\) as \(x\to c\), if \(\limit{x}{c} \frac{f(x)}{g(x)} =0\).
\end{definition}

\begin{mdremark}
    Big-\(O\) has to be true for at \textbf{least one} constant \(M\), whereas little-\(o\) must hold for \textbf{every} \(\eps>0\). Therefore, little-\(o\) makes for a stronger statement than big-\(O\). \\
    Every function that is little-\(o\) of \(g\) is also big-\(O\) of \(g\), but the converse is false i.e. not every function that is big-\(O\) of \(g\) is also little-\(o\) of \(g\).
\end{mdremark}

\begin{mdnote}
    Intuitively this means that \(f\) \textbf{eventually} grows slower than \textbf{any} multiple of \(g\).
\end{mdnote}

\begin{theorem}
    Properties of little-\(o\):
    \begin{enumerate}
        \item If \(c\) is a non-zero constant and \(f=o(g)\) then \(cf=o(g)\).
        \item If \(f=o(F)\) and \(g=o(G)\) then \(f\cdot g=o(F\cdot G)\).
        \item \textbf{Transitivity:} if \(f=o(g)\) and \(g=o(h)\) then \(f=o(h)\).
    \end{enumerate}
\end{theorem}

\subsubsection{\texorpdfstring{\(O\)}{TEXT} terms in arithmetic expressions}

By a term \(O(g(x))\) in any arbitrary arithmetic expression we mean a function, \(f(x)\) that satisfies the inequality in the definition of big \(O\). In other words, and \(O\)-term can be though as a "box" hiding some unknown function and the only information about these functions is that it satisfies the appropriate inequality.

This is a useful convention which greatly simplifies the notation when working with \(O\)-expressions. For example, this convention allows us to write the relation
\[\log(1+x)-x=O(x^2) \quad \text{for } \abs{x} \leq \half,\] 
as
\[\log(1+x)=x+O(x^2) \quad \text{for } \abs{x} \leq \half.\]
The last equation can be though as the following: "\(\log(1+x)\) \textit{is equal to \(x\) plus a function that, in absolute value, is bounded by a constant times \(x^2\) in the range \(\abs{x} \leq \half\)}". 

We can also interpret it as that the absolute value of the error of \(\log(1+x)-x\) is at most some constant times \(\abs{x^2}\) when for the condition outlined.

\begin{mdremark}
    A term of \(O(1)\) simply stands for a bounded function. For example \(f(x)=g(x)+O(1)\) means \(\abs{f(x)-g(x)} \leq 1\).
\end{mdremark}

\section{Continuity at a point}

\begin{definition}
    Let \(f\) be a function on an open interval \((a,b)\) and let \(c\in (a,b)\). The function \(f\) is said to be \textbf{continuous} at \(c\) if 
    \[\limit{x}{c} f(x) = f(c).\]
    Equivalently, 
    \[\forall \eps>0, \exists \delta>0 \text{ such that } \abs{x-c}<\delta \then \abs{f(x)-f(c)}<\eps,\]
\end{definition}

\begin{mdnote}
    More informally, a continuous function is one whose graph can be drawn without lifting the pencil from the paper.
\end{mdnote}

\begin{mdnote}
    Here \(\delta\) may depend on both \(c\) and \(\eps\) thus, we can rewrite \(\delta=\delta(c,\eps)\).
\end{mdnote}

\begin{theorem}
    (In terms of left and right limits) we say that \(f\) is continuous at \(c\), if the left and right limits of \(f(x)\) as \(x\to c\) exist and \[\limit{x}{{c}_+} f(x) =\limit{x}{{c}_-} f(x) =f(c).\]
\end{theorem}

\begin{mdthm}
    A reformulation of continuity using little-\(o\); we say \(f\)  is continuous at \(c\) if and only if 
    \[f(c+h) = f(c)+o_{h\to 0}(1)\]
\end{mdthm}

\begin{proof}
    Recall the definition of continuity: \(f\) is continuous at \(c\) if and only if \(\limit{x}{c} f(x)=f(c)\). Therefore, set \(x=c+h\) then 
    \[\begin{aligned}
        \limit{h}{0} f(c+h)&=f(c)\\
        \iff f(c+h)-f(c) &= o_{h\to 0}(1)\\
        \iff f(c+h) &= f(c)+o_{h\to 0}(1).
    \end{aligned}\]
\end{proof}

\subsection{Discontinuity}

\begin{definition}
    If a function \(f\) is not continuous at a given point \(x\), we say that \(x\) is a point of \textbf{discontinuity} for \(f\).
\end{definition}

\begin{definition}
    Suppose that \(f\) is defined on an interval \((a,b)\) and \(c \in (a,b)\), the different types of discontinuity are:
    \begin{itemize}
        \item \textbf{Removable discontinuity:} \(f(c)\) is undefined but \(\lim_{x\to c} f(x)\) exists.
        \item \textbf{Point discontinuity:} the left and right limits exists, and they are equal but do not equal the value of \(f\) at \(c\) i.e. \[\limit{x}{c_+} f(x) =\limit{x}{c_-} f(x) \neq f(c).\]
        By redefining the value of \(f\) at \(c\) this discontinuity can be removed.
        \item \textbf{Jump discontinuity:} if both the left and right limit exist, but they are not equal to each other. However, you define \(f(c)\), this will not make \(f\) continuous.
        \item \textbf{Infinite discontinuity:} if at least one of the left or right limits is infinite.
        \item \textbf{Oscillatory discontinuity:} if at least one of the left or right limits \textbf{does not} exist.
    \end{itemize}
\end{definition}

\begin{example}
    Discontinuity examples:
    \begin{itemize}
        \item \textbf{Removable discontinuity:} Let \(f(x) =\frac{\sin(x)}{x}\), \(f(0)\) is undefined but \(\lim_{x\to 0} f(x)=1\).
        \item \textbf{Point discontinuity:} Consider the piecewise function 
        \[\begin{aligned}
            f(x) = \begin{cases}
                x^2 &\text{for } x<1 \\
                0 &\text{for } x=1 \\
                2-x &\text{for } x>1
            \end{cases}
        \end{aligned}\]
        The point \(x_0 =1\) is a \textit{point discontinuity}.
        \begin{figure}[H]
            \begin{center}
                \includegraphics[scale=1.5]{./Resources/Discontinuity_removable.eps.png}
            \end{center}
        \end{figure}
        \item \textbf{Jump discontinuity:}
        Consider the piecewise function 
        \[\begin{aligned}
            f(x) = \begin{cases}
                x^2 &\text{for } x<1 \\
                0 &\text{for } x=1 \\
                2-(x-1)^2 &\text{for } x>1
            \end{cases}
        \end{aligned}\]
        The point \(x_0 =1\) is a \textit{jump discontinuity}.
        \begin{figure}[H]
            \begin{center}
                \includegraphics[scale=1.5]{./Resources/Discontinuity_jump.eps.png}
            \end{center}
        \end{figure}
        \item The function \(f(x)=\frac{1}{x}\) has an \textit{infinite discontinuity} at the origin.
        \item The function \(\sin\left( \frac{1}{x} \right)\) has an \textit{oscillatory discontinuity} at the origin.
    \end{itemize}
\end{example}

% \subsection{Continuity and convergent sequences}

% \begin{theorem}
%     Suppose that \(f\) is defined on \((a,b)\) and \(x_0 \in (a,b)\). Then \(\limit{x}{x_0} f(x)=y_0\) if and only if for every sequence of points \(x_n\in (a,b)\) such that \(\limit{n}{\infty} x_n =x_0\) we have \(\limit{n}{\infty} f(x_n)=y_0.\)
% \end{theorem}

% \begin{corollary}
%     A function \(f\) is continuous at \(c\) if and only if for evert sequence of points \(x_n \in (a,b)\) such that \(\limit{n}{\infty} x_n =x_0\) we have \(\limit{n}{\infty} f(x_n)=f(x_0)\).
% \end{corollary}

\subsection{Properties of continuous functions}

% \begin{theorem}[Sandwich rule]
%     Let \(f,g\) and \(h\) be functions such that \(h(x) \leq f(x) \leq g(x)\) for all \(x\) in some neighbourhood of \(c\) and such that \(h(c)=f(c)=g(c)\). If \(h\) and \(g\) are continuous at \(c\) then so is \(f\).
% \end{theorem}

\begin{theorem}[Composition rule]
    Let \(f\) and \(g\) be continuous functions at \(c\) and \(f(c) \) respectively, such that the composite \(g\circ f\) is defined. Then \(g\circ f\) is continuous at \(c\).
\end{theorem}

\begin{theorem}[Inverse rule]
    Suppose that \(f:A\to B\) is a bijection where \(A\) and \(B\) are \textbf{\ul{compact intervals}}. If \(f\) is continuous on \(A\) then \(f\inv\) is continuous at \(B\).
\end{theorem}

\begin{theorem}
    The algebra of continuous functions:
    \begin{itemize}
        \item \textbf{Sum rule:} if \(f\) and \(g\) are continuous at \(c\) then \(f+g\) is continuous at \(c\).
        \item \textbf{Product rule:} if \(f\) and \(g\) are continuous at \(c\) then \(f\cdot g\) is continuous at \(c\).
        \item \textbf{Reciprocal rule:} if \(f\) is continuous at \(c\) and \(f(c) \neq 0\) then \(\frac{1}{f}\) is continuous at \(c\).
    \end{itemize}
\end{theorem}

\subsection{Monotonic functions}

\begin{mdthm}
    Let \(f\) be a monotonic function on \((a,b)\); then all discontinuities of \(f\) are jump discontinuities.
\end{mdthm}

\begin{mdnote}
    This is because 
    \[\begin{aligned}
        \lim_{x \to c_+} f(x) &= \inf_{x >c} f(x) \\
        \lim_{x \to c_-} f(x) &= \sup_{x <c} f(x).
    \end{aligned}\]
\end{mdnote}

\begin{mdcor}
    Let \(f\) be a monotonic function on \((a,b)\). Then the set of discontinuities of \(f\) is either finite or countably infinite.
\end{mdcor}

\begin{mdremark}
    By countably infinite we mean there exists \(f\) bijective between \(\NN\) and \(D\), the set of discontinuities.
\end{mdremark}

\subsection{Continuity and convergent sequences}

\begin{mdthm}
    We have \(\limit{x}{c} f(x)=y\) if and only if for every sequence of points \(x_n \in (a,b)\) such that \(\limit{n}{\infty} x_n =c\) we have \(\limit{n}{\infty} f(x_n)=y\).
\end{mdthm}

\begin{corollary}
    A function \(f\) is continuous at \(c\) if and only if for every sequence of points \(x_n \in (a,b)\) such that \(\limit{n}{\infty} x_n =c\) we have \(\limit{n}{\infty} f(x_n)=f(c)\).
\end{corollary}

\section{Continuity on an interval}

\begin{definition}
    A function \(f\) is said to be continuous on \([a,b]\) if it is continuous at every point of \((a,b)\) and limits \(\limit{x}{a_+}f(x)\) and \(\limit{x}{b_-}f(x)\) exist and the relation 
    \[f(a)=\limit{x}{a_+}f(x) \quad \text{and} \quad f(b)=\limit{x}{b_-}f(x)\] hold true.
\end{definition}

\begin{definition}
    A function \(f\) is said to be continuous on \([a,b)\) if it is continuous on every point of \((a,b)\) and the limit \(\limit{x}{a_+}f(x)\) exist and \[f(a)=\limit{x}{a_+}f(x).\]
\end{definition}

\begin{definition}
    A function \(f\) is said to be continuous on \((a,b]\) if it is continuous on every point of \((a,b)\) and the limit \(\limit{x}{b_-}f(x)\) exist and \[f(b)=\limit{x}{b_-}f(x).\]
\end{definition}

\begin{definition}
    We denote the set of all functions continuous on \((a,b)\) by \(C(a,b)\). Similarly, \(C[a,b]\) denotes the set of all functions continuous on \([a,b]\). Finally, \(C(\RR)\) denotes the set of all functions continuous on the real line.
\end{definition}

\begin{definition}
    We define a \textbf{compact set} to be a set which both closed and bounded.
\end{definition}

\subsection{Three important theorems}

\begin{mdremark}
    These theorems only hold when the domain of \(f\) is a closed bounded interval i.e. the domain of \(f\) is \([a,b]\).
\end{mdremark}

\begin{mdthm}[Boundedness Theorem]
    If \(f\) is a continuous function on a closed bounded interval then, \(f\) is bounded.
\end{mdthm}

\begin{mdnote}
    This theorem claims that if a continuous function \(f\) takes the values \(\alpha\) and \(\beta\) at the endpoints of some interval \([a,b]\) then \(f\) must take all possible values between \(\alpha\) and \(\beta\).
\end{mdnote}

\begin{proof}
    Suppose \(f\) is a continuous function on \([a,b]\). For the sake of contradiction assume \(f\) is \textit{unbounded}. Then there exists a sequence \(x_n \in [a,b]\) such that
    \[\limit{n}{\infty} \abs{f(x_n)} = +\infty.\]
    By the Bolzano-Weierstrass theorem, there exists a subsequence \(\{x_{n_k}\}_{k=1}^{\infty}\) which converges to \(\ell \in [a,b]\) as \(k \to \infty\). Since \(f\) is continuous we must have
    \[\limit{k}{\infty} f(x_{n_k}) =f(\ell).\]
    However, \(f(\ell)\) is a finite number, but we assume \(\limit{n}{\infty} \abs{f(x_n)} = +\infty\). Thus, we have reached a contradiction.
\end{proof}

\begin{mdcor}
    Let \(a \in \RR\) and let \(f\) be a continuous function on \([a,\infty]\) such that the limit \(\limit{x}{\infty} f(x)\) exists. Then \(f\) is bounded on \([a,\infty)\).
\end{mdcor}

\begin{mdthm}[Max/Min theorem]
    If \(f\) is continuous on a closed bounded interval then, \(f\) attains its maximum and minimum values.
\end{mdthm}

\begin{mdnote}
    We can interpret Theorem \(3.2\) as saying that there exists \(c,d\in [a,b]\) such that \(f(c) \leq f(x) \leq f(d)\) for all 
    \(x\in [a,b]\).
\end{mdnote}

\begin{mdthm}[Intermediate Value Theorem]
    Let \(f\) be a continuous function on a closed bounded interval \([a,b]\). Then \(f\) attains every value between \(f(a)\) and \(f(b)\).
\end{mdthm}

\begin{corollary}
    If \(f\) is a continuous function on the closed bounded interval \([a,b]\), then its range is in the closed interval \([m,M]\) where \(m=\inf(f)\) and \(M=\sup(f)\).
\end{corollary}

\begin{mdnote}
    This theorem claims that if \(f\) is a continuous function on \([a,b]\) then \(f([a,b])\) is a closed bounded interval.
\end{mdnote}

\subsection{Uniform continuity}

\begin{definition}
Let \(f\) be a function on an interval \(\Delta\) (which may be bounded or unbounded). Then \(f\) is called \textbf{uniformly continuous} on \(\Delta\), if 
\[\forall \eps > 0, \exists \delta(\eps)>0 \, \forall x,y \in \Delta \text{ such that } \abs{x-y}<\delta(\eps) \then \abs{f(x)-f(y)}<\eps.\]
\end{definition}

\begin{mdnote}
    In uniform continuity \(\delta\) depends \textbf{only} on \(\eps\).
\end{mdnote}

\begin{mdremark}
    This concept of continuity is stronger than our previous definition as \(\delta\) now only depends on \(\eps\).
\end{mdremark}

\begin{corollary}
    If \(f\) is uniformly continuous on \(\Delta\) then \(f\) is continuous at every point in \(\Delta\).
\end{corollary}

\begin{mdthm}[Cantor's theorem]
    Let \(f\) be a continuous function on a \textbf{closed bounded} interval \([a,b]\). Then \(f\) is uniformly continuous on \([a,b]\).
\end{mdthm}

\begin{mdnote}
    The interval \(\Delta=[a,b]\). Therefore, if \(f\) is continuous on \([a,b]\) then \(f\) is uniformly continuous.
\end{mdnote}

\subsection{Lipschitz continuous functions}

\begin{definition}
    A function \(f\) defined on an interval \(\Delta\) is called \textbf{Lipschitz continuous} on \(\Delta\) if there exists a constant \(C>0\) (called a \textbf{Lipschitz constant}) such that 
    \[\forall x,y \in \Delta: \abs{f(x)-f(y)}\leq C\abs{x-y}.\]
\end{definition}

\begin{mdnote}
    Lipschitz is a stronger statement then uniform which in turn is stronger than continuous.
\end{mdnote}

\begin{mdexample}
    Show that the function \(g:[1,2] \to \RR\) defined by \(g(x)=x^3\) is Lipschitz. \\
    \textbf{Solution:} We will use the fact that \(x^3-y^3 = (x-y)(x^2+xy+y^2)\). Take \(x,y \in [1,2]\) then
    \[\begin{aligned}
        \abs{g(x)-g(y)} &= \abs{x^3-y^3} \\
                        &= \abs{(x-y)(x^2+xy+y^2)} \\
                        &\leq (x^2+\abs{xy}+y^2) \cdot \abs{x-y} \\
                        &\leq (4+4+4)\abs{x-y}.
    \end{aligned}\]
    So, \(g\) is Lipschitz continuous with Lipschitz constant \(12\).
\end{mdexample}

\begin{example}
    Show that \(f :\RR \to \RR\) given by \(f(x)=x^2\) is \textbf{NOT} Lipschitz continuous. That is, for any \(L>0\) find \(x,y \in \RR\) such that 
    \[\abs{f(x)-f(y)} > L\abs{x-y}.\]
    \textbf{Solution:} Choose \(x=2L\) and \(y=L\) then,
    \[\begin{aligned}
        \abs{f(x)-f(y)} &= 3L^2 \\
                        &> L^2 \\
                        &= L\abs{x-y}.
    \end{aligned}\] 
\end{example}

\begin{theorem}
    If \(f\) is Lipschitz continuous on \(\Delta\), then \(f\) is uniformly continuous on \(\Delta\).
\end{theorem}

\begin{proof}
    Choose \(\delta = \frac{\eps}{L}\).
\end{proof}

\subsection{Hölder continuity}

\begin{definition}
    A function \(f\) defined on an interval \(\Delta\) is called \textbf{Hölder continuous} on \(\Delta\) is there exist constants \(C \geq 0\) and \(\alpha>0\) such that 
    \[\abs{f(x)-f(y)} \leq C \abs{x-y}^{\alpha} \forall x,y \in \Delta.\]
\end{definition}

\begin{mdremark}
    For \(0 < \alpha \leq 1\) and \(f\) defined on \([a,b]\) we have the following inclusions
    \[\text{Lipschitz continuous} \subset \alpha\text{-Hölder continuous} \subset \text{uniformly continuous} \subset \text{continuous}.\]
\end{mdremark}

\subsubsection{How to check for uniform continuity}

\begin{itemize}
    \item If \(f\) is differentiable on \(\Delta\), check if its derivative is bounded on \(\Delta\). If it is then \(f\) is Lipschitz continuous hence, it is uniformly continuous.
    \item If we can prove \(f\) is Lipschitz continuous then \(f\) is uniformly continuous.
    \item If \(f\) is continuous on \([a,b]\) then \(f\) is uniformly continuous.
    \item If all above fails prove that \(f\) is not uniformly continuous. 
    \begin{itemize}
        \item Identify a point in the domain (finite or infinite) such that \(f\) grows or oscillates near that point.
        \item Construct sequences \(x_n\) and \(x_n'\) converging to the point identified from above such that there exists a \(\abs{x_n-x_n'} \to 0\) and \(\abs{f(x_n)-f(x_n')} \geq \eps>0\).
    \end{itemize} 
\end{itemize}

\section{Differentiation I: theory}

\subsection{The derivative}

\begin{definition}\label{def:differentiable}
    Let \(f\) be a function defined on the interval \(\Delta\), let \(c \in \Delta\). Then we say \(f\) is \textbf{differentiable} at a point \(c\) if there exists a real number denoted by \(f'(c)\) such that 
    \[\limit{x}{c} \frac{f(x)-f(c)}{x-c} = f'(c).\]
    The number \(f'(c)\) is called the \textbf{derivative} of \(f\) at the point \(c\).
\end{definition}

\begin{mdnote}
    We can reformulate Definition \ref{def:differentiable} by setting \(x=c+h\), then 
    \[f'(c)=\limit{x}{c} \frac{f(x)-f(c)}{x-c} = \limit{h}{0} \frac{f(c+h)-f(c)}{h}.\]
\end{mdnote}

\begin{mdthm}
    If \(f\) is differentiable at \(c\) then \(f\) is continuous at \(c\).
\end{mdthm}

\begin{proof}
    Recall the definition of continuity: \(f\) is continuous at \(c\) if \(\limit{x}{c} f(x) =f(c)\). Suppose \(f\) is differentiable so,
    \[f'(c) = \limit{x}{c} \frac{f(x)-f(c)}{x-c}\]
    exists. We know want to show \(\limit{x}{c} f(x) -f(c)=0\), as such we can reformulate the expression as follows
    \[\begin{aligned}
        \limit{x}{c} f(x) -f(c) &= \limit{x}{c} (f(x) -f(c)) \left( \frac{x-c}{x-c} \right) \\
        &= \limit{x}{c}\left( \frac{f(x)-f(c)}{x-c} \right) (x-c) \\
        &= \underbrace{\left( \limit{x}{c}\frac{f(x)-f(c)}{x-c} \right)}_{f'(c)}\underbrace{\limit{x}{c}(x-c)}_{0} \\
        &= f'(c) \cdot 0 \\
        &= 0.
    \end{aligned}\]
    Therefore, we have \(\limit{x}{c} f(x) -f(c)=0\) which implies \(\limit{x}{c} f(x) =f(c)\) i.e. \(f\) is continuous at \(c\).
\end{proof}

\begin{proof}
    Using little-\(o\) notation one can rewrite the definition of the derivative as follows:
    \[f(c+h)=f(c)+hf'(c)+o_{h\to 0 }(h),\]
    therefore,
    \[f(c+h)=f(c)+o_{h\to 0}(1).\]
    Which is the definition of continuity.
\end{proof}

\begin{definition}
    We define the \textbf{left derivative} as 
    \[\limit{x}{c_-} \frac{f(x)-f(c)}{x-c}=f'_-(c).\]
\end{definition}

\begin{definition}
    We define the \textbf{right derivative} as 
    \[\limit{x}{c_+} \frac{f(x)-f(c)}{x-c}=f'_+(c).\]
\end{definition}

\begin{theorem}
    If the left and right derivative of a function exists at point, and they are equal then the derivative of that function at that point exists; i.e. \(f'_+(c)=f'_-(c)=f'(c)\).
\end{theorem}

\subsubsection{Linear approximation}

We can reformulate the definition of the derivative using little-\(o\) notation. A function \(f\) is differentiable at \(x\) if and only if there is a number \(f'(x)\) so that 
\[f(x+h)=f(x)+hf'(x)+o_{h\to 0}(h).\]
Therefore, we have that a function \(f\) is approximated by a linear one with a slope of \(f'(x)\).

\subsubsection{Function differentiable on an interval}

\begin{definition}
    We say that \(f\) is \textbf{differentiable} on \(\Delta\), if the derivative of \(f\) exists at every point \(x\in \Delta\).
\end{definition}

\begin{definition}
    We say that \(f\) is \textbf{continuously differentiable} on \(\Delta\), if \(f\) is differentiable on \(\Delta\) and the derivative \(f'\) is continuous on \(\Delta\).
\end{definition}

\begin{mdremark}
    We denote the set of all continuously differentiable functions on \(\Delta\) by \(C^1(\Delta)\). In other words, \(f\in C^1(\Delta)\) means \(f'\in C(\Delta)\).
\end{mdremark}

\subsection{The algebra of differentiation}

\begin{mdthm}[Inverse rule]
    Suppose that \(f(g(x)) =x\) i.e. \(g=f\inv\) and \(g\) is differentiable at \(x\) and \(f\) is differentiable at \(g(x)\) then 
    \[\begin{aligned}
        g'(x)&=\frac{1}{f'(g(x))} \\
        [f\inv(x)]'&=\frac{1}{f'[f\inv(x)]}.
    \end{aligned}\]
\end{mdthm}

\subsection{The Mean Value Theorem}

\begin{definition}[Extrumum points]
    Let \(f\) be defined on an interval \(\Delta\). We say that \(c \in \Delta\) is a
    \begin{itemize}
        \item \textbf{minimum} of \(f\) on \(\Delta\) if \(f(c) \leq f(x)\).
        \item \textbf{maximum} of \(f\) on \(\Delta\) if \(f(c) \geq f(x)\).
        \item \textbf{local minimum} of \(f\), if \(c\) is a minimum for \(f\) is come interval \(\Delta \cap (c-\eps,c+\eps)\) for some \(\eps>0\).
        \item \textbf{local maximum} of \(f\), if \(c\) is a maximum for \(f\) is come interval \(\Delta \cap (c-\eps,c+\eps)\) for some \(\eps>0\).
    \end{itemize}
\end{definition}

\begin{mdremark}
    Maxima and minima are collectively known as \textbf{extrema}.
\end{mdremark}

\begin{theorem}[Fermat's theorem]
    Let \(f\) be a differentiable function in some interval \(\Delta\). If \(f\) has a local maximum or a local minimum at \(c \in \Delta\) then \(f'(c)=0\).
\end{theorem}

\begin{mdremark}
    In general \(f\) does not have to be differentiable at the point where it has a local minimum or maximum. For example, \(f(x)=\abs{x}\) attains its (global) minimum at \(x=0\), but \(f'(0)\) does not exist.
\end{mdremark}

\begin{theorem}[Rolle's Theorem]
    Let \(f\) be a continuous function on \([a,b]\) such that \(f(a)=f(b)\). If \(f\) is differentiable on \((a,b)\) then there exists a point \(c \in (a,b)\) at which \(f'(c)=0\).
\end{theorem}

\begin{mdnote}
    Rolle's theorem leads to \(2\) consequences on the behaviour of the function:
    \begin{itemize}
        \item \(f\) has more than \(1\) derivatives which are equal to \(0\);
        \item every point derivative is equal to \(0\), i.e. \(f\) is a constant function.
    \end{itemize}
    Therefore, on the interval \([a,b]\) the function is either constant or oscillates between \([a,b]\).
\end{mdnote}

\begin{proof}
    Since \(f\) is continuous on \([a,b]\), by the boundedness theorem it attains a maximum value \(f(c_1)\) and a minimum value \(f(c_2)\) on \([a,b]\). If \(f(c_1)=f(c_2)\) then \(f\) is constant for all \(x\in [a,b]\) as a result \(f'(x)=0\) for all \(x \in [a,b]\) and the result follows. If \(f(c_1) \neq f(c_2)\) then at least one of \(c_1\) and \(c_2\) is not at \(a\) or \(b\). Hence, \(f\) has a local minimum or maximum (or both) in the interval \([a,b]\). By Fermat's theorem \(f'\) is zero at least at a point in \([a,b]\).
\end{proof}

\begin{mdthm}[Mean Value Theorem]
    Let \(f\) be a continuous function on \([a,b]\). If \(f\) is differentiable on \((a,b)\) then there exists a point \(c\in (a,b)\) at which 
    \[f'(c) = \frac{f(b)-f(a)}{b-a}.\]
\end{mdthm}

\begin{mdnote}
    In general, we define \(f\) to be differentiable on \((a,b)\) instead of \([a,b]\) because the derivative is defined as the left and right limits of the derivative which can only be defined if the interval is \((a,b)\).
\end{mdnote}

\begin{mdnote}
    The Mean Value Theorem is more powerful in the following rearrangement:
    \[f(b)-f(a)=f'(c)(b-a),\]
    because it can be used to prove Lipschitz continuity.
\end{mdnote}

\begin{proof}
    Let \(f\) be a function differentiable on the open interval \((a, b)\) and continuous on the closed interval \([a, b]\). Define \(F (x) = f (x) - kx\) where \(k\) is chosen such that \(F (a) = F (b)\), then we have \(F(a)=f(a)-ka\) and \(F(b)=f(b)-kb\) so,
    \[\begin{aligned}
        f(a)-ka&=f(b)-kb \\
        k (b-a) &= f(b)-f(a) \\
        k&=\frac{f(b)-f(a)}{b-a}.
    \end{aligned}\]
    By applying Rolle's Theorem to \(F\) yields a \(c \in (a,b)\) such that \(F'(c)=0\), that is \(f'(c)-k=0\). Hence,
    \[f'(c) = \frac{f(b)-f(a)}{b-a}.\]
\end{proof}

\begin{mdexample}
    Let \(f\) be a function on \([a,b]\) which is \textbf{differentiable} on \((a,b)\) and \textbf{continuous} on \([a,b]\). Prove this function is Lipschitz continuous. \\
    \textbf{Solution:} Applying the MVT:
    \[\begin{aligned}
        f(b)-f(a) &= f'(c)(b-a) \quad \text{for some } x\in(a,b) \\
        \abs{f(b)-f(a)} &=\abs{f'(c)(b-a)} \\
        \abs{f(b)-f(a)} &=\abs{f'(c)}\abs{b-a}. \\
    \end{aligned}\]
    Using the Boundedness Theorem on \(f'\) we know that since it is a continuous function on a closed bounded interval we have that \(f'\) is bounded i.e. \(\abs{f'(c)}\leq M\) for some \(M >0\in \RR\). So we have,
    \[\begin{aligned}
        \abs{f(b)-f(a)} &=\abs{f'(c)}\abs{b-a} \\
        \then \abs{f(b)-f(a)} &\leq M\abs{b-a}.
    \end{aligned}\]
    Which implies \(f\) is Lipschitz continuous.
\end{mdexample}

\begin{example}
    Using the MVT, let us prove the inequality
    \[\abs{\sin x -\sin y} \leq \abs{x-y}.\]
    \textbf{Solution:} Without loss of generality, assume \(x<y\) and apply the MVT to \([x,y]\); we get 
    \[\frac{\sin x -\sin y}{x-y} =\cos c\]
    for some \(c\in (a,b)\). Since \(\abs{\cos c} \leq 1\), this give us the required inequality.
\end{example}

\subsection{Taylor's formula}

\begin{definition}
    We say that \(f\) is \(n\) times differentiable on \((a,b)\) if each derivative of order up to \(n\) exists at every point of the interval.
\end{definition}

\begin{definition}
    We say that \(f\) is \(n\) times continuously differentiable if the final \(n\)-th derivative is continuous on \((a,b)\).
\end{definition}

\begin{mdnote}
    The function \(f\) itself and its first \((n-1)\) derivatives are automatically continuous.
\end{mdnote}

\begin{mdremark}
    The notation \(f\in C^n(a,b)\) means that \(f,f',\ldots, f^{(n)} \in C(a,b)\).
\end{mdremark}

\begin{theorem}[Taylor's formula]
    If \(f\in C^n(a,b)\) and \(z \in (a,b)\), then for each \(x \in (a,b)\) there exists a point \(c\) between \(z\) and \(x\) such that 
    \[\begin{aligned}
        f(x) &= \sum_{k=0}^{n-1} \frac{f^{(k)}(z)}{k!}(x-z)^k + \frac{f^{(n)}(c)}{n!}(x-z)^n \\
        &= f(z)+ f'(z)(x-z)+\cdots + \frac{f^{(n-1)}(z)}{(n-1)!}(x-z)^{n-1} + \frac{f^{(n)}(c)}{n!}(x-z)^n.
    \end{aligned}\]
\end{theorem}

\begin{mdremark}
    Using \(n=1\) gives the Mean Value Theorem.
\end{mdremark}

\begin{mdthm}[Reformulation of Taylor's formula]
    Since \(f^{(n)}\) is continuous on \((a,b)\), it is bounded on a neighbourhood of \(z\). Thus, 
    \[\frac{f^{(n)}(c)}{n!}(x-z)^n = O_{x\to z}[(x-z)^n],\]
    so Taylor's formula can be written as 
    \[\begin{aligned}
        f(x) &= \sum_{k=0}^{n-1} \frac{f^{(k)}(z)}{k!}(x-z)^k +  O_{x\to z}[(x-z)^n] \\
        &=f(z)+ f'(z)(x-z)+\cdots + \frac{f^{(n-1)}(z)}{(n-1)!}(x-z)^{n-1} + O_{x\to z}[(x-z)^n].
    \end{aligned}\]
\end{mdthm}

\begin{mdthm}[Local extrema theorem]
    Suppose that
    \[\begin{aligned}
        f'(a) = \cdots &= f^{(n-1)}(a) =0 \\
        f^{(n)}(a)&\neq 0.
    \end{aligned}\]
    \begin{enumerate}
        \item If \(n\) is \underline{even} and \(f^{(n)}>0\) then, \(f\) has a local minimum at \(a\).
        \item If \(n\) is \underline{even} and \(f^{(n)}<0\) then, \(f\) has a local maximum at \(a\).
        \item If \(n\) is \underline{odd} then, \(f\) has no local extremum.
    \end{enumerate}
\end{mdthm}

\subsection{Asymptotic expansions}

\begin{definition}
    Let \(f\) be defined on \((a,b)\) and let \(z\in (a,b)\). Let \(c_0,c_1,c_2, \cdots \in \RR\). We say that \(f\) has an \textbf{asymptotic expansion} as near \(z\),
    \[
        f(x) \sim \sum_{n=0}^{\infty} c_n (x-z)^n \quad \text{as } x\to z,
    \]
    if for every \(N\in \NN\) we have the asymptotic formula
    \[
        f(x) = \sum_{n=0}^{N} c_n (x-z)^n +O(x^{N+1}) \quad \text{as } x\to z.
    \]
\end{definition}

\begin{mdnote}
    Asymptotic expansions just mean to expand a function near \(z\) with Taylor's formula.
\end{mdnote}

\begin{mdexample}
    Compute the first three non-trivial of the Taylor's expansion for
    \[f(x)=\frac{x}{e^x-1} \quad \text{as } x\to 0.\] 
    \textbf{Solution:} Using Taylor's formula for the exponential we obtain
    \[\begin{aligned}
        \frac{x}{e^x-1} &= \frac{x}{x+\frac{x^2}{2!}+\frac{x^3}{3!}+O(x^4)} \\
        &= \frac{1}{1+\frac{x}{2!}+\frac{x^2}{3!}+O(x^3)}.
    \end{aligned}\]
    By setting \(\eps =1+\frac{x}{2!}+\frac{x^2}{3!}+O(x^3)\) we can use the expansion \(\frac{1}{1+\eps} =1 -\eps+\eps^2 + O(\eps^3)\) to continue. So,
    \[\begin{aligned}
        \frac{x}{e^x-1} &= \frac{1}{1+\frac{x}{2!}+\frac{x^2}{3!}+O(x^3)} \\
        &= 1- \left( \frac{x}{2}+\frac{x^2}{3!} +O(x^3) \right) +\left( \frac{x}{2}+\frac{x^2}{3!} +O(x^3) \right)^2 +O(x^3) \\
        &= 1- \frac{x}{2} -\frac{x^2}{6} +\frac{x^2}{4} +O(x^3) \\
        &= 1- \frac{x}{2} +\frac{x^2}{12} +O(x^3).
    \end{aligned}\]
\end{mdexample}

\section{Differentiation II: Applications}

\subsection{Monotonicity and Extrema}

\begin{theorem}
    Let \(f\) be a continuous and differentiable on \((a,b)\).
    \begin{itemize}
        \item If \(f'(x)>0\) for all \(x\in (a,b)\) then the function \(f\) is \textbf{strictly increasing}.
        \item If \(f'(x)<0\) for all \(x\in (a,b)\) then the function \(f\) is \textbf{strictly decreasing}.
        \item If \(f'(x)\geq 0\) for all \(x\in (a,b)\) then the function \(f\) is \textbf{non-decreasing}.
        \item If \(f'(x)\leq 0\) for all \(x\in (a,b)\) then the function \(f\) is \textbf{non-increasing}.
    \end{itemize}
\end{theorem}

\begin{corollary}
    If \(f\) is differentiable on an interval \((a,b)\) and \(f'(x)=0\) for all \(x\in (a,b)\), then \(f\) is constant on \((a,b)\).
\end{corollary}

\begin{mdthm}
    Let \(f\) be continuous and twice differentiable on \((a,b)\) and let \(c \in (a,b)\).
    \begin{itemize}
        \item If \(f'(c) = 0\) and \(f''(c)>0,\) then \(f\) has a \textbf{local minimum} at \(c\).
        \item If \(f'(c) = 0\) and \(f''(c)<0,\) then \(f\) has a \textbf{local maximum} at \(c\).
    \end{itemize}
\end{mdthm}

\subsection{Convexity and the second derivative}

\begin{definition}
    Let \(f\) be a continuous function on an interval \(\Delta\). We say that \(f\) is \textbf{convex} on \(\Delta\), if for any \(a,b \in \Delta\) and for any \(\theta \in [0,1]\) we have
    \[f(a \theta +(1-\theta)b)\leq \theta f(a) +(1-\theta)f(b).\]
\end{definition}

\begin{definition}
    We say that \(f\) is \textbf{concave} on \(\Delta\) if \((-f)\) is convex on \(\Delta\).
\end{definition}

\begin{mdnote}
    Geometrically, if a function is 
    \begin{itemize}
        \item \textbf{convex}, then the graph of \(f\) between the points \(a\) and \(b\) lies \textbf{below} the secant line connecting \(a\) and \(b\).
        \item \textbf{concave}, then the graph of \(f\) between the points \(a\) and \(b\) lies \textbf{above} the secant line connecting \(a\) and \(b\).
    \end{itemize}
\end{mdnote}

We can think of convex curves as graphs which look like a "smiley-face". Whereas, concave curves as graphs which look like a "frowny face". The figure below illustrates this property.

\begin{figure}[H]
    \begin{center}
        \includegraphics[scale=0.32]{./Resources/Convex and Concave.png}
    \end{center}
\end{figure}

\begin{theorem}
    A curve is 
    \begin{itemize}
        \item \textbf{convex} when \(f''(x)>0\);
        \item \textbf{concave} when \(f''(x)<0\).
    \end{itemize}
\end{theorem}

\begin{definition}
    A point where the curve changes between concave and convex (and vice versa) i.e. where \(f''(x)\) changes between positive and negative is called a \textbf{point of inflection}.
\end{definition}

\begin{theorem}
    At a point of inflection we have \(f''(x)=0\).
\end{theorem}

\begin{mdremark}
    The converse is not true i.e. not all points at which \(f''(x)=0\) are points of inflection.
\end{mdremark}

\begin{mdnote}
    To determine a point of inflection we need to analyse the sign of \(f''\) at either side at the point for which \(f''(x)=0\). If the sign changes then we have a point of inflection.
\end{mdnote}

\begin{example}
    Consider \(f(x)=x\). Clearly, \(f''(x)=0\) for all \(x\) therefore, it has no points of inflection, as the curve is never concave or convex.
\end{example}

\begin{example}
    Consider \(f(x)=x^3-6x^2+9x+1\). We have that \(f''(x)=6x-12\) so \(f''(x) = 0\) when \(x=2\). It follows that \(f''(x)<0\) for \(x<2\) and \(f''(x)>0\) for \(x>2\). The curve changes from concave to convex at this point so, there is a point inflection at \(x=2\).
\end{example}

\subsection{Global extrema}

\subsubsection*{How to find extrema of a differentiable function}

Suppose \(f\) is a differentiable function on \([a,b]\), then to find the extrema of \(f\) we:

\begin{enumerate}
    \item find all points \(c_1,c_2 \ldots, c_n \in [a,b]\) at which \(f'(c_k)=0\);
    \item evaluate \(f(c_k)\) for all \(k\);
    \item evaluate \(f\) at the endpoints of the interval, i.e. \(a\) and \(b\);
    \item from the list \(f(a),f(b),f(c_1), \ldots,\) select the extrema (or extremum).
\end{enumerate}

\begin{mdremark}
    The function \(f\) may have several global extrema on \([a,b]\).
\end{mdremark}

\begin{example}
    Consider the function \(f(x)=x^5-5x^3+10x\) on the interval \([0,2]\). Find the extrema of \(f\). \\
    \textbf{Solution:} We have \(f'(x)=5x^4-15x^2+10 = 5(x^2-2)(x^2-1)\); the equation \(f'(x)=0\) has solutions at \(x=1\) and \(x=\sqrt{2}\) in the interval \([0,2]\). Therefore, we have 
    \[\begin{aligned}
        f(0) &= 0, \\
        f(1) &= 6, \\
        f(\sqrt{2}) &= 4\sqrt{2} \approx 5.65 \\
        f(2) &=12.
    \end{aligned}\]
    The minimum of \(f\) is attained at \(x=0\) and the maximum at \(x=2\) (the point \(x=1\) is a local maximum and \(x=\sqrt{2}\) is a local minimum, but none of them are global extremum).
\end{example}

\subsection{Behaviour at infinity}

\begin{definition}
    If \(f(x)=ax+b +o(1)\) as \(x\to \infty\) we sat that the straight line \(y=ax+b\) is an \textbf{asymptote} of the graph of \(f\).
\end{definition}

\begin{mdremark}
    To find the slant (or oblique) asymptote of a rational function, we can perform:
    \begin{itemize}
        \item polynomial division in which case the polynomial part is the asymptote;
        \item asymptotic expansion as \(x\to \infty\).
    \end{itemize}
\end{mdremark}

\begin{theorem}
    The asymptotic expansion of a function as \(x\to \infty\) is given by 
    \[f(x) \sim \sum_{n=0}^N \frac{c_n}{x^n}, \quad x\to \infty.\]
    Equivalently,
    \[f(x) = \sum_{n=0}^N \frac{c_n}{x^n}+o(x^{-N}), \quad x\to \infty.\]
\end{theorem}

\begin{mdnote}
    In comparison to the asymptotic expansion as \(x\to z\) the error term in terms of negative powers of \(x\).
\end{mdnote}

\begin{mdexample}
    Consider \(f(x)=\sqrt{x^2+x}\) as \(x\to \infty\). Using \(\sqrt{1+\eps} = 1+\half\eps -\frac{1}{8}\eps^2+O(\eps^3)\) as \(\eps \to 0\).
    \textbf{Solution:}
    \[\begin{aligned}
        \sqrt{x^2+x} &= x\sqrt{1+\frac{1}{x}}\\
                    &=x\left( 1+\frac{1}{2x}-\frac{1}{8x^2} +O(x^{-3}) \right) \\
                    &=x+\half -\frac{1}{8x}+O(x^{-2}) \quad x\to \infty.
    \end{aligned}\]
    So, \(y=x+\half\) is an asymptote as \(x\to \infty\).
\end{mdexample}

\subsection{Graph sketching}

To sketch the graph of a concrete function \(f(x)\), given by an explicit formula, follow these steps:

\begin{enumerate}
    \item Find the \textbf{natural domain} of the function (the subset of \(\RR\) where \(f\) is well-defined);
    \item Investigate the limiting behaviour at the boundary of the natural domain, determine asymptotes if they exist;
    \item Find intervals of continuity and determine discontinuities and their type;
    \item Find \textbf{zeros} and intervals of constant sign (or when \(f>0\) or \(f < 0\));
    \item Find \textbf{extrema} and intervals of monotonicity;
    \item Find the intervals of \textbf{convexity} and/or \textbf{concavity}.
\end{enumerate}

\begin{example}
    Sketch the graph of the function \(f(x)=\frac{x^4}{(1+x)^3}\) on its natural domain.
\end{example}

\begin{example}
Sketch the graph of the function \(f(x)=\frac{\ln(x)}{\sqrt{x}}\) on its natural domain.
    \begin{solution}
    The natural domain is \((0,\infty)\). We have 
    \[\lim_{x\to 0^+} f(x)=-\infty \quad \text{and} \quad \lim_{x\to \infty} f(x)=0.\]
    The function is negative for \(x<1\) and positive for \(x>1\), with a zero at \(x=1\). Computing the derivatives we have 
    \[f'(x)=\frac{2-\ln(x)}{2x\sqrt{x}} \quad \text{and} \quad f''(x)=\frac{3\ln(x)-8}{4x^2\sqrt{x}}.\]
    We find that the function is increasing on \((0,e^2)\) and decreasing on \((e^2,\infty)\) with a local maximum at \(e^2\). The function is concave on \((0,e^\frac{8}{3})\) and convex on \((e^\frac{8}{3},\infty)\) with an inflection point at \(e^\frac{8}{3}\). 
    \end{solution}
\end{example}

\section{Integration I: Integrability}

\begin{definition}
    Let \([a,b]\) be a given interval. A \textbf{partition}, \(\mathcal{P}\), of \([a,b]\) is a finite set of points \(\{x_0,x_1,\ldots,x_n\}\) satisfying
    \[a=x_0<x_1<x_2<\cdots<x_{n-1}<x_n=b,\]
    i.e. \(\mathcal{P}\) is the collection of intervals
    \[\mathcal{P}=\{[x_0,x_1], [x_1,x_2],\ldots,[x_{n-1},x_n]\}.\]
    We write \(\Delta x_i=x_i-x_{i-1}\) for \(i \in \{1,2,\ldots, n\}\).
\end{definition}

\begin{mdnote}
    In other words, we divided the interval \([a,b]\) into \(n\) smaller intervals:
    \[[a,b]=[x_0,x_1] \cup [x_1,x_2]\cup [x_2,x_3]\cup \cdots \cup [x_{n-1},x_n].\]
\end{mdnote}

\begin{definition}
    Let \(f\) be a bounded real-valued function on \([a,b]\) and let \(\mathcal{P}\) be a partition of \([a,b]\).
    \begin{itemize}
        \item The \textbf{upper Riemann sum} of \(f\) is defined by
        \[U(\mathcal{P},f)=\sum_{i=1}^n M_i\Delta x_i \quad \text{where} \quad  M_i=\sup\{f(x) : x_{i-1} \leq x \leq x_i\}.\]
        \item The \textbf{lower Riemann sum} of \(f\) is defined by
            \[L(\mathcal{P},f)=\sum_{i=1}^n m_i\Delta x_i \quad \text{where} \quad m_i=\inf\{f(x) : x_{i-1} \leq x \leq x_i\}.\]
    \end{itemize}
\end{definition}

\begin{definition}
    Let \(f\) be a \textit{bounded} real-valued function on \([a,b]\). We set
    \[\begin{aligned}
        \mathcal{L}=\overline{\int_a^b} f(x) \, dx&=\inf_\mathcal{P} U(\mathcal{P},f) \\
        \mathcal{U}=\underline{\int_a^b} f(x) \, dx&=\sup_\mathcal{P} L(\mathcal{P},f),
    \end{aligned}\]
    where the infimum and supremum are taken over all partitions \(\mathcal{P}\) of \([a,b]\).
\end{definition}

\begin{definition}
    Let \(f\) be a \textit{bounded} real-valued function on \([a,b]\). We say \(f\) is \textbf{Riemann integrable} on \([a,b]\) if and only if \(\mathcal{L}=\mathcal{U}\). This common value is denoted by
    \[\int_a^b f(x) \, dx.\]
\end{definition}

\begin{mdremark}
    We use the notation \(f \in \mathcal{R}[a,b]\) to denote that \(f\) is Riemann integrable on \([a,b]\).
\end{mdremark}

\begin{corollary}
    If \(f\) defined and bounded on \([a,b]\) then \(\mathcal{L} \leq \mathcal{U}\)
\end{corollary}

\subsection{Riemann's criterion}

\begin{definition}
    We say that a partition \(\mathcal{P}^*\) is a \textbf{refinement} of \(\mathcal{P}\) if \(\mathcal{P} \subset \mathcal{P}^*\); i.e. every point of \(\mathcal{P}\) is a point of \(\mathcal{P}^*\).
\end{definition}

\begin{mdnote}
    A refinement must contain all the points of \(\mathcal{P}\) thus the refinement \(\mathcal{P}^*\) has more points than \(\mathcal{P}\) i.e. it is divided into more subintervals hence, it is more \textit{refined}.
\end{mdnote}

\begin{mdexample}
    Suppose \(\mathcal{P}\) is the partition of \([0,1]\):
    \[0<0.1<0.3<0.7<1\]
    Then a refinement of \(\mathcal{P}\), we call \(\mathcal{P}^*\) is a partition of the interval \([0.1]\):
    \[\mathbf{0}<\mathbf{0.1}<0.2<\mathbf{0.3}<0.5<\mathbf{0.7}<0.8<0.9<0.96<\mathbf{1}.\]
    The numbers in bold are both in \(\mathcal{P}\) and \(\mathcal{P}^*\). 
\end{mdexample}

\begin{definition}
    Given two partitions, \(\mathcal{P}_1\) and \(\mathcal{P}_2\), we say that \(\mathcal{Q}=\mathcal{P}_1 \cup \mathcal{P}_2\) is the \textbf{common refinement} of \(\mathcal{P}_1\) and \(\mathcal{P}_2\).
\end{definition}


\begin{lemma}[Refinement lemma]
    If \(\mathcal{P}^*\) is a refinement of \(\mathcal{P}\) then,
    \[\begin{aligned}
        L(\mathcal{P},f) &\leq L(\mathcal{P}^*,f) \\
        U(\mathcal{P},f) &\geq U(\mathcal{P}^*,f).
    \end{aligned}\]
\end{lemma}

\begin{mdthm}[Riemann's criterion]
    Let \(f\) be defined and bounded on \([a,b]\). Then \(f \in \mathcal{R}[a,b]\) if and only if for every \(\eps>0\) there exists a partition \(\mathcal{P}\) such that
    \[U(\mathcal{P},f)-L(\mathcal{P},f) <\eps.\]
\end{mdthm}

\subsection{Monotonic functions}

\begin{mdthm}
    Let \(f\) be bounded and monotonic on \([a,b]\); then \(f \in \mathcal{R}[a,b]\).
\end{mdthm}

\subsection{Continuous functions}

\begin{mdthm}
    If \(f\) is continuous on \([a,b]\), then \(f \in \mathcal{R}[a,b]\).
\end{mdthm}

\subsection{Piecewise functions}

\begin{mdlemma}
    Let \(a<c<b\) and let \(f\) be a bounded function on \([a,b]\) such that \(f \in \mathcal{R}[a,c]\) and \(f\in \mathcal{R}[c,b]\). Then, \(f\in \mathcal{R}[a,b]\).
\end{mdlemma}

\subsection{Composition of functions}

\begin{mdthm}
    Let \(f\in \mathcal{R}[a,b]\) and let \(\varphi\) be a \textit{Lipschitz continuous function} on \(\RR\). Then \(\varphi \circ f \in \mathcal{R}[a,b]\).
\end{mdthm}

\subsection{Oscillatory discontinuities}

\begin{mdlemma}
    Let \(f\) be bounded on \([a,b]\) and suppose that \(f \in \mathcal{R}[a+\eps,b-\eps]\) for all sufficiently small \(\eps>0\). Then \(f \in \mathcal{R}[a,b]\).
\end{mdlemma}

\begin{example}
    Define \(f(x)=\sin\left( \frac{1}{x} \right)\) for \(x\in (0,1]\) and \(f(0)=0\). Then \(f\) is bounded on \([0,1]\) and \(f\in C[z\eps,1]\), for any \(0<\eps<1\). Thus, \(f\in \mathcal{R}[\eps,1]\).
\end{example}

\begin{mdthm}
    Let \(f\) be bounded on \([a,b]\) and suppose that \(f\) has finitely many points of discontinuity on \([a,b]\). Then \(f \in \mathcal{R}[a,b]\).
\end{mdthm}

\section{Integration II}

\subsection{Properties of the Riemann integral}

\begin{mdthm}
    If \(f,g \in \mathcal{R}[a,b]\) then all the following integrals exists and have the following properties:
    \begin{itemize}
        \item \(\int_a^b f(x) \, dx =\int_a^c f(x) \, dx +\int_c^b f(x) \, dx\).
        \item \(\int_a^b \alpha f(x) +\beta g(x) \, dx = \alpha\int_a^b f(x) \, dx+\beta\int_a^b g(x) \, dx\).
        \item If \(a<b\), then \(\int_a^b f(x) \, dx =-\int_b^a f(x) \, dx\).
    \end{itemize}
\end{mdthm}

\subsection{Inequalities for the Riemann integral}

\begin{theorem}
    If \(f,g\in \mathcal{R}[a,b]\) are such that \(f\leq g\) on \([a,b]\) then,
    \[\int_a^b f(x) \, dx \leq \int_a^b g(x) \, dx.\]
\end{theorem}

\begin{mdthm}
    If \(f\in \mathcal{R}[a,b]\) then \(\abs{f} \in \mathcal{R}[a,b]\) and 
    \[\abs{\int_a^b f(x) \, dx} \leq \int_a^b \abs{f(x)} \, dx \leq (b-a) \sup_{x\in[a,b]} \abs{f(x)}.\]
\end{mdthm}

\begin{mdnote}
    By \(\sup_{x\in [a,b]}\abs{f(x)}\) we mean \(\sup\left\{\abs{f(x)} : x\in[a,b]\right\}\).
\end{mdnote}

\begin{mdthm}[Jensen's inequality]
    Let \(f \in \mathcal{R}[a,b]\) and let \(\varphi\) be a \textbf{convex} function defined on the range of \(f\). Then \(\varphi \circ f \in \mathcal{R}[a,b]\) and 
    \[\varphi\left( \frac{1}{b-a}\int_a^b f(x) \, dx \right) \leq \frac{1}{b-a} \int_a^b \varphi \circ f(x) \, dx.\]
\end{mdthm}

\subsection{The fundamental theorem of calculus}

\begin{mdthm}[Fundamental Theoreom of Calulus part I]
    Let \(f\in \mathcal{R}[a,b]\) and define \(F(x)=\int_a^x f(t)\, dt\) for \(x\in [a,b]\). Then \(F\) is continuous on \([a,b]\). Furthermore, if it is continuous at a point \(c \in (a,b)\) then \(F\) is differentiable at \(c\) and \(F'(c)=f(c)\).
\end{mdthm}

\begin{mdremark}
    We call \(F\) the \textbf{antiderivative} of \(f\).
\end{mdremark}

\begin{proof}
    Since \(f \in \mathcal{R}[a,b]\) then \(f\) is bounded by definition. Let \(M >0\) be such that \(\abs{f(t)} \leq M\) for all \(t \in [a,b]\). Suppose \(a\leq x \leq y \leq b\) then 
    \[\begin{aligned}
        \abs{F(y)-F(x)} &=\abs{\int_a^y f(t) \, dt - \int_a^x f(t)\, dt} \\
        &=\abs{-\int_y^a f(t) \, dt - \int_a^x f(t)\, dt} \\
        &=\abs{-1}\abs{\int_y^a f(t) \, dt + \int_a^x f(t)\, dt} \\
        &= \abs{\int_x^y f(t) \, dt} \\
        &\leq M(y-x).
    \end{aligned}\]
    Thus, \(F\) is Lipschitz continuous and therefore, continuous on \([a,b]\). Now, suppose \(f\) is continuous at \(c\in (a,b)\). Then 
    \[\begin{aligned}
        \abs{\frac{F(x)-F(c)}{x-c}-f(c)} &= \abs{\frac{\int_a^x f(t)\, dt - \int_a^c f(t)\, dt}{x-c}-f(c)} \\
        &\leq \abs{\frac{\int_c^x f(t)\, dt}{x-c}-f(c)} \\
        &=\abs{\frac{\int_c^x [f(t)-f(c)]\, dt}{x-c}} \\
        &\leq \abs{\frac{\int_c^x \abs{f(t)-f(c)} \, dt}{x-c}}.
    \end{aligned}\]
    Given \(\eps>0\) there exists a \(\delta>0\) such that \(\abs{f(t)-f(c)}<\eps\) for \(\abs{t-c} <\delta\). Hence, for \(0<\abs{x-c}<\delta\) we have 
    \[\begin{aligned}
        \abs{\frac{F(x)-F(c)}{x-c}-f(c)} &\leq \abs{\frac{\int_c^x \abs{f(t)-f(c)} \, dt}{x-c}} \\
        &\leq \abs{\frac{\int_c^x \eps \, dt}{x-c}}\\
        &< \eps.
    \end{aligned}\]
    In other words, \(F'(c)=f(c)\). Hence, \(f\) is differentiable and \(F'=f\).
\end{proof}

\begin{mdthm}[Fundamental Theoreom of Calulus part II]
    Let \(f\in C[a,b]\) with \(f\in C^1(a,b)\) and \(f'\in \mathcal{R}[a,b]\). Then for all \(x\in [a,b]\),
    \[f(x)=f(a)+\int_a^x f'(s) \, ds.\]
\end{mdthm}

\begin{mdnote}
    FTC II can be reformulated as
    \[\int_a^x f'(s) \, ds =f(x)-f(a).\]
\end{mdnote}

\begin{proof}
    Let 
    \[G(x) = f(a) + \int_a^x f'(s) \, ds -f(x).\]
    By FTC I it follows that \(G'(x)=0\) for all \(x \in (a,b)\) thus, \(G\) is constant on \((a,b)\). Furthermore, since \(f\) is continuous at \(x = a\) and \(x=b\) we see that \(G\) is also continuous at \(a\) and \(b\). Since, \(G(a)=0\), we find that \(G\) is identically zero on \([a,b]\) as required.
\end{proof}

\subsection{Methods of integration}

\begin{mdthm}[Integration by parts]
    Let \(f,g \in C[a,b]\) with \(f',g' \in C[a,b]\); then
    \[\int_a^b f(x)g'(x) \, dx =[f(x)g(x)]^b_a -\int_a^b f'(x)g(x)\, dx\]
\end{mdthm}

\begin{mdnote}
    The acronym \textbf{LIATE} can be used to choose which function to differentiate. (The I stands inverse trigonometric/hyperbolic functions).
\end{mdnote}

\begin{proof}
    The function \(h =f \cdot g =fg\) is differentiable and its derivative \(h'\) is continuous on \([a,b]\). The fundamental theorem of calculus applied to \(h'\) on the interval \([a,b]\) gives:
    \[\int_a^b h'(x) \, dx =[h(x)]_a^b=[f(x)g(x)]_a^b.\]
    By the product rule of differentiation applied we have:
    \[h'(x)=f(x)g'(x)+f'(x)g(x).\]
    Hence, 
    \[\int_a^b [f(x)g'(x)+f'(x)g(x)] \, dx = [f(x)g(x)]_a^b,\]
    from the linearity of the integral, it follows that
    \[\int_a^b f(x)g'(x) \, dx =[f(x)g(x)]^b_a -\int_a^b f'(x)g(x)\, dx.\]
\end{proof}

\begin{mdthm}[Change of variable]
    Let \(f\) be continuous and \(g\) be continuously differentiable. Then 
    \[\int_a^b f(x) \, dx = \int_{\alpha}^{\beta} f(g(t)) \, g'(t) \, dt,\]
    where \(g(\alpha)=a\) and \(g(\beta)=b\).
\end{mdthm}

\begin{proof}
    Let \(F(x)=\int_a^x f(t) \, dt\) and \(G(t)=F(g(t))\). By the chain rule we have \(G'(t)=F'(g(t))g'(t)\). But, by the fundamental theorem of calculus \(F'(x)=f(x)\). Thus,
    \[\begin{aligned}
        G'(t) &= F'(g(t))g'(t) \\
            &= f(g(t))g'(t).
    \end{aligned}\]
    Hence,
    \[\begin{aligned}
        \int_{\alpha}^{\beta} f(g(t))g'(t)\, dt &= \int_{\alpha}^{\beta} G'(t)\, dt \\
        &=[G(t)]_{\alpha}^{\beta} \\
        &=G(\beta) -G(\alpha) \\
        &=F(g(\beta))-F(g(\alpha)) \\
        &=F(b)-F(a) \\
        &=[F(x)]_a^b \\
        &=\int_a^b f(x) \, dx.
    \end{aligned}\]
\end{proof}

\section{Integration III}

\subsection{Unbounded intervals}

\begin{definition}
    The integral of a function defined and bounded on an interval that is not bounded is called an \textbf{improper integral of the first kind}.
\end{definition}

\begin{mdnote}
    In this case we have a `nice' function, but the interval is unbounded.
\end{mdnote}

\begin{definition}
    We define \textbf{improper integrals}. We suppose \(f \in \mathcal{R}[a,b]\).
    \begin{itemize}
        \item Let \(f :[a,\infty) \to \RR\) for all \(b>a\); we define
        \[\int_a^{\infty} f(x) \, dx =\limit{b}{\infty} \int_a^b f(x) \, dx,\]
        if the limit exists.
        \item Let \(f:(-\infty, b] \to \RR\) for all \(a<b\); we define
        \[\int_{-\infty}^{b} f(x) \, dx =\limit{a}{-\infty} \int_a^b f(x) \, dx,\]
        if the limit exists.
        \item Let \(f:\RR \to \RR\) for all \(a<b\); we define
        \[\int_{-\infty}^{\infty} f(x) \, dx = \limit{b}{\infty}\ \int_0^b f(x) \, dx+\limit{a}{-\infty} \int_a^0 f(x) \, dx.\]
        if both limits exists.
    \end{itemize}
\end{definition}

\begin{mdremark}
    The existence of the limits above is not guaranteed. If the limit exists, we say that the corresponding integral \textbf{converges}, is \textbf{well-defined} or \textbf{exists}. Otherwise, we say it \textbf{diverges}.
\end{mdremark}

\begin{mdnote}
    In the definition of \(\int_{-\infty}^{\infty} f(x) \, dx\) instead of integrating over \((-\infty,0)\) and \((0,\infty)\) we could have integrated a \textbf{reference point}, \(c \in \RR\), over \((-\infty, c)\) and \((c,\infty)\). The result of the integral is independent of the choice of the reference point.
\end{mdnote}

\begin{mdthm}
    Let \(\alpha \in \RR\) then
    \[\int_1^{\infty} \frac{1}{x^{\alpha}} \, dx\]
    converges if and only if \(\alpha >1\).
\end{mdthm}

\begin{proof}
    We can rewrite the integral as \(\int_1^{\infty} x^{-\alpha} \, dx\) which can be evaluated with standard integration methods as \(\alpha>1\).
    \[\begin{aligned}
        \int_1^{\infty} x^{-\alpha} \, dx &= \limit{b}{\infty} \left[ \frac{x^{1-\alpha}}{1-\alpha}\right]_1^b \\
        &= \limit{b}{\infty}\left(\frac{b^{1-\alpha}}{1-\alpha} - \frac{1}{1-\alpha}\right)\\
        &= -\frac{1}{1-\alpha}.
    \end{aligned}\]
    Thus, the integral converges.
\end{proof}

\begin{mdthm}
    Let \(\beta \in \RR\); then the integral
    \[\int_2^{\infty} \frac{1}{x(\log x)^{\beta}} \, dx\]
    converges if and only if \(\beta>1\).
\end{mdthm}

\begin{proof}
    Use a change of variable \(y=\log{x}\):
    \[\int_2^b \frac{1}{x(\log x)^{\beta}} \, dx =\int_{\log{2}}^{\log{b}} \frac{1}{y^{\beta}} \, dy,\]
    then by previous theorem the proof is complete.
\end{proof}

\subsection{Unbounded functions}

\begin{definition}
    The integral of a function over a bounded interval where the function is not bounded is called an \textbf{improper integral of the second kind}.
\end{definition}

\begin{mdnote}
    In this case the interval of the domain of the function is bounded but within this domain the range in bounded and has a singularity i.e. a point where the function is not defined.
\end{mdnote}

Consider an integral of the form \(\int_{a}^{b} f(x) \, dx\), where \(f(x)\) is continuous over \([a,b)\) and discontinuous at \(b\). Since, the function \(f(x)\) is continuous over \([a,t]\) for all values \(t\) satisfying \(a<t<b\), the integral \(\int_{a}^{t} f(x) \, dx\) is defined for such values of \(t\). Thus, we consider the values of \(\int_{a}^{t} f(x)\, dx\) as \(t\) approaches \(b\) for \(a<t<b\). The figure below illustrates \(\int_{a}^{t} f(x) \, dx\) as areas of regions for values of \(t\) approaching \(b\).

\begin{figure}[H]
     \begin{center}
            \includegraphics[width=\textwidth]{./Resources/Type II improper integral.png}
     \end{center}
\end{figure}

%%From Calculus 2 Book (Open University)

\begin{definition}
    Let \(f : [a,b]\to \RR\).
    \begin{enumerate}
        \item Let \(f\) be continuous over \([a,b)\). Then
        \[\int_{a}^{b} f(x) \, dx = \lim_{t\to b_-} \int_{a}^{t} f(x) \, dx.\]
        \item Let \(f\) be continuous over \((a,b]\). Then
        \[\int_{a}^{b} f(x) \, dx = \lim_{t\to a_+} \int_{t}^{b} f(x) \, dx.\]
        \item If \(f(x)\) is continuous over \([a,b]\) except at a point \(c \in (a,b)\) then 
        \[\int_{a}^{b} f(x) \, dx =\int_{a}^{c} f(x)  \, dx +\int_{c}^{b} f(x) \, dx,\]
        provided both \(\int_{a}^{c} f(x) f(x) \, dx\) and \(\int_{c}^{b} f(x) \, dx\) converge 
    \end{enumerate}
\end{definition}

\begin{mdremark}
    In the first two case, if the limits exist then the improper integral is said to converge. If not then it is said to diverge. In the third case, if either of the integrals diverge then \(\int_{a}^{b} f(x) \, dx\) diverges.
\end{mdremark}

% \begin{definition}
%     Let \(f : [a,b] \to \RR\).
%     \begin{enumerate}
%         \item If \(f\in\mathcal{R}[a+\eps,b]\) for all \(\eps>0\) we define
%         \[\int_a^b f(x) \, dx := \limit{\eps}{0^+} \int_{a+\eps}^b f(x) \, dx.\]
%         \item If \(f\in\mathcal{R}[a,b-\eps]\) for all \(\eps>0\) we define
%         \[\int_a^b f(x) \, dx := \limit{\eps}{0^+} \int_a^{b-\eps} f(x) \, dx.\]
%         \item If \(c \in (a,b)\) with \(f\in \mathcal{R}[a,c-\eps]\) and \(f\in \mathcal{R}[c+\eps,b]\) for all \(\eps>0\), we define
%         \[\int_a^b f(x) \, dx := \limit{\eps}{0^+} \int_a^{c-\eps} f(x) \, dx+\limit{\eps}{0^+} \int_{c+\eps}^b f(x)\, dx.\]
%     \end{enumerate}
% \end{definition}

% \begin{mdnote}
%     Suppose \(f :[a,b]\to \RR\), if \(f\) has a singularity at \(x_0 \in [a,b]\). Then we define,
%     \[\int_a^{x_0} f(t) \, dt = \limit{x}{x_0^-} \int_a^x f(t) \, dt,\]
%     i.e. the limit as \(x \to x_0\) from below.
%     Similarly,
%     \[\int_{x_0}^b f(t) \, dt = \limit{x}{x_0^+} \int_x^b f(t) \, dt,\]
%     is the limit as \(x \to x_0\) from above.
%     This is changed into the \(\eps\) limit by noticing that as \(x\to x_0^-\) we want \(\underbrace{(x_0-x)}_{\eps} \to 0^+\) as \(x \to x_0^-\).
% \end{mdnote}

\begin{example}
    Examples of each type.
    \begin{enumerate}
        \item Evaluate the integral \(\int_{0}^{4} \frac{1}{\sqrt{4-x}} \, dx\).
        \begin{solution}
            First we notice that this is an improper integral of the second kind as there is a discontinuity at \(x=4\) we sweep out the area under the curve from \(0\) to \(t\) as \(t\) approaches \(4\) from the left. Therefore, the integral becomes 
            \[\begin{aligned}
                \int_{0}^{4} \frac{1}{\sqrt{4-x}} \, dx &= \int_{0}^{t} \lim_{t \to 4^-} \frac{1}{\sqrt{4-x}} \, dx \\
                &= \lim_{t \to 4^-} \left[ -2\sqrt{4-x} \right]_0^t \\
                &=\lim_{t \to 4^-} (-2\sqrt{4-t}+4) \\
                &=4.
            \end{aligned}\]
            The figure below illustrates the integral.
            \begin{figure}[H]
                \begin{center}
                   \include{./Resources/Type 2 improper integral ex1.tex}
                \end{center}
           \end{figure}
        \end{solution}
        \item Evaluate the integral \(\int_{0}^{1} \frac{1}{x} \, dx\).
        \begin{solution}
            This is an improper integral as there is a discontinuity at \(x=0\). Therefore, the integral becomes 
            \[\begin{aligned}
                \int_{0}^{1} \frac{1}{x} \, dx &= \lim_{t \to 0^+} \int_{0}^{1} \frac{1}{x} \, dx \\
                &= \lim_{t \to 0^+} \left[ \ln\abs{x} \right]_t^1 \\
                &= \lim_{t \to 0^+} \left( \ln\abs{1} -\ln\abs{t} \right) \\
                &= -(-\infty) \\
                &= +\infty.
            \end{aligned}\]
            The integral diverges. The figure below illustrates the integral.
            \begin{figure}[H]
                 \begin{center}
                    \include{./Resources/Type 2 improper integral ex2.tex}
                 \end{center}
            \end{figure}
        \end{solution}
        \item Evaluate the integral \(\int_{-1}^{1} \frac{1}{x^3} \, dx\).
        \begin{solution}
        This is an improper integral as there is a discontinuity at \(x=0\). Therefore, the integral becomes
        \[\int_{-1}^{1} \frac{1}{x^3} \, dx = \int_{-1}^{0} \frac{1}{x^3} \, dx + \int_{0}^{1} \frac{1}{x^3} \, dx.\]
        If either of the two integral diverges, then the original integral diverges. Begin with \(\int_{-1}^{0} \frac{1}{x^3} \, dx\):
        \[\begin{aligned}
            \int_{-1}^{0} \frac{1}{x^3} \, dx &= \lim_{t\to 0^-} \int_{-1}^{t} \frac{1}{x^3} \, dx \\
            &= \lim_{t \to 0^-} \left[ \frac{1}{-2x^2} \right]_{-1}^{t} \\
            &= \lim_{t \to 0^-} \left( -\frac{1}{2t^2}+\frac{1}{2} \right) \\
            &= +\infty.
        \end{aligned}\]
        Hence, \(\int_{-1}^{0} \frac{1}{x^3} \, dx\) diverges. Since \(\int_{-1}^{0} \frac{1}{x^3} \, dx\) then, \(\int_{-1}^{1} \frac{1}{x^3} \, dx\) diverges.
        \end{solution}
    \end{enumerate}
\end{example}

\begin{example}
    Show that \(\int_0^1 \frac{1}{\sqrt{x}} \, dx\) converges to \(2\). \\
    \textbf{Solution:} The function \(f(x)= \frac{1}{\sqrt{x}}\) is defined \(x>0\) and is continuous on \([\eps,1]\) for \(0<\eps<1\). However, \(f\) is not bounded on \((0,1]\). Now,
    \[\begin{aligned}
        \int_0^1 \frac{1}{\sqrt{x}} \, dx &= \limit{\eps}{0^+} \int_{0+\eps}^{1} \frac{1}{\sqrt{x}} \, dx \\
        &= \limit{\eps}{0^+} [2\sqrt{x}]_{\eps}^1 \\
        &= \limit{\eps}{0^+} 2(1-\sqrt{\eps}) \\
        &= 2.
    \end{aligned}\]
\end{example}

\begin{mdthm}
    Let \(\alpha \in \RR\) then, 
    \[\int_{0}^{1} \frac{1}{x^{\alpha}} \, dx \quad \text{converges} \iff \alpha <1.\]
\end{mdthm}

\begin{mdthm}
    Let \(\beta \in \RR\) then,
    \[\int_0^{\half} \frac{1}{x\abs{\log x}^{\beta}} \, dx \quad \text{converges} \iff \beta>1.\]
\end{mdthm}

\subsection{Absolute and conditional convergence}

\subsubsection{Series}

\begin{definition}
    The series \(\sum_{n=1}^{\infty} a_n\) is \textbf{convergent} to \(A\), if the sequence of the \textbf{partial sum} satisfies
    \[A_N = \sum_{n=1}^{N} a_n\]
    as \(N \to \infty\).
\end{definition}

\begin{theorem}
    If \(\sum_{n=1}^{\infty} a_n\) converges then \(a_n \to 0\) as \(n \to \infty\).
\end{theorem}

\begin{definition}
    The series \(\sum_{n=1}^{\infty} a_n\) is said to be \textbf{absolutely convergent} if \(\sum_{n=1}^{\infty} \abs{a_n}\) converges.
\end{definition}

\begin{definition}
    If \(\sum_{n=1}^{\infty} a_n\) converges but not absolutely then it is called \textbf{conditionally convergent}.
\end{definition}

\begin{mdlemma}
    Let \(\abs{a_n} \leq b_n\) for all \(n \in \NN\) and assume that \(\sum_{n=1}^{\infty} b_n\) converges. Then \(\sum_{n=1}^{\infty} a_n\) converges to some \(A\), and we have the \textbf{remainder estimate} 
    \[\abs{A-\sum_{n=1}^{N} a_n} \leq \sum_{n=N+1}^{\infty} b_n,\]
    for \(N \in \NN\).
\end{mdlemma}

\subsubsection{Integrals}

\begin{definition}
    Let \(f :[a,\infty) \to \RR\) and let \(f \in \mathcal{R}[a,b]\) for any \(b>a\). We say the integral \(\int_a^{\infty} f(x) \, dx\) \textbf{converges absolutely}, if the integral \(\int_a^{\infty} \abs{f(x)} \, dx\) converges.
\end{definition}

\begin{definition}
    If \(\int_a^{\infty} f(x) \, dx\) converges but not absolutely, we say that it \textbf{converges conditionally}.
\end{definition}

\begin{mdthm}
    Let \(a\in \RR\) and let \(f,g\) be bounded functions on \([a,\infty]\). Assume that \(f,g \in \mathcal{R}[a,b]\) for any \(b>a\) and \(\abs{f(x)} \leq g(x)\) for all \(x \geq a\). If the integral \(\int_a^{\infty} g(x) \, dx\) converges then the integral \(\int_a^{\infty} f(x) \, dx\) converges as well.
\end{mdthm}

\begin{lemma}
    Let \(a\in \RR\) and let \(f\) be a function on \([a,\infty)\). Then the limit \(\limit{x}{\infty} f(x)\) exists if and only if for any sequence of points \(\{x_n\}_{n=1}^{\infty}\) such that \(x_n \to \infty\), the limit \(\limit{n}{\infty} f(x_n)\) exists.
\end{lemma}

\begin{mdthm}[Integral test]
    Let \(f(x) \geq 0\) be a bounded non-increasing function on \([0,\infty)\). Then the series \(\sum_{n=1}^{\infty} f(n)\) converges if and only if the integral \(\int_0^{\infty} f(x) \, dx\) converges. Furthermore, one has the estimate
    \[\int_1^{\infty} f(x) \, dx \leq \sum_{n=1}^{\infty} f(n) \leq \int_0^{\infty} f(x) \, dx.\]
\end{mdthm}

\begin{mdnote}
    From tutorial: The integral test theorem also applies for non-decreasing \\ functions however, in such case the inequalities are flipped.
\end{mdnote}

\begin{mdthm}
    Let \(f\) and \(g\) be continuous functions on \([a,\infty)\) such that
    \begin{itemize}
        \item[(i)] the integral \(\int_a^x f(t) \, dt\) is \textbf{bounded} and
        \item[(ii)] \(g(x)\) is \textbf{continuously differentiable}, \(\limit{x}{\infty} g(x) =0\) and is \textbf{monotone}.
    \end{itemize}
    Then the integral \(\int_a^{\infty} f(x)g(x) \, dx\) converges.
\end{mdthm}

% \begin{mdnote}
%     From tutorial: tutor call Theorem 8.3 from lectures "Oscillatory integral test". We call condition \((i)\) the oscillatory condition because for the integral of a function to be bounded we need "areas to cancel out" which only happens with oscillatory functions. We call condition \((ii)\) the "decay condition" as we need to it be monotone and go to \(0\) as \(x \to \infty\). \\
%     \textbf{Remark:} This is the analogue of the alternating series test.
% \end{mdnote}

% \begin{mdexample}
%     From tutorial: let \(f(t) = \sin(t)\) and \(g(t)=\frac{1}{t}\) then 
%     \[\int_a^{\infty} \frac{\sin(t)}{t} \, dt\] converges by the oscillatory integral test.
% \end{mdexample}

\section{Sequences and series of functions I}

\subsection{Pointwise convergence}

\begin{definition}
    Suppose \(\{f_n\}_{n=1}^{\infty}\) is a sequence of functions defined on a set \(\Delta\), the sequence of functions, \(\{f_n\}\), \textbf{converges pointwise} (on \(\Delta\)) to a (limit) function \(f : \Delta \to \RR\), if for all \(x \in \Delta\) the sequence of real numbers \(f_n(x)\) converges to \(f(x)\). That is for all \(x\in \Delta\) we have \(\lim_{n\to \infty} f_n(x) =f(x)\).
\end{definition}

\begin{mdremark}
    Using quantifiers the definition \textit{pointwise convergence} becomes
    \[\forall x \in \Delta \; \forall \eps>0  \; \exists N=N(x,\eps) \text{ such that } \forall n>N \then \abs{f_n(x)-f(x)}<\eps.\]
\end{mdremark}

\begin{example}
    Consider
    \[f_n(x)=\frac{x^2+nx}{n}\]
    on all of \(\RR\). The figure below illustrates graphs of \(f_1,f_5,f_{10}\) and \(f_{20}\), giving an indication of what happens as \(n \to \infty\). 
    \begin{figure}[H]
         \begin{center}
            \include{./Resources/Pointwise convergence.tex}
         \end{center}
    \end{figure}
    Algebraically, we can compute 
    \[\begin{aligned}
        \lim_{n\to \infty} f_n(x) &= \lim_{n \to \infty} \frac{x^2+nx}{n} \\
        &= \lim_{n \to \infty} \frac{x^2}{n} +x \\
        &=x.
    \end{aligned}\]
    Therefore, \(\{f_n\}\) converges pointwise to the limit function \(f(x)=x\) (on \(\RR\)).
\end{example}

\subsection{Uniform convergence}

\begin{definition}
    Let \(\{f_n\}_{n=1}^{\infty}\) be a sequence of functions defined on a set \(\Delta \subseteq \RR\). Then \(\{f_n\}\) \textbf{converges uniformly} (on \(\Delta\)) to a limit function \(f\) (defined on \(\Delta\)) if for every \(\eps>0\),there exists an \(N \in \NN\) such that \(\abs{f_n(x)-f(x)}<\eps\) whenever \(n \geq N\) and \(x \in \Delta\).
\end{definition}

\begin{mdnote}
    Uniform convergence implies there is an overall rate of convergence for each \(x \in \Delta\).
\end{mdnote}

\begin{mdremark}
    Using quantifiers the definition \textit{pointwise convergence} becomes
    \[\forall \eps>0, \exists N(\eps) \in \NN : n \geq N(\eps) \then \abs{f_n(x)-f(x)}<\eps, \forall x \in \Delta.\]
\end{mdremark}

\begin{theorem}
    If \(f_n\) converges uniformly then \(f_n\) also converges pointwise.
\end{theorem}

\subsubsection{How to prove uniform convergence}

\begin{mdexample}
    Let \(g_n(x) = \frac{1}{n(1+x^2)}\). Prove \(g_n(x)\) converges uniformly.
    \begin{solution}
        For any fixed \(x \in \RR\) we can see that \(\lim_{n \to \infty} g_n(x)=0\) thus, \(g(x)=0\) is the pointwise limit function of the sequence \(\{g_n\}\) on \(\RR\). Observe that \(\frac{1}{1+x^2} \leq 1\) for all \(x\in \RR\), which implies that 
        \[\abs{g_n(x)-g(x)}=\abs{\frac{1}{n(1+x^2)}-0} \leq \frac{1}{n}.\]
        Thus, given \(\eps>0\) we can choose \(N > \frac{1}{\eps}\), and it follows that 
        \[n \geq N \then \abs{g_n(x)-g(x)}<\eps\]
        for all \(x \in \RR\). Hence, \(g_n \to 0\) uniformly on \(\RR\).
    \end{solution}
\end{mdexample}

\subsection{Uniform convergence of series}

\begin{definition}
    For each \(n\in \NN\), let \(f_n\) and \(f\) be function defined on a set \(\Delta \subseteq \RR\). The infinite series 
    \[\sum_{n=1}^{\infty} f_n(x)=f_1(x)+f_2(x)+f_3(x)+\ldots\]
    \textbf{converges pointwise} (on \(\Delta\)) to \(f(x)\) if the sequence \(F_N(x)\) of partial sums defined by 
    \[F_N(x)=f_1(x)+f_2(x)+ \ldots +f_N(x)\]
    converges pointwise to \(F(x)\).
\end{definition}

\begin{definition}
    The series as described above \textbf{converges uniformly} (on \(\Delta\)) to \(f\) if the sequence \(F_N(x)\) converges uniformly (on \(\Delta\)) to \(F(x)\).
\end{definition}

\begin{mdthm}[Weierstrass M-test]
    Let \(\{f_n\}_{n=1}^{\infty}\) be a sequence of function on an interval \(\Delta\), and let \(M_n>0\) be a real number satisfying 
    \[\abs{f_n(x)}\leq M_n\]
    for all \(x \in \Delta\). If \(\sum_{n=1}^{\infty} M_n\) converges then \(\sum_{n=1}^{\infty} f_n\) converges absolutely and uniformly on \(\Delta\).
\end{mdthm}

\subsection{Power series}

\begin{definition}
    Functions of the form 
    \[f(x)=\sum_{n=0}^{\infty} a_n x^n = a_0 +a_1 x +a_2 x^2 +a_3 x^3 +\ldots\]
    are called \textbf{power series}.
\end{definition}

\begin{mdthm}
    If a power series \(\sum_{n=0}^{\infty} a_n x^n\) converges at some point \(x_0 \in \RR\), then it converges absolutely for any \(x \in (-\abs{x_0},\abs{x_0})\). For any \(\eps>0\), the convergence is uniform on the sub-interval \([-\abs{x_0}+\eps,\abs{x_0}-\eps]\).
\end{mdthm}

\begin{mdthm}
    The \textbf{radius of convergence}, \(R\), of a power series is given by the formula 
    \[R =\frac{1}{\alpha} \quad \text{where} \quad \alpha = \limsup_{n\to \infty} \abs{a_n}^{\frac{1}{n}},.\]
\end{mdthm}

\begin{mdremark}
    Notice that \(R=\infty\) if \(\alpha=0\) and \(R=0\) if \(\alpha=\infty\).
\end{mdremark}

\subsection{Fourier series}

\begin{definition}
    Let \(\{a_n\}_{n\in \ZZ}\) be a sequence of complex (or real) number. A \textbf{Fourier series} is a series of the form 
    \[f(x)=\sum_{n=-\infty}^{\infty} a_n  e^{inx},\]
    where the index \(n\) takes all the integer values from \(-\infty\) to \(\infty\).
\end{definition}

\begin{mdthm}
    By Weierstrass M-test, the Fourier series converges absolutely and uniformly on \(\RR\).
\end{mdthm}

\begin{theorem}
    The sum of Fourier series is a \(2\pi\) periodic function.
\end{theorem}

\subsection{Dirichlet series}

\begin{definition}
    Let \(\{a_n\}_{n=1}^{\infty}\) be a sequence of complex (or real) numbers. The \textbf{Dirichlet series} is
    \[\sum_{n=1}^{\infty} \frac{a_n}{n^s}.\]
\end{definition}

\begin{mdremark}
    In this course we only consider \(s \in \RR\).
\end{mdremark}

\begin{mdthm}
    If the Dirichlet series converges for some \(s=s_0 \in \RR\), then it converges on \((s_0,\infty)\); for any \(\eps>0\), the convergence is uniform on \((s_0+\eps,\infty)\).
\end{mdthm}

\begin{theorem}
    The domain of convergence of a Dirichlet series must be one of the following forms:
    \begin{itemize}
        \item the whole of \(\RR\);
        \item the empty set;
        \item the half-line \((\sigma,\infty)\) or \([\sigma,\infty)\).
    \end{itemize}
    The number \(\sigma\) is called the \textbf{abscissa of convergence}.
\end{theorem}

\begin{mdthm}
    Suppose \(s\) is regarded as a complex variable. If the Dirichlet series converges for some \(s_0\) then it converges for all \(s\) with \(\text{Re}(s) > \text{Re}(s_0)\).
\end{mdthm}

% \subsection{FROM TUTORIAL}

% \begin{definition}
%     Let \((f_n)_{n\in \NN}\) be a sequence of functions \(f_n : \Delta \to \RR\). Then we say \(f_n \to f\) converges \textbf{pointwise} if 
%     \[\forall x \in \Delta \; \forall \eps>0  \; \exists N=N(x,\eps) \text{ such that } \forall n>N \then \abs{f_n(x)-f(x)}<\eps.\]
% \end{definition}

% \begin{mdnote}
%     For each fixed \(x \in \Delta\) we have \(f_n(x) \to f(x)\) but at different rates. For example, \(f_n(x_1) \to f(x_1)\) very slowly whereas \(f_n(x_2) \to f(x_2)\) very fast.
% \end{mdnote}

% \begin{definition}
%     \textbf{Uniform convergence} 
%     \[\forall \eps>0 \; \exists N=N(\eps) \text{ such that } \forall n>N \then \sup_{x\in \Delta} \abs{f_n(x)-f(x)} < \eps.\]
% \end{definition}

% \begin{mdnote}
%     Uniform convergence implies there is an overall rate of convergence for each \(x \in \Delta\).
% \end{mdnote}

% \begin{mdnote}
%     Why is this important?
%     \begin{itemize}
%         \item If \((f_n)_{n\in \NN}\) is such that \(f_n\) is continuous and \(f_n\) converges uniformly then \(f\) is continuous. BUT if \(f_n\) only converges pointwise then \(f\) is not necessarily continuous.
%         \item Uniform converges implies pointwise converges.
%     \end{itemize}
% \end{mdnote}

% \begin{definition}
%     \(\sum_{n=1}^{\infty} f_n\) converges pointwise / uniformly if \(F_N \to F\) converges pointwise / uniformly where \(F_N(x) = \sum_{n=1}^{N} f_n(x)\).
% \end{definition}

% \begin{mdthm}[Weierstrass M-test]
%     If \(\sup_{x\in \Delta} \abs{f_n(x)} \leq M_n\) and \(\sum_{n=1}^{\infty} < \infty\) then \(\sum_{n=1}^{\infty} f_n\) converges uniformly.
% \end{mdthm}

\section{Sequences and series of functions II}

\subsection{Uniform converges and continuity}

\begin{mdthm}
    If a sequence of continuous functions, \(f_n\), converges uniformly on \(\Delta\) to a function \(f\), the \(f\) is also continuous.
\end{mdthm}

\begin{corollary}
    If \(\sum_{n=1}^{\infty} f_n(x)\) is a series of continuous functions on \(\Delta\) which converges uniformly on \(\Delta\), then the sum of the series is a continuous function.
\end{corollary}

\subsection{Uniform converges and integration}

\begin{mdthm}
    Suppose \(f_n \in \mathcal{R}[a,b]\) for \(n \in \NN\) and suppose that \(f_n \to f\) uniformly on \([a,b]\). Then \(f \in \mathcal{R}[a,b]\) and 
    \[\begin{aligned}
        \lim_{n\to \infty} \int_{a}^{b} f_n(x) \, dx &= \int_{a}^{b} \lim_{n\to \infty}f_n(x) \, dx \\
        &= \int_{a}^{b} f(x) \, dx.
    \end{aligned}\]
\end{mdthm}

\begin{mdcor}
    If \(f_n \in \mathcal{R}[a,b]\) and if the series 
    \[f(x)=\sum_{n=1}^{\infty} f_n(x),\]
    converges uniformly on \([a,b]\), then \(f \in \mathcal{R}[a,b]\) and 
    \[\int_{a}^{b} f(x) \, dx =\sum_{n=1}^{\infty} \int_{a}^{b} f_n(x) \, dx.\]
\end{mdcor}

\begin{mdnote}
    In other words, uniformly convergent series may be integrated term by term.
\end{mdnote}

\subsection{Uniform converges and differentiation}

\begin{mdthm}
    For \(n\in \NN\) let \(f_n \in C[a,b]\) be a sequence of differentiable functions such that \(f'_n \in C[a,b]\) and both sequences \(f_n\) and \(f'_n\) converge uniformly on \([a,b]\). Then the limit of \(f_n\) is differentiable and 
    \[\diff{}{x} \lim_{n\to \infty} f_n(x) =\lim_{n\to \infty}\diff{}{x}f_n(x).\]
\end{mdthm}

\begin{corollary}
    For \(n\in \NN\) let \(f_n \in C[a,b]\) be a sequence of differentiable functions with \(f'_n \in C[a,b]\) such that both series 
    \[\sum_{n=1}^{\infty} f_n \quad \text{and} \quad \sum_{n=1}^{\infty} f'_n\]
    converge uniformly on \([a,b]\). Then the sum of the first series is differentiable and 
    \[\diff{}{x} \sum_{n=1}^{\infty} f_n(x)= \sum_{n=1}^{\infty} \diff{}{x} f_n(x)\]
    for \(x \in (a,b)\).
\end{corollary}

\subsection{Application to power series}

\begin{mdthm}
    Let \(R>0\) be the radius of convergence of a power series 
    \[f(x) = \sum_{n=0}^{\infty} a_n x^n.\]
    Then \(f\) is differentiable on \((-R,R)\) and its derivative is given by the power series 
    \[f'(x)=\sum_{n=1}^{\infty} n a_n x^{n-1},\]
    which converges in \((-R,R)\).
\end{mdthm}

\begin{corollary}
    Let
    \[f(x)=\sum_{n=0}^{\infty} a_n x^n\]
    be a power series with the radius of convergence \(R>0\). Then \(f\) has derivatives of all orders in \((-R,R)\), which are given by 
    \[f^{(k)} (x) = \sum_{n=k}^{\infty} n(n-1) \cdots (n-k+1) a_n x^{n-k}.\]
    In particular,
    \[f^{(k)}(0)=k! a_k\]
    for \(k = 0,1,\ldots\)
\end{corollary}

\subsection{Continuous nowhere differentiable functions}

\begin{mdthm}
    There exists a continuous function on \(\RR\) which is nowhere differentiable.
\end{mdthm}

\begin{mdexample}
    Consider the series 
    \[\sum_{n=1}^{\infty} a^n \sin(b^n x),\]
    where \(0<a<1,b>1\) and \(ab\geq 1\).
\end{mdexample}

% \subsection{FROM TUTORIAL}

% \begin{itemize}
%     \item If \(f_n \to f\) uniformly and 
%         \begin{itemize}
%             \item \(f_n\) is continuous on \(\Delta \then\)  \(f\) is continuous on \(\Delta\)
%             \item \(f_n \in R[a,b] \then f \in R[a,b]\).
%             \item \(f'_n \to g\) uniformly \(\then\) if \(f_n \in C^1[a,b]\) we have that \(f\) is differentiable on \([a,b]\) and \(f'=g\).
%         \end{itemize}
% \end{itemize}

\pagebreak

\appendix

\addcontentsline{toc}{section}{Appendix}
\section*{Appendix}

\section{Increasing functions}

Here is a list of classes of functions that are commonly encountered; the slower growing functions are listed first and \(c\) is some arbitrary constant.

\begin{table}[H]
    \begin{center}
        \begin{tabular}{c|c}
            \textbf{Notation} & \textbf{Name}  \\ \hline
            \(O(1)\) & Constant \\
            \(O[\log n]\) & Logarithmic \\
            \(O[(\log n)^c]\) & Poly-logarithmic \\
            \(O(n)\)& Linear \\
            \(O(n^2)\) & Quadratic \\
            \(O(n^c)\) & Polynomial \\
            \(O(c^n)\) & Exponential \\
            \(O(n!)\) & Factorial       
            \end{tabular}    
    \end{center}
\end{table}

\section{Triangle Inequality}

\begin{theorem}
    For \(x,y \in \RR\) we have 
    \[\abs{x+y} \leq \abs{x}+\abs{y}.\]
\end{theorem}

\section{The Binomial Theorem}

\begin{theorem}[Binomial expansion]
    Let \(x,y \in \RR\) and \(n\) a positive integer, then
    \[\begin{aligned}
        (x+y)^n &= \sum_{k=0}^{n} \begin{pmatrix} n \\k \end{pmatrix} x^{n-k} y^k \\
                &= \begin{pmatrix} n \\ 0 \end{pmatrix} x^n y^0 +\begin{pmatrix} n \\1 \end{pmatrix} x^{n-1} y^1 +\cdots + \begin{pmatrix} n \\ n-1 \end{pmatrix} x^1 y^{n-1} +\begin{pmatrix} n \\n \end{pmatrix} x^0 y^n.
    \end{aligned}\]
    Where the coefficient of each term is given by
    \[\begin{pmatrix} n \\k \end{pmatrix} = \frac{n!}{k!(n-k)!}.\]
\end{theorem}

\section{Common Taylor expansions}

This is a list of common Taylor expansions.

\[\begin{aligned}
    e^x &= 1+x+\frac{x^2}{2!}+\cdots +\frac{x^n}{n!} +O(x^{n+1})  \quad &\text{for } x \in \RR, \\
    \sin(x) &= x-\frac{x^3}{3!} + \cdots + (-1)^{n-1} \frac{x^{2n-1}}{(2n-1)!} + \cdots \quad &\text{for } x \in \RR, \\
    \cos(x) &= 1 -\frac{x^2}{2} + \cdots + (-1)^n \frac{x^{2n}}{(2n)!} +\cdots \quad &\text{for } x \in \RR,\\
    \ln(1+x) &= x - \frac{x^2}{2} +\cdots + (-1)^{n-1} \frac{x^n}{n} +\cdots \quad &\text{for } \abs{x}<1, \\
    (1+x)^{\alpha} &= 1+\alpha x +\frac{\alpha(\alpha-1)}{2!} x^2 +\cdots \frac{\alpha(\alpha-1)\cdots +(\alpha-n+1)}{n!} x^n+\cdots \quad &\text{for } \abs{x}<1.
\end{aligned}\]

\section{Rates of convergence}

\begin{figure}[H]
     \begin{center}
         \includegraphics[width=\textwidth]{./Resources/Rates of convergence.png}
     \end{center}
\end{figure}

\section{Links}

\begin{enumerate}
    \item Pointwise and uniform convergence: \url{https://youtu.be/GsORKmBCLuI}.
\end{enumerate}



\end{document}
