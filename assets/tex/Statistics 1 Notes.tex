\documentclass[12pt, a4paper]{article}   	
\usepackage{geometry}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage[utf8]{inputenc}
\usepackage{color}   
\usepackage{tikz}
\usepackage{tcolorbox}
\usepackage{multicol}
\usepackage[thinc]{esdiff}
\usepackage{physics}
\usepackage{bm}
\usepackage{pdfpages}
\usepackage{pdflscape}
\usepackage{listings}
\usepackage{float}

\usepackage{hyperref}

\hypersetup{colorlinks=true, linktoc=all, linkcolor=black,}

\newcommand{\bb}[1]{\mathbb{#1}}
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\imply}{\Rightarrow}
\newcommand{\Cal}[1]{\mathcal{#1}}
\newcommand{\mb}[1]{\mathbf{#1}}

\newtheorem*{remark}{Remark}
\newtheorem*{note}{Note}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{example}{Example}
\newtheorem{proposition}{Proposition}

\theoremstyle{plain}
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}

\title{Probability and Statistics 1 Notes}
\date{}
\author{Francesco Chotuck}
\begin{document} 
\maketitle 

\tableofcontents

\pagebreak

\section{Elementary probability theory}

\subsection{Axiomatic approach to probability}

\begin{definition}
An \textbf{experiment} is the process by which observations are made.
\end{definition}

\begin{definition}
The outcome of an individual observation is called an \textbf{elementary event}, we associate points called \textbf{sample points} to each elementary event.
\end{definition}

\begin{definition}
The \textbf{sample space} $S$ is the set of all elementary observation. Note, this could be finite or countably infinite.
\end{definition}

\begin{definition}
An \textbf{event} is any subset of $S.$
\end{definition}

\begin{definition}
The \textbf{intersection} of the set $A$ and $B$ is given by $$A\cup B = \{E_i \in S : A \wedge E_i \in B\}.$$
\end{definition}

\begin{definition}
The \textbf{union} of the set $A$ and $B$ is given by $$A \cap B = \{E_i \in S : E_i \in A \lor E_i \in B\}.$$
\end{definition}

\begin{definition}
The \textbf{complement} of the set $A$ is given by $$\bar{A} = \{E_i \in S : E_i \notin A \}.$$
\end{definition}

\begin{remark}
Unions and intersection of events are still events i.e. subsets of $S$.
\end{remark}


\subsection*{Axioms of probability}

Given a sample space $S,$ a probability $P(E)$ is assigned to every event $E \subset S$ such that:

\begin{enumerate}
	
	\item $P(E) \geq 0;$

	\item $P(S) = 1;$

	\item If $A_1,A_2,\ldots,A_k$ is a collection of mutually exclusive events (i.e. if $A_i\cap A_j = \emptyset$ for $j \neq i$), then $P(A_1\cup A_2\cup\ldots\cup A_k) = \sum_{i=1}^{k} P(A_i).$ We also have the same result for an infinite sequence of events.

\end{enumerate}

\subsection{The Basic Principle of Counting}

\begin{theorem}
With $m$ elements $a_1,a_2,\ldots,a_m$ and $n$ elements $b_1,b_2,\ldots,b_n$ one can exactly form $m \times n$ (ordered) pairs $(a_k,b_l).$
\end{theorem}

\begin{proof}
Arrange the elements in an $m \times n$ array and count the number of elements in the array.
\end{proof}

\begin{theorem}
With $n_1$ elements $a_1,a_2,\ldots,a_{n_1}; n_2$ elements $b_1,b_2,\ldots,b_{n_2}$ and $n_k$ elements $l_1,l_2,\ldots,l_{n_k},$ one can exactly form $n_1 \times n_2 \times \ldots \times n_k$ \textbf{multiplets} of $k$ elements. 
\end{theorem}

\begin{definition}
Given $n$ different objects, the complete set of possible arrangements of these objects is called the set of \textbf{permutations.}
\end{definition}

\begin{theorem}
The \textbf{number of permutations} of $n$ objects (all different and distinguishable) is given by $$n! = n(n-1)(n-2)\ldots1.$$
\end{theorem}

\begin{theorem}
The number of ordered arrangements of $r$ objects taken (without replacement) from a set of $n$ distinguishable objects is $P_r^n = \frac{n!}{(n-r)!}=n(n-1)\ldots(n-r+1).$
\end{theorem}

\begin{definition}
A \textbf{combination} is a subset of $n$ objects taken from some larger set of $n$ distinguishable objects (where the order is irrelevant).
\end{definition}

\begin{theorem}
The number of ways of selecting combinations of $r$ objects from a set of $n \geq r$ distinguishable objects is $$C_r^n = \begin{pmatrix} n \\ r\end{pmatrix} = \frac{n!}{(n-r)!r!}.$$
\end{theorem}

\section{Some more combinatorics}

\begin{definition}
A \textbf{partition} is an arrangements of $n$ elements into $k$ groups containing $n_1,n_2,\ldots,n_k$ objects respectively, with $0\leq n_j\leq n$ and $n \sum_{j=1}^{k} n_j.$ (Every element is assigned exactly to one group).
\end{definition}

\begin{remark}
The definition of partition is in the context of probability.
\end{remark}

\begin{theorem}
(Number of partitions). The number of partitions into $k$ distinct groups, containing $n_1,n_2,\ldots,n_k$ elements is $$P_{n_{1}n_{2}\ldots n_{k}}^{n} = \begin{pmatrix} n \\ n_1,n_2,\ldots,n_k \end{pmatrix} = \frac{n!}{n_{1}!n_{2}!\ldots n_{k}!}.$$
\end{theorem}

\section{Probability rules}

\begin{definition}
We say, $A_1,\ldots,A_k$ are \textbf{mutually exclusive} events if $A_i \cap A_j = \emptyset, i \neq j.$
\end{definition}

\textbf{Distributive Law:} $$\begin{aligned}A\cap (B \cup C) &= (A \cap B)\cup (A \cap C)  \text{ and } \\ A\cup (B \cap C) &= (A \cup B)\cap (A \cup C).\end{aligned}$$

\textbf{De Morgan's Law:} $$\begin{aligned} \overline{(A\cup B)} &= \overline{A}\cap \overline{B} \text{ and } \\ \overline{(A \cap B)} &= \overline{A}\cup \overline{B}.\end{aligned}$$

\begin{theorem}
(Law of addition of probabilities) $$P(A \cap B)=P(A)+P(A)-P(A \cup B).$$
\end{theorem}

\section{Conditional probability}

\begin{definition}
Probability of an event $A$ given an event $B$ has occurred is defined as $$P(A|B)=\frac{P(A\cap B)}{P(B)}.$$ In the above definition, we should specify that $P(B) > 0.$
\end{definition}

\begin{definition}
The event $A$ is \textbf{independent} of the event $B$ if $P(A|B) = P(A).$
\end{definition}

\begin{theorem} 
The following statement are equivalent:
\begin{enumerate}
	
	\item $P (A|B) = P (A),$ if $A$ is independent of $B;$

	\item $P(B|A) = P (B),$ if $B$ is independent of $A;$

	\item $P(A \cap B) = P(A)P(B).$

\end{enumerate}
\end{theorem}

\begin{definition}
\textbf{The Multiplicative Law of Probability:} \\
$P (A\cap B) = P (B|A)P(A) = P(A|B)P(B),$ and if $A$ and $B$ are independent, $P(A \cap B) = P (A)P(B).$
\end{definition}

\begin{definition}
A family of sets $A_1,A_2 ,\ldots,A_n$ consisting of mutually exclusive and exhaustive events is called a \textbf{partition of the sample space} $S.$ (Recall that this means that $A_i \cap A_j = \phi$ for $i \neq j,$ and that $S = A_1 \cup A_2\ldots \cup A_n.$
\end{definition}

\textbf{Note:} Exhaustive events cover the probability of the whole sample space.

\begin{theorem}
\textbf{Properties of Independence}
\begin{enumerate}
	
	\item If $A$ is independent of $B,$ then $\overline{A}$ is independent of $B.$

	\item If $A$ is independent of $B,$ then $A$ is independent of $\overline{B}.$

\end{enumerate}
\end{theorem}

\begin{theorem}
(Normalization of conditional probabilities.) If $A_1, A_2,\ldots,A_n$ is a partition of $S,$ then $\sum_{i=1}^{n} P(A_i|B) = 1.$
\end{theorem}

\section{Bayes' theorem}

\begin{theorem}
Given $2$ events $A$ and $B,$ then $$P(A|B)=\frac{P(B|A)P(A)}{P(B)}.$$
\end{theorem}

\begin{theorem}
(Bayes' theorem, general version.) Given an event $A$ and the family $B_1, B_2,\ldots, B_n$ that is a partition of $S,$ $$\frac{P(A|B_j)P(B_j)}{\sum_{i=1}^{n} P(A|B_i)P(B_i)}.$$
\end{theorem}

\section{Discrete random variables and their probability distributions}

\begin{definition}
A \textbf{random variable} (sometimes r.v.) is a function defined on the sample space $S$ mapping each element of $S$ to a real number.
\end{definition}

\begin{definition}
A \textbf{discrete random variable} is a random variable that can only take finitely or countably infinitely many distinct values.
Note that, for any discrete random variable, we have the following two properties 
\begin{enumerate}
	
	\item $0 \leq p(x) \leq 1;$

	\item $\sum_{x}p(x)=1.$

\end{enumerate}
\end{definition}

\subsection{Expectation value of a random variable}

\begin{definition}
Given a random variable $X$ with probability distribution/probability mass function (p.m.f.) $p(x)$ its \textbf{expectation}, or expected value is defined as: $$\sum_{x}xp(x). $$ The above sum is over all values $x$ that $X$ can take.
\end{definition}

The expectation of any function $X$ is given by $$E(f(x))=\sum_{x}f(x)p(x).$$ Similarly, the expectation of $X^n$ is $$E(X^n)=\sum_{x}x^np(x).$$

\begin{definition}
The \textbf{variance} of $X$ is defined as $$V(X) \equiv Var(X) = E((X-E(X))^2) = E(X^2)-E(X)^2.$$
\end{definition}

\begin{theorem}
(\textbf{Properties of Expectations}). For any constants $a$ and $b,$ and any function $f$ and $g,$ we have:
\begin{enumerate}
	
	\item $E(a)=a;$

	\item $E(g(x))=\sum_{x} g(x)p(x);$

	\item $E(af(X))=aE(f(X));$

	\item (Linearity) $E(af(X)+bg(X)) = aE(f(x)) +bE(f(X))$.

\end{enumerate}
\end{theorem}

\begin{theorem}
(\textbf{Properties of Variance}). For any constants $a$ we have: 

\begin{enumerate}

	\item $Var(a)=0;$

	\item $Var(X+a)=Var(X);$

	\item $Var(aX)=a^2Var(X);$
	
	\item $Var(aX\pm b)=a^2Var(X).$

\end{enumerate}
\end{theorem}

\subsection{Standard discrete distributions}
	
\subsubsection{Binomial distribution}
A random variable $X$ follows a binomial distribution if the following $5$ conditions are satisfied: \begin{enumerate}
	
	\item There is a fixed number of trials $(n).$

	\item Each trial results in either \textit{`success'} or \textit{`failure'}.

	\item All the trials are independent.

	\item The probability of `success' $(p)$ is the same in each trial.

	\item The variable, $X,$ is the total number of successes in the $n$ trials. 

\end{enumerate} Then we can say that  the probability mass function is $X \sim Bin(n,p),$ where $n$ is the number of trials and $p$ is the probability of the event being a \textit{'success'}. The probability mass function is given by, $$P(X=x) = \begin{pmatrix} n\\x \end{pmatrix}p^x(1-p)^{n-x}, \quad x=0,1,\ldots,n.$$ The expectation and variance are: $$\begin{aligned}E(X)&=np, \\ Var(X)&=np(1-p).\end{aligned}$$

\subsubsection{Bernoulli distribution}

A special case of the binomial distribution where $n=1.$ A discrete random variable which takes on only 2 values, typically $S = \{0, 1\}.$ 
$$p(x)=\begin{aligned}
\begin{cases} 
p,  \quad y=1;\\ 
1-p, \quad y=0. 
\end{cases}
\end{aligned}$$ The expectations and variance are: $$\begin{aligned}
E(X)&=x\\
Var(X)&=p(1-p).
\end{aligned}$$

\subsubsection{Geometric distribution}

A random variable $X$ follows a geometric distribution if the following $3$ conditions are satisfied: \begin{enumerate}
	
	\item There is a sequence of independent trials with only two possible outcomes -- \textit{`success'} and \textit{`failure'}.

	\item The probability of \textit{`success'},$p,$ is constant.

	\item $X$ is the number of trials until the first success occurs (including the \textit{`successful'} trial itself).

\end{enumerate} Then the p.m.f. is $X \sim Geo(p),$ for $X=1,2,3\ldots$ with probability: $$P(X=x)=p(1-p)^{x-1}.$$ The expectation and variance are: $$\begin{aligned}E(X)&=\frac{1}{p} \\ Var(X)&=\frac{(1-p)}{p^2}.\end{aligned}$$

\subsubsection{Hypergeometric distribution}

The hypergeometric distribution concerns the case of sampling from a set containing a mixture of
$2$ different elements (red, black) without replacement. Suppose we have a set of size $N$ with $r$ red elements and $N - r$ black elements. Then the number of different ways of finding $x$ red elements in a draw of $n$ is $\begin{pmatrix} r\\x \end{pmatrix}\begin{pmatrix} N-r\\n-x \end{pmatrix}.$ Hence the probability of finding x red elements is,for $x=0, 1,\ldots, n,$ $$p(x)=\frac{\begin{pmatrix} r\\x \end{pmatrix}\begin{pmatrix} N-r\\n-x \end{pmatrix}}{\begin{pmatrix} N\\n \end{pmatrix}}.$$ The expectation and variance are $$\begin{aligned}
E(X)&=\frac{nr}{N} \\
Var(X)&=\frac{nr(N-r)(N-n)}{N^2(N-1)}.
\end{aligned}$$

\subsubsection{Poisson distribution}

If $X$ represents the number of events that occur in a particular space or time, then $X$ will follow a Poisson distribution as long as:\begin{enumerate}
	
	\item The events occur randomly, and are independent of each other.

	\item The events happen singly i.e. one at a time.

	\item The events happen (on average) at a constant rate (either in space or time).
\end{enumerate} The Poisson parameter $\lambda$ is then the average rate at which these events occur (i.e. the average number of events in a given interval of space or time). Then the p.m.f. is $X \sim Po(\lambda),$ for $X=0,1,2,\ldots$ with probability: $$P(X=x)=\frac{e^{-\lambda}\lambda^x}{x!}.$$ The expectation and variance are: $$\begin{aligned}E(X)&=\lambda \\ Var(X)&=\lambda.\end{aligned}$$

\section{Moment generating functions}

\begin{definition}
The $n^{\text{th}}$ moment of a random variable $X$ is given by $$\mu_n=\sum_{x} x^np(x).$$ In particular, $\mu_0=1,$ and $\mu_1=E(X).$ Recall that the variance of $X$ is given by $$V(X)\equiv Var(X)=E(X^2)-(E(X))^2=\mu_2-\mu_1^2.$$
\end{definition}

\begin{definition}
The \textbf{moment generating function} of $X$ is defined as $$M_X(t)=E(e^{tX})=\sum_{x} e^{tx}p(x).$$ 
\end{definition}

\begin{theorem}
Moments of a distribution can be computed via derivatives of the moment generating functions. $$\mu_k=E(X^k)=\diff[k]{M_X(t)}{t}\Big|_{t=0}.$$
\end{theorem}

\section{Continuous random variable (C.R.V.)}

\begin{definition}
Let $X$ be a random variable whose set of possible values $S$ is a continuum of numbers such as an interval, i.e. the set of possible values is uncountable. We say that $X$ is a \textbf{continuous random variable} if there exists a non-negative function $f$ , such that the following property holds for any set $B$ of real numbers: $$P(X\in B)=\int_B f(x)\,dx.$$
\end{definition}

\begin{definition}
The function $f \geq 0,$ piecewise continuous (i.e. continuous perhaps except for finitely many values of $x$) in the definition above is called the probability density function (p.d.f.) of the random variable $X.$
\end{definition}

\begin{proposition}
The piecewise continuous function $f ,$ defined for every $x \in \bb{R}$ is a probability density function (p.d.f.) if \begin{itemize}

	\item $f(x)\geq 0;$
	\item $\int_{-\infty}^{\infty}f(x)\,dx=1;$
	\item $P(X=a)=\int_{a}^{a}f(x)\,dx=0.$

\end{itemize}
\end{proposition}

\begin{definition}
Let $X$ be a c.r.v. with p.d.f. $f.$ Then, the \textbf{cumulative distribution function} (c.d.f.) is given by $$F(x)=P(X\leq x)=\int_{-\infty}^{x} f(t)\,dt.$$Consequently, $F$ is continuous and the relationship between the p.d.f. and the cumulative distribution function $F$ is expressed by $$f(x)=\diff{}{x}F(x).$$

\end{definition}

\begin{theorem}
(\textbf{Properties of the C.D.F.}) \begin{enumerate}
	
	\item $F(-\infty)=\lim_{x \to -\infty}F(x)=0;$
	\item $F(+\infty)=\lim_{x \to +\infty}F(x)=1;$
	\item $F(x)$ is a non decreasing function of $x,$ i.e. if $x_1<x_2,$ then we have $F(x_1)\leq F(x_2).$

\end{enumerate}
\end{theorem}

\begin{remark}
When drawing the c.d.f. you must draw the lines for which the function takes $0$ and $1.$
\end{remark}

\subsection{Expectation values for a continuous random variable}

\begin{definition}
The expectation value of a continuous random variable with p.d.f. $f(x)$ is $$E(X)=\int_{-\infty}^{\infty}xf(x)\,dx.$$
\end{definition}

\begin{theorem}
(Properties of Expectations) For any constant $a$ and $b,$ and any function $f$ and $g,$ \begin{enumerate}
	
	\item $E(a)=a;$
	\item $E(af(X))=aE(f(X));$
	\item $E(af(X)+bg(X))=aE(f(X))+bE(g(X)),$ where $E(g(X))=\int_{-\infty}^{\infty}g(x)p(x) \, dx.$

\end{enumerate} 
\end{theorem}

\subsection{Quantiles}

\begin{definition}
Given any random variable $X,$ and $0 < p < 1,$ the $p$-quantile of $X,$ $\Phi_p$, is the smallest number such that $$P(X\leq \Phi_p)=F(\Phi_p)\geq p.$$
\end{definition}

\subsection{Probability distribution of continuous random variable}

\subsubsection{The uniform random variable on an interval [a,b]}

\begin{definition}
A random variable $X$ is \textbf{uniform} on the interval $[a, b],$ with $a < b,$ if its p.d.f. is given by $$f(x)=\frac{1}{b-a}, \quad a\leq x\leq b.$$
[Notation: $X\sim U[a,b]$].
\end{definition}

\begin{theorem}
If $X$ is a uniform random variable on the interval $[a, b]$ then \begin{itemize}

	\item $E(X)=\frac{a+b}{2};$
	\item $Var(X)=\frac{(b-a)^2}{12};$
	\item the c.d.f, $F$ is given by $$\begin{aligned}
F(x)=\int_{-\infty}^{x} f(t) \,dt=\begin{cases}
0, \quad y<a; \\
\frac{x-a}{b-a}, \quad a\leq x<b; \\
1, \quad x\geq b.
\end{cases}
\end{aligned}$$

\end{itemize}
\end{theorem}

\subsubsection{The Gaussian or Normal distributed random variables}

\begin{definition}
A random variable $X$ is said to be a \textbf{normal random variable}, or simply $X$ is normally distributed, with parameters $\mu \in \bb{R}$ and $\sigma^2 >0$ if the p.d.f. of $X$ is given by $$f(x)=\frac{1}{\sqrt{2\pi}\sigma}e^{\frac{-(x-\mu)^2}{2\sigma^2}}, \quad x\in \bb{R}.$$ [Notation: $X\sim N(\mu,\sigma^2)$]. 
The p.d.f. is a bell-shaped curve that is symmetric about $\mu$.
\end{definition}

\begin{theorem}
If $X\sim N(\mu,\sigma^2)$ then \begin{itemize}

	\item $E(X)=\mu;$
	\item $Var(X)=\sigma^2;$
	\item if $Y=aX+b$ then $Y\sim N(a\mu+b,a^2\sigma^2).$

\end{itemize}
\end{theorem}

\begin{corollary}
If $X\sim N(\mu,\sigma^2),$ then $$Z=\frac{X-\mu}{\sigma}\sim N(0,1).$$ Such a random variable is said to be a \textbf{standard normal random variable.} It is customary to denote the c.d.f. of $Z$ by $\Phi(x).$ That is, $$\Phi(x)=\int_{-\infty}^{x} \frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}}\,dt.$$ The values of the standard c.d.f. for are given in Statistical tables.

\end{corollary}

\subsubsection{Normal approximation to the Binomial}

\begin{theorem}
If $S_n$ denotes the number of successes that occur when $n$ independent trials, each resulting in a success with probability $p,$ are performed, then, for any $x \in \bb{R},$ $$P\left(\frac{S_n-np}{\sqrt{np(1-p)}}\right)\to_{n\to\infty}\Phi(x).$$ In general, this approximation is quite good when $np(1 - p) \geq 10.$
 
\end{theorem}


\subsubsection{Exponentially distributed random variable}

\begin{definition}
A random variable $X$ whose probability density function is given, for some $\lambda > 0,$ by $$f(x)=\lambda e^{-\lambda x}, \quad x\geq0,$$ is said to be an \textbf{exponential random variable} with parameter $\lambda$
[notation: $X \sim Exp(\lambda)$].
\end{definition}

\begin{remark}
In practice, the exponential distribution often arises as the distribution of the amount of time until some specific event occurs.
\end{remark}

\begin{theorem}
If $X$ is an exponential random variable with parameter $\lambda$ then \begin{itemize}
	
	\item $E(X)=\frac{1}{\lambda};$
	\item $Var(X)=\frac{1}{\lambda^2};$
	\item the c.d.f., $F$ is given by $$\begin{aligned}
	F(X)=\begin{cases}0, \quad x<0 \\ 1-e^{-\lambda x}, \quad x\geq 0\end{cases}
	\end{aligned}$$

\end{itemize}
\end{theorem}

\begin{theorem}
The exponential distribution satisfies the \textbf{memoryless property.} That is, if $X \sim Exp(\lambda)$ then $$P(X>t+s|X>t)=P(X>s),$$ for all $s,t>0.$
\end{theorem}

\begin{proposition}
The time interval $T$ between two consecutive Poisson occurrences at the rate $\lambda > 0$ is exponentially distributed with mean value $\frac{1}{\lambda}.$
\end{proposition}

\subsubsection{\texorpdfstring{$\Gamma$}{TEXT}-Distributed random variable}

\begin{definition}
A random variable $X$ with p.d.f. given by $$f(x) = \frac{\beta^\alpha}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}, \quad x>0,$$ is said to be a \textbf{gamma distributed} random variable with parameters $(\alpha, \beta)$ [notation: $X \sim Gamma(\alpha, \beta)$].
\end{definition}

The function $\Gamma(\alpha),$ called the \textbf{Gamma function}, is defined as $$\Gamma(\alpha)=\int_{0}^{\infty} t^{\alpha-1}e^{-t}\,dt.$$

\begin{proposition}
$$\Gamma(n)=(n-1)!$$
\end{proposition}

\subsubsection{\texorpdfstring{$\chi^2$}{TEXT} distribution}

\begin{definition}
A random variable is said to have a $\chi^2$ distribution with $\nu$ degrees of freedom if it is a $\Gamma$-distributed random variable with $\alpha = \f{\nu}{2}$ and$ \beta = 2,$ i.e. $$Y\sim \chi^2_\nu \iff Y \sim \Gamma\left(\frac{\nu}{2},2\right).$$
\end{definition}

\section{Chebyshev’s theorem}

\begin{theorem}
Let $X$ be a random variable with finite mean $\mu$ and variance $\sigma^2.$ Then, for any $k > 0,$ we have: $$\begin{aligned} P(|X-\mu|<k\sigma)&\geq 1-\frac{1}{k^2} \quad \text{equivalently} \\ P(|X-\mu|\geq k\sigma)&\leq\frac{1}{k^2}.\end{aligned}$$
\end{theorem}

\section{Bivariate probability distributions}

\begin{definition}
Let $(X , Y )$ be a random pair. We define the \textbf{joint cumulative distribution function} (joint c.d.f.) of $(X , Y )$ by $$F(x,y)=P(X\leq x, Y\leq y), \quad (x,y)\in \bb{R}^2.$$
\end{definition}

\begin{definition}
Let $(X , Y )$ be a random pair, where $X$ and $Y$ are both discrete random variables. We define the \textbf{joint probability mass function} (joint p.m.f.) of $(X,Y)$ by $$p(x_i,y_j)=P(X=x_i,Y=y_j), \quad (x_i,y_j)\in X(\Omega)\times Y(\Omega)$$
\end{definition}

The joint p.m.f. of $(X , Y )$ is usually given in the form of a table.

\begin{proposition}
The joint p.m.f. satisfies the conditions:
\begin{itemize}

	\item $p(x_i,x_j)=P(X=x_i,Y=y_j)\geq 0, \quad \forall{(x_i,x_j)};$

	\item $\sum_{i=1}^{\infty}\sum_{j=1}^{\infty}p(x_i,x_j)=P(X=x_i,Y=y_j)=1.$

\end{itemize}
\end{proposition}

\begin{definition}
The functions $$\begin{aligned}
p_X(x_i)&= \sum_{j=1}^{\infty}p(x_i,y_j)  \text{ and}\\ p_Y(y_j)&=\sum_{i=1}^{\infty}p(x_i,y_j)
\end{aligned}$$ are called the \textbf{marginal probability mass functions} of $X$ and $Y ,$ respectively.
\end{definition}

\begin{definition}
The random variables $X$ and $Y$ are called jointly continuous if there exists a function $f (x , y )$ defined for $x , y \in \bb{R}$ such that, for every set $C$ of pairs of real numbers $$P((X,Y)\in C) = \iint_{(x,y)\in C}f(x,y) \, dxdy.$$ The function $f(x,y)$ is called the \textbf{joint probability density function} of $X$ and $Y .$
\end{definition}

\begin{proposition}
The joint p.d.f. satisfies the conditions: \begin{itemize}

	\item $f(x,y)\geq 0$ for all $(x,y)\in \bb{R}^2;$
	\item $\iint_{\bb{R}^2}f(x,y)\,dxdy=1.$
\end{itemize}
\end{proposition}

\begin{definition}
The functions $$\begin{aligned}
f_X(x)&=\int_{-\infty}^{+\infty} f(x,y) \,dy \text{ and}\\ 
f_Y(y)&=\int_{-\infty}^{+\infty} f(x,y)\,dx
\end{aligned}$$ are called the \textbf{ marginal probability density functions} of $X$ and $Y ,$ respectively.
\end{definition}

\subsection{Independent random variables}

\begin{definition}
The random variables $X$ and $Y$ are said to be independent if, for any two sets of real numbers $A$ and $B,$ $$P(X\in A, Y\in B)=P(X\in A)P(Y\in B).$$ In other words, $X$ and $Y$ are independent if the events $E = \{X \in A\}$ and $F = \{Y \in B\}$ are independent.
\end{definition}

\begin{proposition}
The equality $P(X\in A, Y\in B)=P(X\in A)P(Y\in B) $holds if and only if, for all $x,y \in \bb{R},$ $$P(X\leq x , Y\leq y)=P(X\leq x)P(Y\leq y).$$ In terms of the joint c.d.f. F of the random pair $(X,Y),$ $X$ and $Y$ are independent r.v.’s if $$F(x,y)=F_X(x)F_y(y), \quad x,y\in \bb{R}.$$

\end{proposition}

\subsection{Conditional distribution}

\begin{definition}
Let $X$ and $Y$ be jointly discrete random variables. It is natural to define the \textbf{conditional probability mass function} of $X$ given $Y = y$ by $$p_{X|Y}(x_i|y)=P(X=x_i|Y=y)=\frac{p(x_i,y)}{p_Y(y)},$$ for all values of $Y$ such that $p_Y (y) > 0$
\end{definition}

\pagebreak 

\section{DRAFT 2}

\section{Bivariate probability distributions}

\subsection{Joint distribution functions}

\begin{definition}
The \textbf{joint cumulative distribution function} (or joint c.d.f.) of $X$ and $Y,$ denoted by $F_{XY}(x,y),$ is the function defined by $$F_{XY}=P(X\leq x, Y \leq y).$$
\end{definition}

\begin{remark}
The event $(X\leq x, Y \leq y)$ is equivalent to the event $A\cup B,$ where $A$ and $B$ are events of $S$ defined by $P(A)=F_X(x)$ and $P(B)=F_Y(y).$ 
\end{remark}

\begin{definition}
Two random variables $X$ and $Y$ are called \textbf{independent} if $$F_{XY}(x,y)=F_X(x)F_Y(y)$$ for every value of $x$ and $y.$
\end{definition}


\end{document}			